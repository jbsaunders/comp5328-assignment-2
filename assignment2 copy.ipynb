{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9IrXtNd8l-9H"
   },
   "source": [
    "## COMP5328 - Advanced Machine Learning\n",
    "## Assignment 2: Title\n",
    "----------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Previous Log File does not exist.\n"
     ]
    }
   ],
   "source": [
    "# Common imports\n",
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import time\n",
    "import datetime as dt\n",
    "from PIL import Image\n",
    "from collections import Counter, defaultdict\n",
    "from datetime import datetime\n",
    "\n",
    "# Ploting\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Evaluation \n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import normalized_mutual_info_score\n",
    "\n",
    "# Experiment\n",
    "from joblib import Parallel, delayed\n",
    "import traceback\n",
    "\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import os\n",
    "import torch.optim as optim\n",
    "import random\n",
    "\n",
    "\n",
    "# Log file\n",
    "log_file = 'experiment_results.txt'\n",
    "\n",
    "if os.path.exists(log_file):\n",
    "    os.remove(log_file)\n",
    "    print(\"Previous Log File deleted.\")\n",
    "else:\n",
    "    print(\"Previous Log File does not exist.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Experiment variables\n",
    "# Common\n",
    "datasets = [\"cifar\", \"fashion03\", \"fashion06\"]\n",
    "\n",
    "\n",
    "\n",
    "dataset_folder = 'data/'\n",
    "cifar_dataset = dataset_folder+'CIFAR.npz'\n",
    "MNISTO3_dataset = dataset_folder+'FashionMNIST0.3.npz'\n",
    "MNISTO6_dataset = dataset_folder+'FashionMNIST0.6.npz'\n",
    "\n",
    "\n",
    "DATA_PATHS = {\n",
    "    'fashion03': cifar_dataset,\n",
    "    'fashion06': MNISTO3_dataset,\n",
    "    'cifar':     MNISTO6_dataset,\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "known_T_fashion_03 = np.array(  [[0.7,0.3,0.0],\n",
    "                                [0.0,0.7,0.3],\n",
    "                                [0.3,0.0,0.7]], dtype=np.float32)\n",
    "\n",
    "known_T_fashion_06 = np.array(  [[0.3,0.4,0.3],\n",
    "                                [0.4,0.3,0.3],\n",
    "                                [0.3,0.3,0.4]], dtype=np.float32)\n",
    "\n",
    "def pick_known_T(tag):\n",
    "    if tag == 'fashion03':\n",
    "        return known_T_fashion_03\n",
    "    elif tag == 'fashion06':\n",
    "        return known_T_fashion_06\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def set_seed(seed=0):\n",
    "    import random\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qYzWuiytl-9I"
   },
   "source": [
    "## 1. Load Dataset\n",
    "\n",
    "### 1.0 Data Folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unzip:  cannot find or open datasets.zip, datasets.zip.zip or datasets.zip.ZIP.\n"
     ]
    }
   ],
   "source": [
    "# Path to your dataset zip stored in Drive\n",
    "zip_path = \"datasets.zip\"\n",
    "\n",
    "# Unzip file\n",
    "!unzip -o -q \"$zip_path\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OxEr9ihznSqa",
    "outputId": "8b9fd2c9-b2f9-45f4-94aa-d0d473b7e140"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 172688\n",
      "-rw-r--r--@ 1 jamie.saunders  staff  55440974 Oct  4  2019 CIFAR.npz\n",
      "-rw-r--r--@ 1 jamie.saunders  staff  16485974 Oct 10  2021 FashionMNIST0.3.npz\n",
      "-rw-r--r--@ 1 jamie.saunders  staff  16485974 Oct 10  2021 FashionMNIST0.6.npz\n"
     ]
    }
   ],
   "source": [
    "# The structure of data folder.\n",
    "!ls -l data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_npz(path):\n",
    "    d = np.load(path)\n",
    "    Xtr, Str = d['Xtr'], d['Str']\n",
    "    Xts, Yts = d['Xts'], d['Yts']\n",
    "    return Xtr, Str, Xts, Yts\n",
    "\n",
    "# A helper class, it is used as an input of the DataLoader object.\n",
    "class DatasetArray(Dataset):\n",
    "    def __init__(self, data, labels=None, transform=None):\n",
    "        if labels != None:\n",
    "            self.data_arr = np.asarray(data).astype(np.float32)\n",
    "            self.label_arr = np.asarray(labels).astype(np.long)\n",
    "        else:\n",
    "            tmp_arr = np.asarray(data)\n",
    "            self.data_arr = tmp_arr[:,:-1].astype(np.float32)\n",
    "            self.label_arr = tmp_arr[:,-1].astype(np.long)\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data_arr)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "     \n",
    "        data = self.data_arr[index]\n",
    "        label = self.label_arr[index]\n",
    "        \n",
    "        if self.transform is not None:\n",
    "            data = self.transform(data)\n",
    "            \n",
    "        return (data, label)\n",
    "    \n",
    "    \n",
    "# Splitting the data into three parts.\n",
    "def train_val_test_random_split(data, fracs=[0.7,0.1,0.2]):\n",
    "    r\"\"\"Split the data into training, validation and test set.\n",
    "    Args:\n",
    "        fracs: a list of length three\n",
    "    \"\"\"\n",
    "    assert len(fracs) == 3\n",
    "    assert sum(fracs) == 1\n",
    "    assert all(frac > 0 for frac in fracs)\n",
    "    n = len(data)\n",
    "    subset_lens = [int(n*frac) for frac in fracs]\n",
    "    idxs = list(range(n))\n",
    "    random.shuffle(idxs)\n",
    "    data = np.array(data)\n",
    "    new_data = []\n",
    "    start_idx = 0\n",
    "    for subset_len in subset_lens:\n",
    "        end_idx = start_idx + subset_len\n",
    "        cur_idxs = idxs[start_idx:end_idx]\n",
    "        new_data.append(data[cur_idxs,:].tolist())\n",
    "        start_idx = end_idx\n",
    "    return new_data\n",
    "\n",
    "# Preparation of the data for training, validation and testing a pytorch network. \n",
    "# Note that the test data is not in use for this lab.\n",
    "def get_loader(batch_size =128, num_workers = 0, train_val_test_split = [0.7,0.1,0.2], data=None):\n",
    "    r\"\"\"This function is used to read the data file and split the data into three subsets, i.e, \n",
    "    train data, validation data and test data. Their corresponding DataLoader objects are returned.\"\"\"\n",
    "    \n",
    "    [train_data, val_data, test_data] = train_val_test_random_split(data, fracs = train_val_test_split)\n",
    "\n",
    "    train_data = DatasetArray(data = train_data)\n",
    "    val_data = DatasetArray(data = val_data)\n",
    "    test_data = DatasetArray(data = test_data)\n",
    "\n",
    "    #The pytorch built-in class DataLoader can help us to shuffle the data, draw mini-batch,\n",
    "    #do transformations, etc. \n",
    "    train_loader = DataLoader(\n",
    "        train_data,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=num_workers,\n",
    "    )\n",
    "\n",
    "    val_loader = DataLoader(\n",
    "        val_data,\n",
    "        batch_size=100,\n",
    "        shuffle=False,\n",
    "        num_workers=num_workers,\n",
    "    )\n",
    "\n",
    "    test_loader = DataLoader(\n",
    "        test_data,\n",
    "        batch_size=100,\n",
    "        num_workers=num_workers,\n",
    "        shuffle=False,\n",
    "    )\n",
    "    return train_loader, val_loader, test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NpzDataset(Dataset):\n",
    "    def __init__(self, X, y, is_cifar=False):\n",
    "        self.X = X.astype(np.float32)\n",
    "        self.y = y.astype(np.int64)\n",
    "        self.is_cifar = is_cifar\n",
    "\n",
    "        # Normalize to [0,1]\n",
    "        self.X = self.X / 255.0 if self.X.max() > 1.0 else self.X\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x = self.X[idx]\n",
    "        if x.ndim == 1:\n",
    "            # flat; try to infer shape 28x28 or 32x32x3\n",
    "            if x.size == 28*28:\n",
    "                x = x.reshape(1, 28, 28)\n",
    "            elif x.size == 32*32*3:\n",
    "                x = x.reshape(3, 32, 32)\n",
    "            else:\n",
    "                raise ValueError(\"Unknown flat image shape: {}\".format(x.shape))\n",
    "        else:\n",
    "            # (H,W) or (H,W,C)\n",
    "            if x.ndim == 2:\n",
    "                x = x[None, ...]  # to (1,H,W)\n",
    "            elif x.ndim == 3:\n",
    "                # assume HWC -> CHW\n",
    "                x = np.transpose(x, (2, 0, 1))\n",
    "            else:\n",
    "                raise ValueError(f\"Unexpected image dims: {x.shape}\")\n",
    "        return torch.from_numpy(x), torch.tensor(self.y[idx])\n",
    "\n",
    "\n",
    "def load_npz(path):\n",
    "    d = np.load(path)\n",
    "    Xtr, Str = d['Xtr'], d['Str']\n",
    "    Xts, Yts = d['Xts'], d['Yts']\n",
    "    return Xtr, Str, Xts, Yts\n",
    "\n",
    "\n",
    "def make_loaders(Xtr, Str, batch_size=128, seed=0, test_size=0.2):\n",
    "    # 80/20 split each repetition\n",
    "    X_tr, X_val, y_tr, y_val = train_test_split(\n",
    "        Xtr, Str, test_size=test_size, random_state=seed, stratify=Str\n",
    "    )\n",
    "\n",
    "    is_cifar = (X_tr.shape[-1] == 3) if X_tr.ndim == 4 else (X_tr.shape[-1] == 32*32*3)\n",
    "\n",
    "    train_ds = NpzDataset(X_tr, y_tr, is_cifar)\n",
    "    val_ds   = NpzDataset(X_val, y_val, is_cifar)\n",
    "\n",
    "    train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "    val_loader   = DataLoader(val_ds,   batch_size=batch_size, shuffle=False, num_workers=0)\n",
    "\n",
    "    return train_loader, val_loader, is_cifar\n",
    "\n",
    "def make_test_loader(Xts, Yts, batch_size=256):\n",
    "    is_cifar = (Xts.shape[-1] == 3) if Xts.ndim == 4 else (Xts.shape[-1] == 32*32*3)\n",
    "    test_ds = NpzDataset(Xts, Yts, is_cifar)\n",
    "    return DataLoader(test_ds, batch_size=batch_size, shuffle=False, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ForwardCorrectedCE(nn.Module):\n",
    "    \"\"\"\n",
    "    Forward loss correction: minimizes CE between T^T p and noisy labels.\n",
    "    T: class-transition matrix where T[i,j] = P(S=j | Y=i). Shape [C,C].\n",
    "    \"\"\"\n",
    "    def __init__(self, T):\n",
    "        super().__init__()\n",
    "        self.register_buffer('T', T)  # [C,C]\n",
    "\n",
    "    def forward(self, logits, y_noisy):\n",
    "        # logits -> p(y|x)\n",
    "        p = F.softmax(logits, dim=1)  # [B,C]\n",
    "        # mix via T^T\n",
    "        mixed = torch.clamp(p @ self.T.t(), 1e-6, 1.0)\n",
    "        log_mixed = torch.log(mixed)\n",
    "        return F.nll_loss(log_mixed, y_noisy)\n",
    "\n",
    "\n",
    "class GeneralizedCrossEntropy(nn.Module):\n",
    "    \"\"\"\n",
    "    GCE loss: L_q(p, y) = (1 - p_y^q) / q, with q in (0,1].\n",
    "    q→1 recovers CE; smaller q is more robust to label noise.\n",
    "    \"\"\"\n",
    "    def __init__(self, q=0.7):\n",
    "        super().__init__()\n",
    "        assert 0 < q <= 1\n",
    "        self.q = q\n",
    "\n",
    "    def forward(self, logits, y):\n",
    "        p = F.softmax(logits, dim=1)\n",
    "        p_y = p.gather(1, y.view(-1,1)).clamp(min=1e-6, max=1.0)\n",
    "        if self.q == 1.0:\n",
    "            return -torch.log(p_y).mean()\n",
    "        return ((1 - p_y.pow(self.q)) / self.q).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_block(cin, cout):\n",
    "    return nn.Sequential(\n",
    "        nn.Conv2d(cin, cout, 3, padding=1),\n",
    "        nn.BatchNorm2d(cout),\n",
    "        nn.ReLU(inplace=True),\n",
    "        nn.MaxPool2d(2)\n",
    "    )\n",
    "\n",
    "class SmallCNN28(nn.Module):\n",
    "    \"\"\"For 1×28×28 images.\"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            conv_block(1, 32),  # 14x14\n",
    "            conv_block(32, 64), # 7x7\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(64*7*7, 128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(128, 3)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "\n",
    "class SmallCNNCifar(nn.Module):\n",
    "    \"\"\"For 3×32×32 images.\"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            conv_block(3, 32),   # 16x16\n",
    "            conv_block(32, 64),  # 8x8\n",
    "            conv_block(64, 128), # 4x4\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(128*4*4, 256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(256, 3)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "\n",
    "def make_model(is_cifar):\n",
    "    return SmallCNNCifar() if is_cifar else SmallCNN28()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def predict_proba(model, loader, device):\n",
    "    model.eval()\n",
    "    p_arr = []\n",
    "    ys = []\n",
    "    for xb, yb in loader:\n",
    "        xb = xb.to(device)\n",
    "        output = model(xb)\n",
    "        p = F.softmax(output, dim=1).cpu().numpy()\n",
    "        p_arr.append(p)\n",
    "        ys.append(yb.numpy())\n",
    "    return np.concatenate(p_arr), np.concatenate(ys)\n",
    "\n",
    "\n",
    "def estimate_transition_anchor(t, train_loader, is_cifar, num_classes=3, device='cpu', epochs=5):\n",
    "    \"\"\"\n",
    "    Simple anchor/confident-example estimator (Patrini et al., 2017 style):\n",
    "    1) Train a base classifier on noisy data.\n",
    "    2) Get p(y|x) on training set.\n",
    "    3) For each clean class i, find indices whose predicted argmax == noisy label == i and with high confidence.\n",
    "    4) For those indices, estimate column i of T as average of empirical noisy label distribution given model predicts i.\n",
    "    Here: since we only have noisy labels S, we approximate T[:, i] ≈ E[ onehot(S) | argmax p = i, p_i >= τ ].\n",
    "    Normalize columns to sum to 1.\n",
    "    \"\"\"\n",
    "\n",
    "    device = torch.device(device)\n",
    "    model = make_model(is_cifar).to(device)\n",
    "\n",
    "    optimizer = optim.Adam(model.parameters(), lr=1e-3, weight_decay=1e-4)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    # quick warmup training on noisy labels\n",
    "    for _ in range(epochs):\n",
    "        train_one_epoch(model, train_loader, optimizer, criterion, device=device)  # default CE inside\n",
    "\n",
    "    # collect probs & noisy labels\n",
    "    p_arr, y_noisy = predict_proba(model, train_loader, device)\n",
    "    preds = p_arr.argmax(axis=1)\n",
    "    maxp = p_arr.max(axis=1)\n",
    "\n",
    "    C = num_classes\n",
    "    T = t\n",
    "    # choose class-wise thresholds based on quantiles for stability\n",
    "    for i in range(C):\n",
    "        idx = np.where(preds == i)[0]\n",
    "        if idx.size == 0:\n",
    "            T[:, i] = np.ones(C) / C\n",
    "            continue\n",
    "        # high-confidence subset (top 30% by p_i)\n",
    "        conf = maxp[idx]\n",
    "        if conf.size > 50:\n",
    "            tau = np.quantile(conf, 0.7)\n",
    "        else:\n",
    "            tau = np.min(conf)  # keep all if tiny\n",
    "        keep = idx[conf >= tau]\n",
    "        if keep.size == 0:\n",
    "            keep = idx\n",
    "        # empirical distribution of noisy labels in this confident set\n",
    "        hist = np.bincount(y_noisy[keep], minlength=C).astype(np.float64)\n",
    "        if hist.sum() == 0:\n",
    "            T[:, i] = np.ones(C) / C\n",
    "        else:\n",
    "            T[:, i] = hist / hist.sum()\n",
    "\n",
    "    # column-normalize\n",
    "    colsum = T.sum(axis=0, keepdims=True)\n",
    "    T = np.divide(T, np.maximum(colsum, 1e-8))\n",
    "    return T.astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_transition_trevision(\n",
    "    T_init,\n",
    "    train_loader,\n",
    "    is_cifar,\n",
    "    device=\"cpu\",\n",
    "    epochs=5,\n",
    "    lambda_reg=1e-4,\n",
    "    lr_t=5e-3,\n",
    "    lr_model=1e-3,\n",
    "    warmup_epochs=3,\n",
    "    log_every=1\n",
    "):\n",
    "    \"\"\"\n",
    "    Refine transition matrix using T-Revision method (Patrini et al. style).\n",
    "\n",
    "    Args:\n",
    "        T_init (np.ndarray or torch.Tensor): initial transition matrix [C, C]\n",
    "        train_loader: noisy dataloader (x, y_noisy)\n",
    "        is_cifar (bool): dataset selector for model architecture\n",
    "        num_classes (int): number of classes\n",
    "        device (str): 'cpu', 'cuda', or 'mps'\n",
    "        epochs (int): number of refinement epochs for ΔT\n",
    "        lambda_reg (float): regularization to keep T close to T_init\n",
    "        lr_t (float): learning rate for ΔT\n",
    "        lr_model (float): learning rate for model warm-up\n",
    "        warmup_epochs (int): number of model warm-up epochs\n",
    "        log_every (int): print interval\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: refined transition matrix [C, C]\n",
    "    \"\"\"\n",
    "\n",
    "    device = torch.device(device)\n",
    "    C = 3\n",
    "\n",
    "    # ---------------------------\n",
    "    # 1. Base model setup\n",
    "    # ---------------------------\n",
    "    model = make_model(is_cifar).to(device)\n",
    "    opt_model = optim.Adam(model.parameters(), lr=lr_model, weight_decay=1e-4)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    # Warm-up (train classifier on noisy labels)\n",
    "    print(f\"[Warm-up] training base classifier for {warmup_epochs} epochs...\")\n",
    "    for e in range(warmup_epochs):\n",
    "        train_one_epoch(model, train_loader, opt_model, criterion=criterion, device=device)\n",
    "        print(f\"  done epoch {e+1}/{warmup_epochs}\")\n",
    "\n",
    "    # ---------------------------\n",
    "    # 2. Get predicted probabilities\n",
    "    # ---------------------------\n",
    "    probs, y_noisy = predict_proba(model, train_loader, device)\n",
    "    p = torch.tensor(probs, dtype=torch.float32, device=device)\n",
    "    y_t = torch.tensor(y_noisy, dtype=torch.long, device=device)\n",
    "\n",
    "    # ---------------------------\n",
    "    # 3. Initialize learnable ΔT (T-Revision)\n",
    "    # ---------------------------\n",
    "    T_init_torch = torch.tensor(T_init, dtype=torch.float32, device=device)\n",
    "    delta_T = nn.Parameter(torch.zeros_like(T_init_torch))\n",
    "    optimizer_T = optim.Adam([delta_T], lr=lr_t)\n",
    "\n",
    "    print(f\"[Optimization] refining transition matrix for {epochs} epochs...\")\n",
    "\n",
    "    # ---------------------------\n",
    "    # 4. Optimize ΔT\n",
    "    # ---------------------------\n",
    "    for ep in range(epochs):\n",
    "        optimizer_T.zero_grad()\n",
    "\n",
    "        # Proposed transition\n",
    "        T_prime = T_init_torch + delta_T\n",
    "        T_prime = torch.clamp(T_prime, min=1e-6)\n",
    "\n",
    "        # Forward correction: p(y_noisy | x) = p(y|x) * T'\n",
    "        noisy_pred = torch.clamp(p @ T_prime.t(), 1e-6, 1.0)\n",
    "        log_noisy = torch.log(noisy_pred)\n",
    "\n",
    "        # Loss = NLL + regularization\n",
    "        loss_ce = nn.NLLLoss()(log_noisy, y_t)\n",
    "        reg = lambda_reg * torch.norm(T_prime - T_init_torch, p=\"fro\")\n",
    "        loss = loss_ce + reg\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer_T.step()\n",
    "\n",
    "        if (ep + 1) % log_every == 0:\n",
    "            grad_norm = delta_T.grad.abs().mean().item() if delta_T.grad is not None else 0\n",
    "            print(f\"Epoch {ep+1}/{epochs} | loss={loss.item():.5f} | grad={grad_norm:.5e}\")\n",
    "\n",
    "    # ---------------------------\n",
    "    # 5. Normalize once at the end\n",
    "    # ---------------------------\n",
    "    T_final = T_init_torch + delta_T.data\n",
    "    T_final = torch.clamp(T_final, min=1e-6)\n",
    "    T_final = T_final / T_final.sum(dim=0, keepdim=True)\n",
    "\n",
    "    print(\"\\n[Done] Refined Transition Matrix:\")\n",
    "    print(T_final.detach().cpu().numpy())\n",
    "\n",
    "    return T_final.detach().cpu().numpy().astype(np.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def accuracy(model, loader, device):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for xb, yb in loader:\n",
    "        xb, yb = xb.to(device), yb.to(device)\n",
    "        logits = model(xb)\n",
    "        pred = logits.argmax(dim=1)\n",
    "        correct += (pred == yb).sum().item()\n",
    "        total += yb.numel()\n",
    "    return correct / max(total, 1)\n",
    "\n",
    "\n",
    "def train_one_epoch(model, loader, optimizer, criterion, device='mps'):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    for xb, yb in loader:\n",
    "        xb, yb = xb.to(device), yb.to(device)\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        logits = model(xb)\n",
    "        loss = criterion(logits, yb)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item() * yb.size(0)\n",
    "    return total_loss / len(loader.dataset)\n",
    "\n",
    "\n",
    "def fit_model(model, train_loader, val_loader, device, loss_name='gce', T=None, q=0.7, beta=0.2, epochs=10, lr=1e-3):\n",
    "\n",
    "    if loss_name == 'forward':\n",
    "        assert T is not None, \"Forward correction requires known/estimated T\"\n",
    "        print('forward loss')\n",
    "        criterion = ForwardCorrectedCE(torch.tensor(T, dtype=torch.float32, device=device))\n",
    "    else:\n",
    "        print('GCE loss')\n",
    "        criterion = GeneralizedCrossEntropy(q=q)\n",
    "\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=1e-4)\n",
    "\n",
    "    best_val = -np.inf\n",
    "    best_state = None\n",
    "\n",
    "    for _ in range(epochs):\n",
    "        train_one_epoch(model, train_loader, optimizer, criterion, device)\n",
    "        # early stopping on val accuracy (cheap)\n",
    "        val_acc = accuracy(model, val_loader, device)\n",
    "        if val_acc > best_val:\n",
    "            best_val = val_acc\n",
    "            best_state = {k: v.detach().cpu().clone() for k, v in model.state_dict().items()}\n",
    "\n",
    "    if best_state is not None:\n",
    "        model.load_state_dict(best_state)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_once(args, seed):\n",
    "    set_seed(seed)\n",
    "    device = torch.device('mps')\n",
    "\n",
    "    # load data\n",
    "    Xtr, Str, Xts, Yts = load_npz(DATA_PATHS[args['dataset']])\n",
    "\n",
    "    # loaders for this split\n",
    "    train_loader, val_loader, is_cifar = make_loaders(Xtr, Str, batch_size=args['batch_size'], seed=seed)\n",
    "    test_loader = make_test_loader(Xts, Yts, batch_size=512)\n",
    "  \n",
    "    # choose model\n",
    "    model = make_model(is_cifar).to(device)\n",
    "\n",
    "    # Transition matrix\n",
    "    T = None\n",
    "    if args['loss'] == 'forward' or args['loss'] == 'gce':\n",
    "        if args['estimate_T'] or args['dataset']=='cifar':\n",
    "            print('estimating matrix')\n",
    "            T = np.zeros((3, 3), dtype=np.float64)\n",
    "            T = estimate_transition_anchor(T, train_loader, is_cifar, device=device, epochs=args['est_epochs'])\n",
    "            T = estimate_transition_trevision(T, train_loader, is_cifar, device=device, epochs=args['est_epochs'])\n",
    "            print(T)\n",
    "        else:\n",
    "            T = pick_known_T(args['dataset'])\n",
    "            if T is None:\n",
    "                raise ValueError(\"Forward loss selected but no known T for this dataset; use --estimate_T.\")\n",
    "            \n",
    "    if args['dataset']=='fashion03':\n",
    "        q = 0.3\n",
    "    elif args['dataset']=='fashion06':\n",
    "        q = 0.6\n",
    "    else:\n",
    "        q = 0.7\n",
    "\n",
    "    # fit\n",
    "    model = fit_model(\n",
    "        model,\n",
    "        train_loader,\n",
    "        val_loader,\n",
    "        device,\n",
    "        loss_name=args['loss'],\n",
    "        T=T,\n",
    "        q=q,\n",
    "        beta=args['beta'],\n",
    "        epochs=args['epochs'],\n",
    "        lr=args['lr'],\n",
    "    )\n",
    "\n",
    "    # evaluate on clean test set\n",
    "    test_acc = accuracy(model, test_loader, device)\n",
    "    return float(test_acc), (T.tolist() if T is not None else None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = ['forward','gce']\n",
    "datasets = ['cifar', \"fashion03\", \"fashion06\"]\n",
    "base = {\n",
    "    \"runs\":10,\n",
    "    \"epochs\": 10,\n",
    "    \"estimate_T\":True,\n",
    "    \"loss\":'forward',\n",
    "    \"batch_size\":4096,\n",
    "    \"q\":0.6,\n",
    "    \"est_epochs\":5,\n",
    "    \"beta\":0.2,\n",
    "    \"lr\":1e-3,\n",
    "    \"num_classes\":3,\n",
    "    \"device\":'mps'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'runs': 10, 'epochs': 10, 'estimate_T': True, 'loss': 'forward', 'batch_size': 4096, 'q': 0.6, 'est_epochs': 5, 'beta': 0.2, 'lr': 0.001, 'num_classes': 3, 'device': 'mps', 'dataset': 'cifar', 'out': 'cifar_forward_2025-10-2823:31_0.json'}, {'runs': 10, 'epochs': 10, 'estimate_T': True, 'loss': 'gce', 'batch_size': 4096, 'q': 0.6, 'est_epochs': 5, 'beta': 0.2, 'lr': 0.001, 'num_classes': 3, 'device': 'mps', 'dataset': 'cifar', 'out': 'cifar_gce_2025-10-2823:31_0.json'}, {'runs': 10, 'epochs': 10, 'estimate_T': True, 'loss': 'forward', 'batch_size': 4096, 'q': 0.6, 'est_epochs': 5, 'beta': 0.2, 'lr': 0.001, 'num_classes': 3, 'device': 'mps', 'dataset': 'fashion03', 'out': 'fashion03_forward_2025-10-2823:31_1.json'}, {'runs': 10, 'epochs': 10, 'estimate_T': True, 'loss': 'gce', 'batch_size': 4096, 'q': 0.6, 'est_epochs': 5, 'beta': 0.2, 'lr': 0.001, 'num_classes': 3, 'device': 'mps', 'dataset': 'fashion03', 'out': 'fashion03_gce_2025-10-2823:31_1.json'}, {'runs': 10, 'epochs': 10, 'estimate_T': True, 'loss': 'forward', 'batch_size': 4096, 'q': 0.6, 'est_epochs': 5, 'beta': 0.2, 'lr': 0.001, 'num_classes': 3, 'device': 'mps', 'dataset': 'fashion06', 'out': 'fashion06_forward_2025-10-2823:31_2.json'}, {'runs': 10, 'epochs': 10, 'estimate_T': True, 'loss': 'gce', 'batch_size': 4096, 'q': 0.6, 'est_epochs': 5, 'beta': 0.2, 'lr': 0.001, 'num_classes': 3, 'device': 'mps', 'dataset': 'fashion06', 'out': 'fashion06_gce_2025-10-2823:31_2.json'}]\n"
     ]
    }
   ],
   "source": [
    "now = datetime.now()\n",
    "now = now.strftime(\"%Y-%m-%d%_H:%M\")\n",
    "# create each cfg\n",
    "configs = []\n",
    "for i, ds in enumerate(datasets):\n",
    "    for loss in losses:\n",
    "        cfg = {**base, \"dataset\": ds, \"out\": ds+'_'+loss+'_'+now+'.json', \"loss\":loss}\n",
    "        configs.append(cfg)\n",
    "\n",
    "print(configs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "estimating matrix\n",
      "[Warm-up] training base classifier for 3 epochs...\n",
      "  done epoch 1/3\n",
      "  done epoch 2/3\n",
      "  done epoch 3/3\n",
      "[Optimization] refining transition matrix for 5 epochs...\n",
      "Epoch 1/5 | loss=1.10623 | grad=3.39153e-01\n",
      "Epoch 2/5 | loss=1.09109 | grad=3.33920e-01\n",
      "Epoch 3/5 | loss=1.07618 | grad=3.28883e-01\n",
      "Epoch 4/5 | loss=1.06151 | grad=3.24002e-01\n",
      "Epoch 5/5 | loss=1.04707 | grad=3.19271e-01\n",
      "\n",
      "[Done] Refined Transition Matrix:\n",
      "[[0.38325098 0.32982907 0.1348577 ]\n",
      " [0.32277548 0.35082367 0.39535794]\n",
      " [0.29397357 0.31934726 0.46978438]]\n",
      "[[0.38325098 0.32982907 0.1348577 ]\n",
      " [0.32277548 0.35082367 0.39535794]\n",
      " [0.29397357 0.31934726 0.46978438]]\n",
      "forward loss\n",
      "Run 01/10: test acc = 33.33%\n",
      "mps: 1 steps -> 15.29 sec | avg 15292.3 ms/step\n",
      "estimating matrix\n",
      "[Warm-up] training base classifier for 3 epochs...\n",
      "  done epoch 1/3\n",
      "  done epoch 2/3\n",
      "  done epoch 3/3\n",
      "[Optimization] refining transition matrix for 5 epochs...\n",
      "Epoch 1/5 | loss=1.09941 | grad=3.33800e-01\n",
      "Epoch 2/5 | loss=1.08450 | grad=3.28821e-01\n",
      "Epoch 3/5 | loss=1.06982 | grad=3.24022e-01\n",
      "Epoch 4/5 | loss=1.05536 | grad=3.19365e-01\n",
      "Epoch 5/5 | loss=1.04112 | grad=3.14844e-01\n",
      "\n",
      "[Done] Refined Transition Matrix:\n",
      "[[0.33333284 0.31911957 0.33333284]\n",
      " [0.33333492 0.37446716 0.33333495]\n",
      " [0.3333323  0.3064133  0.3333323 ]]\n",
      "[[0.33333284 0.31911957 0.33333284]\n",
      " [0.33333492 0.37446716 0.33333495]\n",
      " [0.3333323  0.3064133  0.3333323 ]]\n",
      "forward loss\n",
      "Run 02/10: test acc = 54.20%\n",
      "mps: 2 steps -> 10.89 sec | avg 5442.5 ms/step\n",
      "estimating matrix\n",
      "[Warm-up] training base classifier for 3 epochs...\n",
      "  done epoch 1/3\n",
      "  done epoch 2/3\n",
      "  done epoch 3/3\n",
      "[Optimization] refining transition matrix for 5 epochs...\n",
      "Epoch 1/5 | loss=1.09949 | grad=3.33880e-01\n",
      "Epoch 2/5 | loss=1.08458 | grad=3.28897e-01\n",
      "Epoch 3/5 | loss=1.06989 | grad=3.24095e-01\n",
      "Epoch 4/5 | loss=1.05543 | grad=3.19434e-01\n",
      "Epoch 5/5 | loss=1.04118 | grad=3.14909e-01\n",
      "\n",
      "[Done] Refined Transition Matrix:\n",
      "[[0.33333245 0.3064134  0.33333245]\n",
      " [0.33333513 0.38674244 0.33333516]\n",
      " [0.33333248 0.30684415 0.33333248]]\n",
      "[[0.33333245 0.3064134  0.33333245]\n",
      " [0.33333513 0.38674244 0.33333516]\n",
      " [0.33333248 0.30684415 0.33333248]]\n",
      "forward loss\n",
      "Run 03/10: test acc = 54.13%\n",
      "mps: 3 steps -> 10.78 sec | avg 3591.9 ms/step\n",
      "estimating matrix\n",
      "[Warm-up] training base classifier for 3 epochs...\n",
      "  done epoch 1/3\n",
      "  done epoch 2/3\n",
      "  done epoch 3/3\n",
      "[Optimization] refining transition matrix for 5 epochs...\n",
      "Epoch 1/5 | loss=1.10643 | grad=3.39029e-01\n",
      "Epoch 2/5 | loss=1.09129 | grad=3.33816e-01\n",
      "Epoch 3/5 | loss=1.07639 | grad=3.28796e-01\n",
      "Epoch 4/5 | loss=1.06172 | grad=3.23930e-01\n",
      "Epoch 5/5 | loss=1.04728 | grad=3.19214e-01\n",
      "\n",
      "[Done] Refined Transition Matrix:\n",
      "[[0.30232093 0.33333167 0.2999288 ]\n",
      " [0.20928603 0.33332902 0.33289802]\n",
      " [0.4883931  0.3333393  0.36717314]]\n",
      "[[0.30232093 0.33333167 0.2999288 ]\n",
      " [0.20928603 0.33332902 0.33289802]\n",
      " [0.4883931  0.3333393  0.36717314]]\n",
      "forward loss\n",
      "Run 04/10: test acc = 60.93%\n",
      "mps: 4 steps -> 10.89 sec | avg 2722.2 ms/step\n",
      "estimating matrix\n",
      "[Warm-up] training base classifier for 3 epochs...\n",
      "  done epoch 1/3\n",
      "  done epoch 2/3\n",
      "  done epoch 3/3\n",
      "[Optimization] refining transition matrix for 5 epochs...\n",
      "Epoch 1/5 | loss=1.09887 | grad=3.33568e-01\n",
      "Epoch 2/5 | loss=1.08397 | grad=3.28597e-01\n",
      "Epoch 3/5 | loss=1.06930 | grad=3.23806e-01\n",
      "Epoch 4/5 | loss=1.05485 | grad=3.19156e-01\n",
      "Epoch 5/5 | loss=1.04062 | grad=3.14641e-01\n",
      "\n",
      "[Done] Refined Transition Matrix:\n",
      "[[0.36975592 0.33333394 0.3134464 ]\n",
      " [0.31980377 0.33333194 0.30558187]\n",
      " [0.3104403  0.3333341  0.3809717 ]]\n",
      "[[0.36975592 0.33333394 0.3134464 ]\n",
      " [0.31980377 0.33333194 0.30558187]\n",
      " [0.3104403  0.3333341  0.3809717 ]]\n",
      "forward loss\n",
      "Run 05/10: test acc = 93.00%\n",
      "mps: 5 steps -> 10.97 sec | avg 2194.9 ms/step\n",
      "estimating matrix\n",
      "[Warm-up] training base classifier for 3 epochs...\n",
      "  done epoch 1/3\n",
      "  done epoch 2/3\n",
      "  done epoch 3/3\n",
      "[Optimization] refining transition matrix for 5 epochs...\n",
      "Epoch 1/5 | loss=1.09940 | grad=3.33933e-01\n",
      "Epoch 2/5 | loss=1.08449 | grad=3.28947e-01\n",
      "Epoch 3/5 | loss=1.06980 | grad=3.24140e-01\n",
      "Epoch 4/5 | loss=1.05534 | grad=3.19475e-01\n",
      "Epoch 5/5 | loss=1.04109 | grad=3.14948e-01\n",
      "\n",
      "[Done] Refined Transition Matrix:\n",
      "[[0.39061528 0.3333344  0.30339235]\n",
      " [0.305114   0.33333123 0.3045445 ]\n",
      " [0.30427068 0.3333344  0.3920631 ]]\n",
      "[[0.39061528 0.3333344  0.30339235]\n",
      " [0.305114   0.33333123 0.3045445 ]\n",
      " [0.30427068 0.3333344  0.3920631 ]]\n",
      "forward loss\n",
      "Run 06/10: test acc = 63.43%\n",
      "mps: 6 steps -> 10.67 sec | avg 1778.2 ms/step\n",
      "estimating matrix\n",
      "[Warm-up] training base classifier for 3 epochs...\n",
      "  done epoch 1/3\n",
      "  done epoch 2/3\n",
      "  done epoch 3/3\n",
      "[Optimization] refining transition matrix for 5 epochs...\n",
      "Epoch 1/5 | loss=1.09896 | grad=3.33506e-01\n",
      "Epoch 2/5 | loss=1.08407 | grad=3.28540e-01\n",
      "Epoch 3/5 | loss=1.06940 | grad=3.23753e-01\n",
      "Epoch 4/5 | loss=1.05495 | grad=3.19107e-01\n",
      "Epoch 5/5 | loss=1.04072 | grad=3.14596e-01\n",
      "\n",
      "[Done] Refined Transition Matrix:\n",
      "[[0.39993665 0.30251428 0.30961365]\n",
      " [0.31085    0.4058126  0.29215118]\n",
      " [0.2892133  0.29167318 0.39823523]]\n",
      "[[0.39993665 0.30251428 0.30961365]\n",
      " [0.31085    0.4058126  0.29215118]\n",
      " [0.2892133  0.29167318 0.39823523]]\n",
      "forward loss\n",
      "Run 07/10: test acc = 94.27%\n",
      "mps: 7 steps -> 10.81 sec | avg 1544.7 ms/step\n",
      "estimating matrix\n",
      "[Warm-up] training base classifier for 3 epochs...\n",
      "  done epoch 1/3\n",
      "  done epoch 2/3\n",
      "  done epoch 3/3\n",
      "[Optimization] refining transition matrix for 5 epochs...\n",
      "Epoch 1/5 | loss=1.09859 | grad=3.33327e-01\n",
      "Epoch 2/5 | loss=1.08370 | grad=3.28368e-01\n",
      "Epoch 3/5 | loss=1.06904 | grad=3.23587e-01\n",
      "Epoch 4/5 | loss=1.05460 | grad=3.18947e-01\n",
      "Epoch 5/5 | loss=1.04038 | grad=3.14442e-01\n",
      "\n",
      "[Done] Refined Transition Matrix:\n",
      "[[0.38057315 0.33333328 0.29211906]\n",
      " [0.31352296 0.3333332  0.3470379 ]\n",
      " [0.3059039  0.3333335  0.360843  ]]\n",
      "[[0.38057315 0.33333328 0.29211906]\n",
      " [0.31352296 0.3333332  0.3470379 ]\n",
      " [0.3059039  0.3333335  0.360843  ]]\n",
      "forward loss\n",
      "Run 08/10: test acc = 80.87%\n",
      "mps: 8 steps -> 10.87 sec | avg 1358.5 ms/step\n",
      "estimating matrix\n",
      "[Warm-up] training base classifier for 3 epochs...\n",
      "  done epoch 1/3\n",
      "  done epoch 2/3\n",
      "  done epoch 3/3\n",
      "[Optimization] refining transition matrix for 5 epochs...\n",
      "Epoch 1/5 | loss=1.09887 | grad=3.33495e-01\n",
      "Epoch 2/5 | loss=1.08398 | grad=3.28529e-01\n",
      "Epoch 3/5 | loss=1.06931 | grad=3.23742e-01\n",
      "Epoch 4/5 | loss=1.05486 | grad=3.19095e-01\n",
      "Epoch 5/5 | loss=1.04064 | grad=3.14585e-01\n",
      "\n",
      "[Done] Refined Transition Matrix:\n",
      "[[0.33333275 0.31524315 0.33333275]\n",
      " [0.33333433 0.36240685 0.33333433]\n",
      " [0.333333   0.32235003 0.333333  ]]\n",
      "[[0.33333275 0.31524315 0.33333275]\n",
      " [0.33333433 0.36240685 0.33333433]\n",
      " [0.333333   0.32235003 0.333333  ]]\n",
      "forward loss\n",
      "Run 09/10: test acc = 58.70%\n",
      "mps: 9 steps -> 11.53 sec | avg 1280.7 ms/step\n",
      "estimating matrix\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 7\u001b[39m\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m r \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(cfg[\u001b[33m'\u001b[39m\u001b[33mruns\u001b[39m\u001b[33m'\u001b[39m]):\n\u001b[32m      6\u001b[39m     start = time.perf_counter()\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m     acc, T = \u001b[43mrun_once\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1000\u001b[39;49m\u001b[43m+\u001b[49m\u001b[43mr\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      8\u001b[39m     all_acc.append(acc)\n\u001b[32m      9\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m cfg[\u001b[33m'\u001b[39m\u001b[33mestimate_T\u001b[39m\u001b[33m'\u001b[39m]:\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 21\u001b[39m, in \u001b[36mrun_once\u001b[39m\u001b[34m(args, seed)\u001b[39m\n\u001b[32m     19\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m'\u001b[39m\u001b[33mestimating matrix\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m     20\u001b[39m T = np.zeros((\u001b[32m3\u001b[39m, \u001b[32m3\u001b[39m), dtype=np.float64)\n\u001b[32m---> \u001b[39m\u001b[32m21\u001b[39m T = \u001b[43mestimate_transition_anchor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mT\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_cifar\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m=\u001b[49m\u001b[43margs\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mest_epochs\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     22\u001b[39m T = estimate_transition_trevision(T, train_loader, is_cifar, device=device, epochs=args[\u001b[33m'\u001b[39m\u001b[33mest_epochs\u001b[39m\u001b[33m'\u001b[39m])\n\u001b[32m     23\u001b[39m \u001b[38;5;28mprint\u001b[39m(T)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 33\u001b[39m, in \u001b[36mestimate_transition_anchor\u001b[39m\u001b[34m(t, train_loader, is_cifar, num_classes, device, epochs)\u001b[39m\n\u001b[32m     31\u001b[39m \u001b[38;5;66;03m# quick warmup training on noisy labels\u001b[39;00m\n\u001b[32m     32\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epochs):\n\u001b[32m---> \u001b[39m\u001b[32m33\u001b[39m     \u001b[43mtrain_one_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# default CE inside\u001b[39;00m\n\u001b[32m     35\u001b[39m \u001b[38;5;66;03m# collect probs & noisy labels\u001b[39;00m\n\u001b[32m     36\u001b[39m p_arr, y_noisy = predict_proba(model, train_loader, device)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 25\u001b[39m, in \u001b[36mtrain_one_epoch\u001b[39m\u001b[34m(model, loader, optimizer, criterion, device)\u001b[39m\n\u001b[32m     23\u001b[39m     loss.backward()\n\u001b[32m     24\u001b[39m     optimizer.step()\n\u001b[32m---> \u001b[39m\u001b[32m25\u001b[39m     total_loss += \u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m * yb.size(\u001b[32m0\u001b[39m)\n\u001b[32m     26\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m total_loss / \u001b[38;5;28mlen\u001b[39m(loader.dataset)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "for cfg in configs:\n",
    "    all_acc = []\n",
    "    last_T = None\n",
    "    t_arr = []\n",
    "    for r in range(cfg['runs']):\n",
    "        start = time.perf_counter()\n",
    "        acc, T = run_once(cfg, seed=1000+r)\n",
    "        all_acc.append(acc)\n",
    "        if cfg['estimate_T']:\n",
    "            t_arr.append(T)\n",
    "        last_T = T if T is not None else last_T\n",
    "        print(f\"Run {r+1:02d}/{cfg['runs']}: test acc = {acc*100:.2f}%\")\n",
    "        end = time.perf_counter()\n",
    "        print(f\"{cfg['device']}: {r+1} steps -> {end - start:.2f} sec | avg {1000*(end - start)/(r+1):.1f} ms/step\")\n",
    "    mean = float(np.mean(all_acc))\n",
    "    std  = float(np.std(all_acc))\n",
    "\n",
    "    summary = {\n",
    "        'dataset': cfg['dataset'],\n",
    "        'loss': cfg['loss'],\n",
    "        'estimate_T': bool(cfg['estimate_T']),\n",
    "        'epochs': cfg['epochs'],\n",
    "        'runs': cfg['runs'],\n",
    "        'mean_test_acc': mean,\n",
    "        'std_test_acc': std,\n",
    "        'last_estimated_T': last_T,\n",
    "        't_arr':t_arr,\n",
    "        'per_run_acc': all_acc,\n",
    "    }\n",
    "    print(\"=\"*72)\n",
    "    print(f\"{cfg['dataset']} | {cfg['loss']} | mean±std over {cfg['runs']} runs: {mean*100:.2f}±{std*100:.2f}%\")\n",
    "\n",
    "    with open(cfg['out'], 'w') as f:\n",
    "        json.dump(summary, f, indent=2)\n",
    "    print(f\"Saved summary to {cfg['out']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[[6.76716268e-01 9.52430071e-07 3.13660324e-01]\n",
      " [2.89468050e-01 6.80009067e-01 9.52428707e-07]\n",
      " [3.38155814e-02 3.19989949e-01 6.86338723e-01]]\n",
      "[[0.7 0.3 0. ]\n",
      " [0.  0.7 0.3]\n",
      " [0.3 0.  0.7]]\n",
      "Fro error: 0.732495419457479\n",
      "rre error: 0.5553033406514764\n",
      "mae error: 0.20513741989448894\n",
      "Per-row correlations: [np.float64(0.6036007048179056), np.float64(0.637690711506016), np.float64(0.6262318010639769)]\n",
      "Mean: 0.6225077391292995\n",
      "\n",
      "[[0.33333215 0.30232149 0.33333215]\n",
      " [0.33333504 0.38071254 0.33333504]\n",
      " [0.33333275 0.316966   0.33333275]]\n",
      "[[0.3 0.4 0.3]\n",
      " [0.4 0.3 0.3]\n",
      " [0.3 0.3 0.4]]\n",
      "Fro error: 0.17226882406525493\n",
      "rre error: 0.17057154869594465\n",
      "mae error: 0.05133569902843899\n",
      "Per-row correlations: [np.float64(-1.0), np.float64(-0.4999999999999999), np.float64(0.5)]\n",
      "Mean: -0.3333333333333333\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "\n",
    "\n",
    "# pattern for files starting with \"name\" and ending with \".json\"\n",
    "files = sorted(glob.glob(\"fashion03*.json\"))\n",
    "\n",
    "# pick the first matching file\n",
    "first_file = files[0]\n",
    "print(\"Loading:\", first_file)\n",
    "\n",
    "# load the JSON contents\n",
    "with open(first_file, \"r\", encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "T_prime = np.array(data['last_estimated_T'])\n",
    "T_true = pick_known_T('fashion03')\n",
    "\n",
    "#checking recreation performance\n",
    "print(T_prime)\n",
    "print(T_true)\n",
    "print(f\"Fro error: {np.linalg.norm(T_prime - T_true, 'fro')}\")\n",
    "print(f\"rre error: {np.linalg.norm(T_prime - T_true, 'fro') / np.linalg.norm(T_true, 'fro')}\")\n",
    "print(f\"mae error: {np.mean(np.abs(T_prime - T_true))}\")\n",
    "import scipy.stats as st\n",
    "\n",
    "corrs = [st.pearsonr(T_true[i], T_prime[i])[0] for i in range(C)]\n",
    "print(\"Per-row correlations:\", corrs)\n",
    "print(\"Mean:\", np.mean(corrs))\n",
    "\n",
    "\n",
    "# pattern for files starting with \"name\" and ending with \".json\"\n",
    "files = sorted(glob.glob(\"fashion06*.json\"))\n",
    "\n",
    "# pick the first matching file\n",
    "first_file = files[0]\n",
    "print(\"Loading:\", first_file)\n",
    "\n",
    "print()\n",
    "T_prime = np.array(data['last_estimated_T'])\n",
    "T_true = pick_known_T('fashion06')\n",
    "\n",
    "#checking recreation performance\n",
    "print(T_prime)\n",
    "print(T_true)\n",
    "print(f\"Fro error: {np.linalg.norm(T_prime - T_true, 'fro')}\")\n",
    "print(f\"rre error: {np.linalg.norm(T_prime - T_true, 'fro') / np.linalg.norm(T_true, 'fro')}\")\n",
    "print(f\"mae error: {np.mean(np.abs(T_prime - T_true))}\")\n",
    "import scipy.stats as st\n",
    "\n",
    "corrs = [st.pearsonr(T_true[i], T_prime[i])[0] for i in range(C)]\n",
    "print(\"Per-row correlations:\", corrs)\n",
    "print(\"Mean:\", np.mean(corrs))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
