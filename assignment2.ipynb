{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9IrXtNd8l-9H"
   },
   "source": [
    "## COMP5328 - Advanced Machine Learning\n",
    "## Assignment 2: Title\n",
    "----------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Common imports\n",
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import json\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "# Ploting\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import os\n",
    "import torch.optim as optim\n",
    "import random\n",
    "import scipy.stats as st"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment variables\n",
    "# Common\n",
    "num_classes=3\n",
    "dataset_folder = 'data/'\n",
    "cifar_dataset = dataset_folder+'CIFAR.npz'\n",
    "MNISTO3_dataset = dataset_folder+'FashionMNIST0.3.npz'\n",
    "MNISTO6_dataset = dataset_folder+'FashionMNIST0.6.npz'\n",
    "\n",
    "\n",
    "DATA_PATHS = {\n",
    "    'fashion03': MNISTO3_dataset,\n",
    "    'fashion06': MNISTO6_dataset,\n",
    "    'cifar':     cifar_dataset\n",
    "}\n",
    "\n",
    "losses = ['forward','gce', 'forwardGCE']\n",
    "datasets = ['cifar', \"fashion03\", \"fashion06\"]\n",
    "base = {\n",
    "    \"runs\":10,\n",
    "    \"epochs\": 15,\n",
    "    \"loss\":'forward',\n",
    "    \"batch_size\":4096,\n",
    "    \"q\":0.6,\n",
    "    \"est_epochs\":10,\n",
    "    \"beta\":0.2,\n",
    "    \"lr\":1e-3,\n",
    "    \"device\":'mps'\n",
    "}\n",
    "\n",
    "\n",
    "known_T_fashion_03 = np.array(  [[0.7,0.3,0.0],\n",
    "                                [0.0,0.7,0.3],\n",
    "                                [0.3,0.0,0.7]], dtype=np.float32)\n",
    "\n",
    "known_T_fashion_06 = np.array(  [[0.4,0.3,0.3],\n",
    "                                [0.3,0.4,0.3],\n",
    "                                [0.3,0.3,0.4]], dtype=np.float32)\n",
    "\n",
    "def pick_known_T(tag):\n",
    "    if tag == 'fashion03':\n",
    "        return known_T_fashion_03\n",
    "    elif tag == 'fashion06':\n",
    "        return known_T_fashion_06\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def set_seed(seed=0):\n",
    "    import random\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "def sanity(T, name, dataset):\n",
    "    print(f\"\\n{name}\")\n",
    "    print(\"row sums:\", T.sum(axis=1))\n",
    "    print(\"col sums:\", T.sum(axis=0))\n",
    "    if dataset == 'fashion03':\n",
    "        T_true = known_T_fashion_03\n",
    "    elif dataset == 'fashion06':\n",
    "        T_true = known_T_fashion_06\n",
    "    if dataset != 'cifar':\n",
    "        # if you know T_true for this dataset:\n",
    "        print(\"Fro:\", np.linalg.norm(T - T_true, 'fro'))\n",
    "        print(\"MAE:\", np.mean(np.abs(T - T_true)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qYzWuiytl-9I"
   },
   "source": [
    "## 1. Load Dataset\n",
    "\n",
    "### 1.0 Data Folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unzip:  cannot find or open datasets.zip, datasets.zip.zip or datasets.zip.ZIP.\n"
     ]
    }
   ],
   "source": [
    "# Path to your dataset zip stored in Drive\n",
    "zip_path = \"datasets.zip\"\n",
    "\n",
    "# Unzip file\n",
    "!unzip -o -q \"$zip_path\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OxEr9ihznSqa",
    "outputId": "8b9fd2c9-b2f9-45f4-94aa-d0d473b7e140"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 172688\n",
      "-rw-r--r--@ 1 jamie.saunders  staff  55440974 Oct  4  2019 CIFAR.npz\n",
      "-rw-r--r--@ 1 jamie.saunders  staff  16485974 Oct 10  2021 FashionMNIST0.3.npz\n",
      "-rw-r--r--@ 1 jamie.saunders  staff  16485974 Oct 10  2021 FashionMNIST0.6.npz\n"
     ]
    }
   ],
   "source": [
    "# The structure of data folder.\n",
    "!ls -l data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_npz(path):\n",
    "    d = np.load(path)\n",
    "    Xtr, Str = d['Xtr'], d['Str']\n",
    "    Xts, Yts = d['Xts'], d['Yts']\n",
    "    return Xtr, Str, Xts, Yts\n",
    "\n",
    "# A helper class, it is used as an input of the DataLoader object.\n",
    "class DatasetArray(Dataset):\n",
    "    def __init__(self, data, labels=None, transform=None):\n",
    "        if labels != None:\n",
    "            self.data_arr = np.asarray(data).astype(np.float32)\n",
    "            self.label_arr = np.asarray(labels).astype(np.long)\n",
    "        else:\n",
    "            tmp_arr = np.asarray(data)\n",
    "            self.data_arr = tmp_arr[:,:-1].astype(np.float32)\n",
    "            self.label_arr = tmp_arr[:,-1].astype(np.long)\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data_arr)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "     \n",
    "        data = self.data_arr[index]\n",
    "        label = self.label_arr[index]\n",
    "        \n",
    "        if self.transform is not None:\n",
    "            data = self.transform(data)\n",
    "            \n",
    "        return (data, label)\n",
    "    \n",
    "    \n",
    "# Splitting the data into three parts.\n",
    "def train_val_test_random_split(data, fracs=[0.7,0.1,0.2]):\n",
    "    r\"\"\"Split the data into training, validation and test set.\n",
    "    Args:\n",
    "        fracs: a list of length three\n",
    "    \"\"\"\n",
    "    assert len(fracs) == 3\n",
    "    assert sum(fracs) == 1\n",
    "    assert all(frac > 0 for frac in fracs)\n",
    "    n = len(data)\n",
    "    subset_lens = [int(n*frac) for frac in fracs]\n",
    "    idxs = list(range(n))\n",
    "    random.shuffle(idxs)\n",
    "    data = np.array(data)\n",
    "    new_data = []\n",
    "    start_idx = 0\n",
    "    for subset_len in subset_lens:\n",
    "        end_idx = start_idx + subset_len\n",
    "        cur_idxs = idxs[start_idx:end_idx]\n",
    "        new_data.append(data[cur_idxs,:].tolist())\n",
    "        start_idx = end_idx\n",
    "    return new_data\n",
    "\n",
    "# Preparation of the data for training, validation and testing a pytorch network. \n",
    "# Note that the test data is not in use for this lab.\n",
    "def get_loader(batch_size =128, num_workers = 0, train_val_test_split = [0.7,0.1,0.2], data=None):\n",
    "    r\"\"\"This function is used to read the data file and split the data into three subsets, i.e, \n",
    "    train data, validation data and test data. Their corresponding DataLoader objects are returned.\"\"\"\n",
    "    \n",
    "    [train_data, val_data, test_data] = train_val_test_random_split(data, fracs = train_val_test_split)\n",
    "\n",
    "    train_data = DatasetArray(data = train_data)\n",
    "    val_data = DatasetArray(data = val_data)\n",
    "    test_data = DatasetArray(data = test_data)\n",
    "\n",
    "    #The pytorch built-in class DataLoader can help us to shuffle the data, draw mini-batch,\n",
    "    #do transformations, etc. \n",
    "    train_loader = DataLoader(\n",
    "        train_data,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=num_workers,\n",
    "    )\n",
    "\n",
    "    val_loader = DataLoader(\n",
    "        val_data,\n",
    "        batch_size=100,\n",
    "        shuffle=False,\n",
    "        num_workers=num_workers,\n",
    "    )\n",
    "\n",
    "    test_loader = DataLoader(\n",
    "        test_data,\n",
    "        batch_size=100,\n",
    "        num_workers=num_workers,\n",
    "        shuffle=False,\n",
    "    )\n",
    "    return train_loader, val_loader, test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NpzDataset(Dataset):\n",
    "    def __init__(self, X, y, is_cifar=False):\n",
    "        self.X = X.astype(np.float32)\n",
    "        self.y = y.astype(np.int64)\n",
    "        self.is_cifar = is_cifar\n",
    "\n",
    "        # Normalize to [0,1]\n",
    "        self.X = self.X / 255.0 if self.X.max() > 1.0 else self.X\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x = self.X[idx]\n",
    "        if x.ndim == 1:\n",
    "            # flat; try to infer shape 28x28 or 32x32x3\n",
    "            if x.size == 28*28:\n",
    "                x = x.reshape(1, 28, 28)\n",
    "            elif x.size == 32*32*3:\n",
    "                x = x.reshape(3, 32, 32)\n",
    "            else:\n",
    "                raise ValueError(\"Unknown flat image shape: {}\".format(x.shape))\n",
    "        else:\n",
    "            # (H,W) or (H,W,C)\n",
    "            if x.ndim == 2:\n",
    "                x = x[None, ...]  # to (1,H,W)\n",
    "            elif x.ndim == 3:\n",
    "                # assume HWC -> CHW\n",
    "                x = np.transpose(x, (2, 0, 1))\n",
    "            else:\n",
    "                raise ValueError(f\"Unexpected image dims: {x.shape}\")\n",
    "        return torch.from_numpy(x), torch.tensor(self.y[idx])\n",
    "\n",
    "\n",
    "def load_npz(path):\n",
    "    d = np.load(path)\n",
    "    Xtr, Str = d['Xtr'], d['Str']\n",
    "    Xts, Yts = d['Xts'], d['Yts']\n",
    "    return Xtr, Str, Xts, Yts\n",
    "\n",
    "\n",
    "def make_loaders(Xtr, Str, batch_size=128, seed=0, test_size=0.2):\n",
    "    # 80/20 split each repetition\n",
    "    X_tr, X_val, y_tr, y_val = train_test_split(\n",
    "        Xtr, Str, test_size=test_size, random_state=seed, stratify=Str\n",
    "    )\n",
    "\n",
    "    is_cifar = (X_tr.shape[-1] == 3) if X_tr.ndim == 4 else (X_tr.shape[-1] == 32*32*3)\n",
    "\n",
    "    train_ds = NpzDataset(X_tr, y_tr, is_cifar)\n",
    "    val_ds   = NpzDataset(X_val, y_val, is_cifar)\n",
    "\n",
    "    train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "    val_loader   = DataLoader(val_ds,   batch_size=batch_size, shuffle=False, num_workers=0)\n",
    "\n",
    "    return train_loader, val_loader, is_cifar\n",
    "\n",
    "def make_test_loader(Xts, Yts, batch_size=256):\n",
    "    is_cifar = (Xts.shape[-1] == 3) if Xts.ndim == 4 else (Xts.shape[-1] == 32*32*3)\n",
    "    test_ds = NpzDataset(Xts, Yts, is_cifar)\n",
    "    return DataLoader(test_ds, batch_size=batch_size, shuffle=False, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ForwardCorrectedCE(nn.Module):\n",
    "    \"\"\"\n",
    "    Forward loss correction: minimizes CE between T^T p and noisy labels.\n",
    "    T: class-transition matrix where T[i,j] = P(S=j | Y=i). Shape [C,C].\n",
    "    \"\"\"\n",
    "    def __init__(self, T):\n",
    "        super().__init__()\n",
    "        self.register_buffer('T', T)  # [C,C]\n",
    "\n",
    "    def forward(self, logits, y_noisy):\n",
    "        # logits -> p(y|x)\n",
    "        p = F.softmax(logits, dim=1)  # [B,C]\n",
    "        # mix via T^T\n",
    "        mixed = torch.clamp(p @ self.T.t(), 1e-6, 1.0)\n",
    "        log_mixed = torch.log(mixed)\n",
    "        return F.nll_loss(log_mixed, y_noisy)\n",
    "\n",
    "class ForwardCorrectedGCE(nn.Module):\n",
    "    def __init__(self, T, q=0.7):\n",
    "        super().__init__()\n",
    "        self.register_buffer('T', T)\n",
    "        self.q = q\n",
    "    def forward(self, logits, y_noisy):\n",
    "        p_noisy = torch.clamp(F.softmax(logits,1) @ self.T, 1e-6, 1.0)\n",
    "        p_s = p_noisy.gather(1, y_noisy.view(-1,1)).clamp(1e-6,1.0)\n",
    "        return (-(p_s.log()) if self.q==1.0 else (1 - p_s**self.q)/self.q).mean()\n",
    "\n",
    "\n",
    "class GeneralizedCrossEntropy(nn.Module):\n",
    "    \"\"\"\n",
    "    GCE loss: L_q(p, y) = (1 - p_y^q) / q, with q in (0,1].\n",
    "    q→1 recovers CE; smaller q is more robust to label noise.\n",
    "    \"\"\"\n",
    "    def __init__(self, q=0.7):\n",
    "        super().__init__()\n",
    "        assert 0 < q <= 1\n",
    "        self.q = q\n",
    "\n",
    "    def forward(self, logits, y):\n",
    "        p = F.softmax(logits, dim=1)\n",
    "        p_y = p.gather(1, y.view(-1,1)).clamp(min=1e-6, max=1.0)\n",
    "        if self.q == 1.0:\n",
    "            return -torch.log(p_y).mean()\n",
    "        return ((1 - p_y.pow(self.q)) / self.q).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_block(cin, cout):\n",
    "    return nn.Sequential(\n",
    "        nn.Conv2d(cin, cout, 3, padding=1),\n",
    "        nn.BatchNorm2d(cout),\n",
    "        nn.ReLU(inplace=True),\n",
    "        nn.MaxPool2d(2)\n",
    "    )\n",
    "\n",
    "class SmallCNN28(nn.Module):\n",
    "    \"\"\"For 1×28×28 images.\"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            conv_block(1, 32),  # 14x14\n",
    "            conv_block(32, 64), # 7x7\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(64*7*7, 128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(128, 3)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "\n",
    "class SmallCNNCifar(nn.Module):\n",
    "    \"\"\"For 3×32×32 images.\"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            conv_block(3, 32),   # 16x16\n",
    "            conv_block(32, 64),  # 8x8\n",
    "            conv_block(64, 128), # 4x4\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(128*4*4, 256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(256, 3)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "\n",
    "def make_model(is_cifar):\n",
    "    return SmallCNNCifar() if is_cifar else SmallCNN28()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def predict_proba(model, loader, device):\n",
    "    model.eval()\n",
    "    p_arr = []\n",
    "    ys = []\n",
    "    for xb, yb in loader:\n",
    "        xb = xb.to(device)\n",
    "        output = model(xb)\n",
    "        p = F.softmax(output, dim=1).cpu().numpy()\n",
    "        p_arr.append(p)\n",
    "        ys.append(yb.numpy())\n",
    "    return np.concatenate(p_arr), np.concatenate(ys)\n",
    "\n",
    "\n",
    "def estimate_transition_anchor(t, train_loader, is_cifar, q, device='cpu', epochs=5):\n",
    "    \"\"\"\n",
    "    Simple anchor/confident-example estimator (Patrini et al., 2017 style):\n",
    "    1) Train a base classifier on noisy data.\n",
    "    2) Get p(y|x) on training set.\n",
    "    3) For each clean class i, find indices whose predicted argmax == noisy label == i and with high confidence.\n",
    "    4) For those indices, estimate column i of T as average of empirical noisy label distribution given model predicts i.\n",
    "    Here: since we only have noisy labels S, we approximate T[:, i] ≈ E[ onehot(S) | argmax p = i, p_i >= τ ].\n",
    "    Normalize columns to sum to 1.\n",
    "    \"\"\"\n",
    "\n",
    "    device = torch.device(device)\n",
    "    model = make_model(is_cifar).to(device)\n",
    "\n",
    "    optimizer = optim.Adam(model.parameters(), lr=1e-3, weight_decay=1e-4)\n",
    "    criterion = GeneralizedCrossEntropy(q=q)\n",
    "    # quick warmup training on noisy labels\n",
    "    for _ in range(epochs):\n",
    "        train_one_epoch(model, train_loader, optimizer, criterion, device=device) \n",
    "\n",
    "    # collect probs & noisy labels\n",
    "    p_arr, y_noisy = predict_proba(model, train_loader, device)\n",
    "    preds = p_arr.argmax(axis=1)\n",
    "    maxp = p_arr.max(axis=1)\n",
    "\n",
    "    C = num_classes\n",
    "    T = t\n",
    "    # choose class-wise thresholds based on quantiles for stability\n",
    "    for i in range(C):\n",
    "        idx = np.where(preds == i)[0]\n",
    "        if idx.size == 0:\n",
    "            T[:, i] = np.ones(C) / C\n",
    "            continue\n",
    "        # high-confidence subset (top 30% by p_i)\n",
    "        conf = maxp[idx]\n",
    "        if conf.size > 50:\n",
    "            tau = np.quantile(conf, 0.7)\n",
    "        else:\n",
    "            tau = np.min(conf)  # keep all if tiny\n",
    "        keep = idx[conf >= tau]\n",
    "        if keep.size == 0:\n",
    "            keep = idx\n",
    "        # empirical distribution of noisy labels in this confident set\n",
    "        hist = np.bincount(y_noisy[keep], minlength=C).astype(np.float64)\n",
    "        if hist.sum() == 0:\n",
    "            T[:, i] = np.ones(C) / C\n",
    "        else:\n",
    "            T[:, i] = hist / hist.sum()\n",
    "\n",
    "    # column-normalize\n",
    "    colsum = T.sum(axis=0, keepdims=True)\n",
    "    T = np.divide(T, np.maximum(colsum, 1e-8))\n",
    "    return T.astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_transition_trevision(T_init, train_loader, is_cifar, q, device=\"cpu\", epochs=5, lambda_reg=1e-4, lr_t=5e-3, lr_model=1e-3, warmup_epochs=3, log_every=1\n",
    "):\n",
    "    \"\"\"\n",
    "    Refine transition matrix using T-Revision method (Patrini et al. style).\n",
    "\n",
    "    Args:\n",
    "        T_init (np.ndarray or torch.Tensor): initial transition matrix [C, C]\n",
    "        train_loader: noisy dataloader (x, y_noisy)\n",
    "        is_cifar (bool): dataset selector for model architecture\n",
    "        device (str): 'cpu', 'cuda', or 'mps'\n",
    "        epochs (int): number of refinement epochs for ΔT\n",
    "        lambda_reg (float): regularization to keep T close to T_init\n",
    "        lr_t (float): learning rate for ΔT\n",
    "        lr_model (float): learning rate for model warm-up\n",
    "        warmup_epochs (int): number of model warm-up epochs\n",
    "        log_every (int): print interval\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: refined transition matrix [C, C]\n",
    "    \"\"\"\n",
    "\n",
    "    device = torch.device(device)\n",
    "    C = 3\n",
    "\n",
    "    # ---------------------------\n",
    "    # 1. Base model setup\n",
    "    # ---------------------------\n",
    "    model = make_model(is_cifar).to(device)\n",
    "    opt_model = optim.Adam(model.parameters(), lr=lr_model, weight_decay=1e-4)\n",
    "    criterion = GeneralizedCrossEntropy(q=q)\n",
    "\n",
    "    # Warm-up (train classifier on noisy labels)\n",
    "    #print(f\"[Warm-up] training base classifier for {warmup_epochs} epochs...\")\n",
    "    for e in range(warmup_epochs):\n",
    "        train_one_epoch(model, train_loader, opt_model, criterion=criterion, device=device)\n",
    "        #print(f\"  done epoch {e+1}/{warmup_epochs}\")\n",
    "\n",
    "    # ---------------------------\n",
    "    # 2. Get predicted probabilities\n",
    "    # ---------------------------\n",
    "    probs, y_noisy = predict_proba(model, train_loader, device)\n",
    "    p = torch.tensor(probs, dtype=torch.float32, device=device)\n",
    "    y_t = torch.tensor(y_noisy, dtype=torch.long, device=device)\n",
    "\n",
    "    # ---------------------------\n",
    "    # 3. Initialize learnable ΔT (T-Revision)\n",
    "    # ---------------------------\n",
    "    T_init_torch = torch.tensor(T_init, dtype=torch.float32, device=device)\n",
    "    delta_T = nn.Parameter(torch.zeros_like(T_init_torch))\n",
    "    optimizer_T = optim.Adam([delta_T], lr=lr_t)\n",
    "\n",
    "    #print(f\"[Optimization] refining transition matrix for {epochs} epochs...\")\n",
    "\n",
    "    # ---------------------------\n",
    "    # 4. Optimize ΔT\n",
    "    # ---------------------------\n",
    "    for ep in range(epochs):\n",
    "        optimizer_T.zero_grad()\n",
    "\n",
    "        # Proposed transition\n",
    "        T_prime = T_init_torch + delta_T\n",
    "        T_prime = torch.clamp(T_prime, min=1e-6)\n",
    "\n",
    "        # Forward correction: p(y_noisy | x) = p(y|x) * T'\n",
    "        noisy_pred = torch.clamp(p @ T_prime.t(), 1e-6, 1.0)\n",
    "        log_noisy = torch.log(noisy_pred)\n",
    "\n",
    "        # Loss = NLL + regularization\n",
    "        loss_ce = nn.NLLLoss()(log_noisy, y_t)\n",
    "        reg = lambda_reg * torch.norm(T_prime - T_init_torch, p=\"fro\")\n",
    "        loss = loss_ce + reg\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer_T.step()\n",
    "\n",
    "        if (ep + 1) % log_every == 0:\n",
    "            grad_norm = delta_T.grad.abs().mean().item() if delta_T.grad is not None else 0\n",
    "            #print(f\"Epoch {ep+1}/{epochs} | loss={loss.item():.5f} | grad={grad_norm:.5e}\")\n",
    "\n",
    "    # ---------------------------\n",
    "    # 5. Normalize once at the end\n",
    "    # ---------------------------\n",
    "    T_final = T_init_torch + delta_T.data\n",
    "    T_final = torch.clamp(T_final, min=1e-6)\n",
    "    T_final = T_final / T_final.sum(dim=0, keepdim=True)\n",
    "\n",
    "    #print(\"\\n[Done] Refined Transition Matrix:\")\n",
    "    #print(T_final.detach().cpu().numpy())\n",
    "\n",
    "    return T_final.detach().cpu().numpy().astype(np.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def accuracy(model, loader, device):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for xb, yb in loader:\n",
    "        xb, yb = xb.to(device), yb.to(device)\n",
    "        logits = model(xb)\n",
    "        pred = logits.argmax(dim=1)\n",
    "        correct += (pred == yb).sum().item()\n",
    "        total += yb.numel()\n",
    "    return correct / max(total, 1)\n",
    "\n",
    "\n",
    "def train_one_epoch(model, loader, optimizer, criterion, device='mps'):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    for xb, yb in loader:\n",
    "        xb, yb = xb.to(device), yb.to(device)\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        logits = model(xb)\n",
    "        loss = criterion(logits, yb)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item() * yb.size(0)\n",
    "    return total_loss / len(loader.dataset)\n",
    "\n",
    "\n",
    "def fit_model(model, train_loader, val_loader, device, loss_name='gce', T=None, q=0.7, beta=0.2, epochs=10, lr=1e-3):\n",
    "\n",
    "    if loss_name == 'forward':\n",
    "        assert T is not None, \"Forward correction requires known/estimated T\"\n",
    "        print('forward loss')\n",
    "        criterion = ForwardCorrectedCE(torch.tensor(T, dtype=torch.float32, device=device))\n",
    "    elif loss_name == 'forwardGCE':\n",
    "        print('forwardGCE loss')\n",
    "        criterion = ForwardCorrectedGCE(q=q)\n",
    "    else:\n",
    "        print('GCE loss')\n",
    "        criterion = GeneralizedCrossEntropy(q=q)\n",
    "\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=1e-4)\n",
    "\n",
    "    best_val = -np.inf\n",
    "    best_state = None\n",
    "\n",
    "    for _ in range(epochs):\n",
    "        train_one_epoch(model, train_loader, optimizer, criterion, device)\n",
    "        # early stopping on val accuracy (cheap)\n",
    "        val_acc = accuracy(model, val_loader, device)\n",
    "        if val_acc > best_val:\n",
    "            best_val = val_acc\n",
    "            best_state = {k: v.detach().cpu().clone() for k, v in model.state_dict().items()}\n",
    "\n",
    "    if best_state is not None:\n",
    "        model.load_state_dict(best_state)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_once(args, seed):\n",
    "    set_seed(seed)\n",
    "\n",
    "    if args['dataset']=='fashion03':\n",
    "        q = 0.3\n",
    "    elif args['dataset']=='fashion06':\n",
    "        q = 0.6\n",
    "    else:\n",
    "        q = 0.7\n",
    "    device = torch.device('mps')\n",
    "    print(f'q:{q}')\n",
    "    # load data\n",
    "    Xtr, Str, Xts, Yts = load_npz(DATA_PATHS[args['dataset']])\n",
    "\n",
    "    # loaders for this split\n",
    "    train_loader, val_loader, is_cifar = make_loaders(Xtr, Str, batch_size=args['batch_size'], seed=seed)\n",
    "    test_loader = make_test_loader(Xts, Yts, batch_size=512)\n",
    "  \n",
    "    # choose model\n",
    "    model = make_model(is_cifar).to(device)\n",
    "\n",
    "    # Transition matrix\n",
    "    T = None\n",
    "    if args['loss'] == 'forward' or args['loss'] == 'gce':\n",
    "        if args['estimate_T'] or args['dataset']=='cifar':\n",
    "            T = np.zeros((3, 3), dtype=np.float64)\n",
    "            T = estimate_transition_anchor(T, train_loader, is_cifar, q, device=device, epochs=args['est_epochs'])\n",
    "            sanity(T, 'Est T After Anchor point', args['dataset'])\n",
    "            T = estimate_transition_trevision(T, train_loader, is_cifar, q, device=device, epochs=args['est_epochs'])\n",
    "            sanity(T, 'Est T After T Revision', args['dataset'])\n",
    "        else:\n",
    "            T = pick_known_T(args['dataset'])\n",
    "            if T is None:\n",
    "                raise ValueError(\"Forward loss selected but no known T for this dataset; use --estimate_T.\")\n",
    "\n",
    "    # fit\n",
    "    model = fit_model(\n",
    "        model,\n",
    "        train_loader,\n",
    "        val_loader,\n",
    "        device,\n",
    "        loss_name=args['loss'],\n",
    "        T=T,\n",
    "        q=q,\n",
    "        beta=args['beta'],\n",
    "        epochs=args['epochs'],\n",
    "        lr=args['lr'],\n",
    "    )\n",
    "\n",
    "    # evaluate on clean test set\n",
    "    test_acc = accuracy(model, test_loader, device)\n",
    "    return float(test_acc), (T.tolist() if T is not None else None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset: fashion03, estimate_T:True, loss:forward\n",
      "dataset: fashion03, estimate_T:False, loss:forward\n",
      "dataset: fashion03, estimate_T:True, loss:gce\n",
      "dataset: fashion03, estimate_T:False, loss:gce\n",
      "dataset: fashion03, estimate_T:True, loss:forwardGCE\n",
      "dataset: fashion03, estimate_T:False, loss:forwardGCE\n"
     ]
    }
   ],
   "source": [
    "now = datetime.now()\n",
    "now = now.strftime(\"%Y-%m-%d-%H:%M\")\n",
    "folder = \"results\"+now\n",
    "if os.path.exists(folder) and os.path.isdir(folder):\n",
    "    os.rmdir(folder)\n",
    "    os.mkdir(folder)\n",
    "else:\n",
    "    os.mkdir(folder)\n",
    "datasets = ['fashion03']\n",
    "# create each cfg\n",
    "estimate = [True, False]\n",
    "configs = []\n",
    "for i, ds in enumerate(datasets):\n",
    "    for loss in losses:\n",
    "        for t in estimate:\n",
    "            cfg = {**base, \"dataset\": ds, \"out\": folder+'/'+ds+'_'+loss+'_'+str(t)+'_'+now+'.json', \"loss\":loss, \"estimate_T\":t}\n",
    "            if t and ds !='cifar':\n",
    "                print(f\"dataset: {ds}, estimate_T:{t}, loss:{loss}\")\n",
    "                configs.append(cfg)\n",
    "            elif not t:\n",
    "                print(f\"dataset: {ds}, estimate_T:{t}, loss:{loss}\")\n",
    "                configs.append(cfg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "q:0.3\n",
      "\n",
      "Est T After Anchor point\n",
      "row sums: [1.0159502 0.9755118 1.008538 ]\n",
      "col sums: [1. 1. 1.]\n",
      "Fro: 0.7002944\n",
      "MAE: 0.19909726\n",
      "\n",
      "Est T After T Revision\n",
      "row sums: [0.98345774 0.96573895 1.0508033 ]\n",
      "col sums: [1.        1.        1.0000001]\n",
      "Fro: 0.70211977\n",
      "MAE: 0.19250295\n",
      "forward loss\n",
      "Run 01/10: test acc = 98.37%\n",
      "mps: 1 steps -> 19.95 sec | avg 19947.8 ms/step\n",
      "q:0.3\n",
      "\n",
      "Est T After Anchor point\n",
      "row sums: [0.9758113  0.98749506 1.0366936 ]\n",
      "col sums: [0.99999994 1.         1.        ]\n",
      "Fro: 0.6827116\n",
      "MAE: 0.19896436\n",
      "\n",
      "Est T After T Revision\n",
      "row sums: [0.9469021 0.9766675 1.0764304]\n",
      "col sums: [1.        1.        1.0000001]\n",
      "Fro: 0.683123\n",
      "MAE: 0.19226576\n",
      "forward loss\n",
      "Run 02/10: test acc = 98.40%\n",
      "mps: 2 steps -> 18.57 sec | avg 9283.8 ms/step\n",
      "q:0.3\n",
      "\n",
      "Est T After Anchor point\n",
      "row sums: [1.0210499 0.9751001 1.00385  ]\n",
      "col sums: [1. 1. 1.]\n",
      "Fro: 0.7091402\n",
      "MAE: 0.19640109\n",
      "\n",
      "Est T After T Revision\n",
      "row sums: [0.9750798 1.0082357 1.0166847]\n",
      "col sums: [1.        1.        1.0000001]\n",
      "Fro: 0.69183856\n",
      "MAE: 0.19627047\n",
      "forward loss\n",
      "Run 03/10: test acc = 98.20%\n",
      "mps: 3 steps -> 18.18 sec | avg 6061.4 ms/step\n",
      "q:0.3\n",
      "\n",
      "Est T After Anchor point\n",
      "row sums: [1.0156524 0.9599815 1.024366 ]\n",
      "col sums: [1. 1. 1.]\n",
      "Fro: 0.7023603\n",
      "MAE: 0.19624624\n",
      "\n",
      "Est T After T Revision\n",
      "row sums: [0.9835422 0.9516979 1.06476  ]\n",
      "col sums: [1. 1. 1.]\n",
      "Fro: 0.7066982\n",
      "MAE: 0.19582537\n",
      "forward loss\n",
      "Run 04/10: test acc = 98.53%\n",
      "mps: 4 steps -> 18.22 sec | avg 4555.3 ms/step\n",
      "q:0.3\n",
      "\n",
      "Est T After Anchor point\n",
      "row sums: [1.0124834 0.975152  1.0123647]\n",
      "col sums: [1.0000001 1.        1.       ]\n",
      "Fro: 0.70181155\n",
      "MAE: 0.19657053\n",
      "\n",
      "Est T After T Revision\n",
      "row sums: [0.96764034 1.0084981  1.0238616 ]\n",
      "col sums: [1.         0.99999994 1.0000001 ]\n",
      "Fro: 0.6838522\n",
      "MAE: 0.19244191\n",
      "forward loss\n",
      "Run 05/10: test acc = 98.67%\n",
      "mps: 5 steps -> 18.17 sec | avg 3633.2 ms/step\n",
      "q:0.3\n",
      "\n",
      "Est T After Anchor point\n",
      "row sums: [1.0036061 0.96466   1.031734 ]\n",
      "col sums: [1. 1. 1.]\n",
      "Fro: 0.6861474\n",
      "MAE: 0.19925077\n",
      "\n",
      "Est T After T Revision\n",
      "row sums: [0.97161233 0.9564359  1.0719517 ]\n",
      "col sums: [1.0000001  0.99999994 1.        ]\n",
      "Fro: 0.6864095\n",
      "MAE: 0.1897263\n",
      "forward loss\n",
      "Run 06/10: test acc = 98.13%\n",
      "mps: 6 steps -> 17.99 sec | avg 2998.7 ms/step\n",
      "q:0.3\n",
      "\n",
      "Est T After Anchor point\n",
      "row sums: [1.0004258 0.9785496 1.0210245]\n",
      "col sums: [1. 1. 1.]\n",
      "Fro: 0.70888144\n",
      "MAE: 0.19777222\n",
      "\n",
      "Est T After T Revision\n",
      "row sums: [0.9701508 0.9679163 1.0619328]\n",
      "col sums: [1.        0.9999999 1.0000001]\n",
      "Fro: 0.712453\n",
      "MAE: 0.19794485\n",
      "forward loss\n",
      "Run 07/10: test acc = 98.47%\n",
      "mps: 7 steps -> 18.06 sec | avg 2579.6 ms/step\n",
      "q:0.3\n",
      "\n",
      "Est T After Anchor point\n",
      "row sums: [1.028259   0.95508707 1.0166538 ]\n",
      "col sums: [0.99999994 1.         1.        ]\n",
      "Fro: 0.7061061\n",
      "MAE: 0.1968426\n",
      "\n",
      "Est T After T Revision\n",
      "row sums: [0.9947735 0.9473687 1.0578579]\n",
      "col sums: [0.99999994 1.         1.0000001 ]\n",
      "Fro: 0.7099623\n",
      "MAE: 0.19700482\n",
      "forward loss\n",
      "Run 08/10: test acc = 98.67%\n",
      "mps: 8 steps -> 17.99 sec | avg 2249.2 ms/step\n",
      "q:0.3\n",
      "\n",
      "Est T After Anchor point\n",
      "row sums: [1.0276824  0.976848   0.99546957]\n",
      "col sums: [1. 1. 1.]\n",
      "Fro: 0.71173626\n",
      "MAE: 0.19856074\n",
      "\n",
      "Est T After T Revision\n",
      "row sums: [0.99449176 0.9666833  1.0388248 ]\n",
      "col sums: [1.         0.99999994 1.        ]\n",
      "Fro: 0.71460414\n",
      "MAE: 0.19846576\n",
      "forward loss\n",
      "Run 09/10: test acc = 98.43%\n",
      "mps: 9 steps -> 17.98 sec | avg 1997.7 ms/step\n",
      "q:0.3\n",
      "\n",
      "Est T After Anchor point\n",
      "row sums: [1.0341668  0.97556037 0.9902728 ]\n",
      "col sums: [0.99999994 1.         1.        ]\n",
      "Fro: 0.7105553\n",
      "MAE: 0.19852233\n",
      "\n",
      "Est T After T Revision\n",
      "row sums: [0.98646784 1.0088522  1.00468   ]\n",
      "col sums: [0.99999994 1.         1.        ]\n",
      "Fro: 0.6925883\n",
      "MAE: 0.1959788\n",
      "forward loss\n",
      "Run 10/10: test acc = 98.27%\n",
      "mps: 10 steps -> 18.18 sec | avg 1818.5 ms/step\n",
      "========================================================================\n",
      "fashion03 | forward | mean±std over 10 runs: 98.41±0.17%\n",
      "Saved summary to results2025-10-30-16:27/fashion03_forward_True_2025-10-30-16:27.json\n",
      "q:0.3\n",
      "forward loss\n",
      "Run 01/10: test acc = 72.53%\n",
      "mps: 1 steps -> 9.70 sec | avg 9699.0 ms/step\n",
      "q:0.3\n",
      "forward loss\n",
      "Run 02/10: test acc = 63.27%\n",
      "mps: 2 steps -> 9.71 sec | avg 4852.7 ms/step\n",
      "q:0.3\n",
      "forward loss\n",
      "Run 03/10: test acc = 68.53%\n",
      "mps: 3 steps -> 9.93 sec | avg 3309.3 ms/step\n",
      "q:0.3\n",
      "forward loss\n",
      "Run 04/10: test acc = 59.23%\n",
      "mps: 4 steps -> 9.89 sec | avg 2473.3 ms/step\n",
      "q:0.3\n",
      "forward loss\n",
      "Run 05/10: test acc = 61.23%\n",
      "mps: 5 steps -> 9.88 sec | avg 1975.0 ms/step\n",
      "q:0.3\n",
      "forward loss\n",
      "Run 06/10: test acc = 53.17%\n",
      "mps: 6 steps -> 9.69 sec | avg 1615.3 ms/step\n",
      "q:0.3\n",
      "forward loss\n",
      "Run 07/10: test acc = 55.67%\n",
      "mps: 7 steps -> 9.80 sec | avg 1399.3 ms/step\n",
      "q:0.3\n",
      "forward loss\n",
      "Run 08/10: test acc = 54.57%\n",
      "mps: 8 steps -> 9.81 sec | avg 1226.5 ms/step\n",
      "q:0.3\n",
      "forward loss\n",
      "Run 09/10: test acc = 52.77%\n",
      "mps: 9 steps -> 9.92 sec | avg 1102.4 ms/step\n",
      "q:0.3\n",
      "forward loss\n",
      "Run 10/10: test acc = 56.50%\n",
      "mps: 10 steps -> 9.67 sec | avg 966.9 ms/step\n",
      "========================================================================\n",
      "fashion03 | forward | mean±std over 10 runs: 59.75±6.34%\n",
      "Saved summary to results2025-10-30-16:27/fashion03_forward_False_2025-10-30-16:27.json\n",
      "q:0.3\n",
      "\n",
      "Est T After Anchor point\n",
      "row sums: [1.0159502 0.9755118 1.008538 ]\n",
      "col sums: [1. 1. 1.]\n",
      "Fro: 0.7002944\n",
      "MAE: 0.19909726\n",
      "\n",
      "Est T After T Revision\n",
      "row sums: [0.98345774 0.96573895 1.0508033 ]\n",
      "col sums: [1.        1.        1.0000001]\n",
      "Fro: 0.70211977\n",
      "MAE: 0.19250295\n",
      "GCE loss\n",
      "Run 01/10: test acc = 97.47%\n",
      "mps: 1 steps -> 18.08 sec | avg 18080.0 ms/step\n",
      "q:0.3\n",
      "\n",
      "Est T After Anchor point\n",
      "row sums: [0.9758113  0.98749506 1.0366936 ]\n",
      "col sums: [0.99999994 1.         1.        ]\n",
      "Fro: 0.6827116\n",
      "MAE: 0.19896436\n",
      "\n",
      "Est T After T Revision\n",
      "row sums: [0.9469021 0.9766675 1.0764304]\n",
      "col sums: [1.        1.        1.0000001]\n",
      "Fro: 0.683123\n",
      "MAE: 0.19226576\n",
      "GCE loss\n",
      "Run 02/10: test acc = 97.77%\n",
      "mps: 2 steps -> 17.91 sec | avg 8957.3 ms/step\n",
      "q:0.3\n",
      "\n",
      "Est T After Anchor point\n",
      "row sums: [1.0210499 0.9751001 1.00385  ]\n",
      "col sums: [1. 1. 1.]\n",
      "Fro: 0.7091402\n",
      "MAE: 0.19640109\n",
      "\n",
      "Est T After T Revision\n",
      "row sums: [0.9750798 1.0082357 1.0166847]\n",
      "col sums: [1.        1.        1.0000001]\n",
      "Fro: 0.69183856\n",
      "MAE: 0.19627047\n",
      "GCE loss\n",
      "Run 03/10: test acc = 97.87%\n",
      "mps: 3 steps -> 17.96 sec | avg 5986.9 ms/step\n",
      "q:0.3\n",
      "\n",
      "Est T After Anchor point\n",
      "row sums: [1.0156524 0.9599815 1.024366 ]\n",
      "col sums: [1. 1. 1.]\n",
      "Fro: 0.7023603\n",
      "MAE: 0.19624624\n",
      "\n",
      "Est T After T Revision\n",
      "row sums: [0.9835422 0.9516979 1.06476  ]\n",
      "col sums: [1. 1. 1.]\n",
      "Fro: 0.7066982\n",
      "MAE: 0.19582537\n",
      "GCE loss\n",
      "Run 04/10: test acc = 97.57%\n",
      "mps: 4 steps -> 18.00 sec | avg 4500.2 ms/step\n",
      "q:0.3\n",
      "\n",
      "Est T After Anchor point\n",
      "row sums: [1.0124834 0.975152  1.0123647]\n",
      "col sums: [1.0000001 1.        1.       ]\n",
      "Fro: 0.70181155\n",
      "MAE: 0.19657053\n",
      "\n",
      "Est T After T Revision\n",
      "row sums: [0.96764034 1.0084981  1.0238616 ]\n",
      "col sums: [1.         0.99999994 1.0000001 ]\n",
      "Fro: 0.6838522\n",
      "MAE: 0.19244191\n",
      "GCE loss\n"
     ]
    }
   ],
   "source": [
    "for cfg in configs:\n",
    "    all_acc = []\n",
    "    last_T = None\n",
    "    t_arr = []\n",
    "    for r in range(cfg['runs']):\n",
    "        start = time.perf_counter()\n",
    "        acc, T = run_once(cfg, seed=1000+r)\n",
    "\n",
    "        all_acc.append(acc)\n",
    "        if cfg['estimate_T'] or cfg['dataset']=='cifar':\n",
    "            t_arr.append(T)\n",
    "        last_T = T if T is not None else last_T\n",
    "        print(f\"Run {r+1:02d}/{cfg['runs']}: test acc = {acc*100:.2f}%\")\n",
    "        end = time.perf_counter()\n",
    "        print(f\"{cfg['device']}: {r+1} steps -> {end - start:.2f} sec | avg {1000*(end - start)/(r+1):.1f} ms/step\")\n",
    "    mean = float(np.mean(all_acc))\n",
    "    std  = float(np.std(all_acc))\n",
    "\n",
    "    summary = {\n",
    "        'cfg':cfg,\n",
    "        'dataset': cfg['dataset'],\n",
    "        'loss': cfg['loss'],\n",
    "        'estimate_T': bool(cfg['estimate_T']),\n",
    "        'epochs': cfg['epochs'],\n",
    "        'runs': cfg['runs'],\n",
    "        'mean_test_acc': mean,\n",
    "        'std_test_acc': std,\n",
    "        'last_estimated_T': last_T,\n",
    "        't_arr':t_arr,\n",
    "        'per_run_acc': all_acc,\n",
    "    }\n",
    "    print(\"=\"*72)\n",
    "    print(f\"{cfg['dataset']} | {cfg['loss']} | mean±std over {cfg['runs']} runs: {mean*100:.2f}±{std*100:.2f}%\")\n",
    "\n",
    "    with open(cfg['out'], 'w') as f:\n",
    "        json.dump(summary, f, indent=2)\n",
    "    print(f\"Saved summary to {cfg['out']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fashion03_forward_True_2025-10-30-09:05.json:\n",
      "  dataset       = fashion03\n",
      "  loss          = forward\n",
      "  mean_test_acc = 0.4004\n",
      "  std_test_acc  = 0.0798\n",
      "------------------------------------------------------------\n",
      "fashion06_forward_False_2025-10-30-09:05.json:\n",
      "  dataset       = fashion06\n",
      "  loss          = forward\n",
      "  mean_test_acc = 0.3010\n",
      "  std_test_acc  = 0.0589\n",
      "------------------------------------------------------------\n",
      "fashion03_gce_True_2025-10-30-09:05.json:\n",
      "  dataset       = fashion03\n",
      "  loss          = gce\n",
      "  mean_test_acc = 0.5101\n",
      "  std_test_acc  = 0.0464\n",
      "------------------------------------------------------------\n",
      "fashion03_forward_False_2025-10-30-09:05.json:\n",
      "  dataset       = fashion03\n",
      "  loss          = forward\n",
      "  mean_test_acc = 0.3668\n",
      "  std_test_acc  = 0.0373\n",
      "------------------------------------------------------------\n",
      "fashion03_gce_False_2025-10-30-09:05.json:\n",
      "  dataset       = fashion03\n",
      "  loss          = gce\n",
      "  mean_test_acc = 0.4860\n",
      "  std_test_acc  = 0.0492\n",
      "------------------------------------------------------------\n",
      "fashion06_gce_False_2025-10-30-09:05.json:\n",
      "  dataset       = fashion06\n",
      "  loss          = gce\n",
      "  mean_test_acc = 0.9760\n",
      "  std_test_acc  = 0.0027\n",
      "------------------------------------------------------------\n",
      "cifar_forward_False_2025-10-30-09:05.json:\n",
      "  dataset       = cifar\n",
      "  loss          = forward\n",
      "  mean_test_acc = 0.8063\n",
      "  std_test_acc  = 0.1954\n",
      "------------------------------------------------------------\n",
      "fashion06_forward_True_2025-10-30-09:05.json:\n",
      "  dataset       = fashion06\n",
      "  loss          = forward\n",
      "  mean_test_acc = 0.9845\n",
      "  std_test_acc  = 0.0015\n",
      "------------------------------------------------------------\n",
      "cifar_gce_False_2025-10-30-09:05.json:\n",
      "  dataset       = cifar\n",
      "  loss          = gce\n",
      "  mean_test_acc = 0.9302\n",
      "  std_test_acc  = 0.0176\n",
      "------------------------------------------------------------\n",
      "fashion06_gce_True_2025-10-30-09:05.json:\n",
      "  dataset       = fashion06\n",
      "  loss          = gce\n",
      "  mean_test_acc = 0.9774\n",
      "  std_test_acc  = 0.0027\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for filename in os.listdir(folder):\n",
    "    if filename.endswith(\".json\"):\n",
    "        filepath = os.path.join(folder, filename)\n",
    "        try:\n",
    "            with open(filepath, \"r\", encoding=\"utf-8\") as f:\n",
    "                data = json.load(f)\n",
    "            \n",
    "            dataset = data.get(\"dataset\", \"N/A\")\n",
    "            loss = data.get(\"loss\", \"N/A\")\n",
    "            mean_acc = data.get(\"mean_test_acc\", None)\n",
    "            std_acc = data.get(\"std_test_acc\", None)\n",
    "\n",
    "            print(f\"{filename}:\")\n",
    "            print(f\"  dataset       = {dataset}\")\n",
    "            print(f\"  loss          = {loss}\")\n",
    "            print(f\"  mean_test_acc = {mean_acc:.4f}\" if mean_acc is not None else \"  mean_test_acc = N/A\")\n",
    "            print(f\"  std_test_acc  = {std_acc:.4f}\" if std_acc is not None else \"  std_test_acc = N/A\")\n",
    "            print(\"-\" * 60)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error reading {filename}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading: results2025-10-30-09:05/fashion03_forward_True_2025-10-30-09:05.json\n",
      "[[0.37333062 0.35970184 0.27876514]\n",
      " [0.32170659 0.35969746 0.31239942]\n",
      " [0.30496281 0.28060067 0.40883544]]\n",
      "[[0.7 0.3 0. ]\n",
      " [0.  0.7 0.3]\n",
      " [0.3 0.  0.7]]\n",
      "Fro error: 0.755632206441637\n",
      "rre error: 0.5728432934797962\n",
      "mae error: 0.21291920873853895\n",
      "Per-row correlations: [np.float64(0.8905745381555876), np.float64(0.8091376512635876), np.float64(0.966010180771437)]\n",
      "Mean: 0.8885741233968707\n",
      "\n",
      "Loading: results2025-10-30-09:05/fashion06_forward_True_2025-10-30-09:05.json\n",
      "\n",
      "[[6.48417354e-01 9.09397670e-07 3.19373101e-01]\n",
      " [3.01015973e-01 7.01399863e-01 9.09404264e-07]\n",
      " [5.05666547e-02 2.98599243e-01 6.80626035e-01]]\n",
      "[[0.4 0.3 0.3]\n",
      " [0.3 0.4 0.3]\n",
      " [0.3 0.3 0.4]]\n",
      "Fro error: 0.6884026830183025\n",
      "rre error: 0.6816202084504638\n",
      "mae error: 0.18907384606246877\n",
      "Per-row correlations: [np.float64(0.8702991343561096), np.float64(0.903904007053476), np.float64(0.9205048686529939)]\n",
      "Mean: 0.8982360033541932\n"
     ]
    }
   ],
   "source": [
    "C = 3\n",
    "\n",
    "# pattern for files starting with \"name\" and ending with \".json\"\n",
    "files = sorted(glob.glob(os.path.join(folder,\"fashion03*True*.json\")))\n",
    "\n",
    "# pick the first matching file\n",
    "first_file = files[0]\n",
    "print(\"Loading:\", first_file)\n",
    "\n",
    "# load the JSON contents\n",
    "with open(first_file, \"r\", encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "T_prime = np.array(data['last_estimated_T'])\n",
    "T_true = pick_known_T('fashion03')\n",
    "\n",
    "#checking recreation performance\n",
    "print(T_prime)\n",
    "print(T_true)\n",
    "print(f\"Fro error: {np.linalg.norm(T_prime - T_true, 'fro')}\")\n",
    "print(f\"rre error: {np.linalg.norm(T_prime - T_true, 'fro') / np.linalg.norm(T_true, 'fro')}\")\n",
    "print(f\"mae error: {np.mean(np.abs(T_prime - T_true))}\")\n",
    "\n",
    "\n",
    "corrs = [st.pearsonr(T_true[i], T_prime[i])[0] for i in range(C)]\n",
    "print(\"Per-row correlations:\", corrs)\n",
    "print(\"Mean:\", np.mean(corrs))\n",
    "\n",
    "\n",
    "# pattern for files starting with \"name\" and ending with \".json\"\n",
    "files = sorted(glob.glob(os.path.join(folder,\"fashion06*True**.json\")))\n",
    "\n",
    "\n",
    "# pick the first matching file\n",
    "first_file = files[0]\n",
    "print(\"\\nLoading:\", first_file)\n",
    "# load the JSON contents\n",
    "with open(first_file, \"r\", encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)\n",
    "print()\n",
    "T_prime = np.array(data['last_estimated_T'])\n",
    "T_true = pick_known_T('fashion06')\n",
    "\n",
    "#checking recreation performance\n",
    "print(T_prime)\n",
    "print(T_true)\n",
    "print(f\"Fro error: {np.linalg.norm(T_prime - T_true, 'fro')}\")\n",
    "print(f\"rre error: {np.linalg.norm(T_prime - T_true, 'fro') / np.linalg.norm(T_true, 'fro')}\")\n",
    "print(f\"mae error: {np.mean(np.abs(T_prime - T_true))}\")\n",
    "\n",
    "\n",
    "corrs = [st.pearsonr(T_true[i], T_prime[i])[0] for i in range(C)]\n",
    "print(\"Per-row correlations:\", corrs)\n",
    "print(\"Mean:\", np.mean(corrs))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
