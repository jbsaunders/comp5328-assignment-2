{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9IrXtNd8l-9H"
   },
   "source": [
    "## COMP5328 - Advanced Machine Learning\n",
    "## Assignment 2: Title\n",
    "----------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Previous Log File does not exist.\n"
     ]
    }
   ],
   "source": [
    "# Common imports\n",
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import time\n",
    "import datetime as dt\n",
    "from PIL import Image\n",
    "from collections import Counter, defaultdict\n",
    "from datetime import datetime\n",
    "\n",
    "# Ploting\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Evaluation \n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import normalized_mutual_info_score\n",
    "\n",
    "# Experiment\n",
    "from joblib import Parallel, delayed\n",
    "import traceback\n",
    "\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import os\n",
    "import torch.optim as optim\n",
    "\n",
    "# Notebook variables\n",
    "seed = 0\n",
    "# np.random.seed(seed)\n",
    "# rng = np.random.default_rng(seed)\n",
    "\n",
    "# Log file\n",
    "log_file = 'experiment_results.txt'\n",
    "\n",
    "if os.path.exists(log_file):\n",
    "    os.remove(log_file)\n",
    "    print(\"Previous Log File deleted.\")\n",
    "else:\n",
    "    print(\"Previous Log File does not exist.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Experiment variables\n",
    "# Common\n",
    "datasets = [\"cifar\", \"fashion03\", \"fashion06\"]\n",
    "\n",
    "\n",
    "\n",
    "dataset_folder = 'data/'\n",
    "cifar_dataset = dataset_folder+'CIFAR.npz'\n",
    "MNISTO3_dataset = dataset_folder+'FashionMNIST0.3.npz'\n",
    "MNISTO6_dataset = dataset_folder+'FashionMNIST0.6.npz'\n",
    "\n",
    "known_T_fashion_03 = np.array(  [[0.7,0.3,0.0],\n",
    "                                [0.0,0.7,0.3],\n",
    "                                [0.0,0.0,0.7]], dtype=np.float32)\n",
    "\n",
    "known_T_fashion_06 = np.array(  [[0.3,0.4,0.3],\n",
    "                                [0.4,0.3,0.3],\n",
    "                                [0.3,0.3,0.4]], dtype=np.float32)\n",
    "\n",
    "def set_seed(seed):\n",
    "    import random\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_experiment(df, experiment_name, filename=log_file):\n",
    "  \"\"\"\n",
    "  Record the experimental result\n",
    "  \"\"\"\n",
    "  mode = \"a\" if os.path.exists(filename) else \"w\"\n",
    "  with open(filename, mode) as f:\n",
    "      if mode == \"w\":  # first record\n",
    "          f.write(\"===== Experiment Log Start ======================================\\n\\n\")\n",
    "      f.write(f\"===== {dt.datetime.now()} == New Experiment: {experiment_name} =====\\n\")\n",
    "      f.write(df.to_string(index=False))\n",
    "      f.write(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qYzWuiytl-9I"
   },
   "source": [
    "## 1. Load Dataset\n",
    "\n",
    "### 1.0 Data Folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unzip:  cannot find or open datasets.zip, datasets.zip.zip or datasets.zip.ZIP.\n"
     ]
    }
   ],
   "source": [
    "# Path to your dataset zip stored in Drive\n",
    "zip_path = \"datasets.zip\"\n",
    "\n",
    "# Unzip file\n",
    "!unzip -o -q \"$zip_path\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OxEr9ihznSqa",
    "outputId": "8b9fd2c9-b2f9-45f4-94aa-d0d473b7e140"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 172688\n",
      "-rw-r--r--  1 jamie.saunders  staff  55440974 Oct 28 22:10 CIFAR.npz\n",
      "-rw-r--r--  1 jamie.saunders  staff  16485974 Oct 28 22:10 FashionMNIST0.3.npz\n",
      "-rw-r--r--  1 jamie.saunders  staff  16485974 Oct 28 22:10 FashionMNIST0.6.npz\n"
     ]
    }
   ],
   "source": [
    "# The structure of data folder.\n",
    "!ls -l data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_npz(path):\n",
    "    d = np.load(path)\n",
    "    Xtr, Str = d['Xtr'], d['Str']\n",
    "    Xts, Yts = d['Xts'], d['Yts']\n",
    "    return Xtr, Str, Xts, Yts\n",
    "\n",
    "# A helper class, it is used as an input of the DataLoader object.\n",
    "class DatasetArray(Dataset):\n",
    "    r\"\"\"This is a child class of the pytorch Dataset object.\"\"\"\n",
    "    def __init__(self, data, labels=None, transform=None):\n",
    "        if labels != None:\n",
    "            self.data_arr = np.asarray(data).astype(np.float32)\n",
    "            self.label_arr = np.asarray(labels).astype(np.long)\n",
    "        else:\n",
    "            tmp_arr = np.asarray(data)\n",
    "            self.data_arr = tmp_arr[:,:-1].astype(np.float32)\n",
    "            self.label_arr = tmp_arr[:,-1].astype(np.long)\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data_arr)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "     \n",
    "        data = self.data_arr[index]\n",
    "        label = self.label_arr[index]\n",
    "        \n",
    "        if self.transform is not None:\n",
    "            data = self.transform(data)\n",
    "            \n",
    "        return (data, label)\n",
    "    \n",
    "    \n",
    "# Splitting the data into three parts.\n",
    "def train_val_test_random_split(data, fracs=[0.7,0.1,0.2]):\n",
    "    r\"\"\"Split the data into training, validation and test set.\n",
    "    Args:\n",
    "        fracs: a list of length three\n",
    "    \"\"\"\n",
    "    assert len(fracs) == 3\n",
    "    assert sum(fracs) == 1\n",
    "    assert all(frac > 0 for frac in fracs)\n",
    "    n = len(data)\n",
    "    subset_lens = [int(n*frac) for frac in fracs]\n",
    "    idxs = list(range(n))\n",
    "    random.shuffle(idxs)\n",
    "    data = np.array(data)\n",
    "    new_data = []\n",
    "    start_idx = 0\n",
    "    for subset_len in subset_lens:\n",
    "        end_idx = start_idx + subset_len\n",
    "        cur_idxs = idxs[start_idx:end_idx]\n",
    "        new_data.append(data[cur_idxs,:].tolist())\n",
    "        start_idx = end_idx\n",
    "    return new_data\n",
    "\n",
    "# Preparation of the data for training, validation and testing a pytorch network. \n",
    "# Note that the test data is not in use for this lab.\n",
    "def get_loader(batch_size =128, num_workers = 0, train_val_test_split = [0.7,0.1,0.2], data=None):\n",
    "    r\"\"\"This function is used to read the data file and split the data into three subsets, i.e, \n",
    "    train data, validation data and test data. Their corresponding DataLoader objects are returned.\"\"\"\n",
    "    \n",
    "    [train_data, val_data, test_data] = train_val_test_random_split(data, fracs = train_val_test_split)\n",
    "\n",
    "    train_data = DatasetArray(data = train_data)\n",
    "    val_data = DatasetArray(data = val_data)\n",
    "    test_data = DatasetArray(data = test_data)\n",
    "\n",
    "    #The pytorch built-in class DataLoader can help us to shuffle the data, draw mini-batch,\n",
    "    #do transformations, etc. \n",
    "    train_loader = DataLoader(\n",
    "        train_data,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=num_workers,\n",
    "    )\n",
    "\n",
    "    val_loader = DataLoader(\n",
    "        val_data,\n",
    "        batch_size=100,\n",
    "        shuffle=False,\n",
    "        num_workers=num_workers,\n",
    "    )\n",
    "\n",
    "    test_loader = DataLoader(\n",
    "        test_data,\n",
    "        batch_size=100,\n",
    "        num_workers=num_workers,\n",
    "        shuffle=False,\n",
    "    )\n",
    "    return train_loader, val_loader, test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(18000, 784)\n",
      "(18000,)\n",
      "(3000, 784)\n",
      "(3000,)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "# Remember t o r e p l a c e t h e $FILE PATH\n",
    "Xtr, Str, Xts, Yts = load_npz(MNISTO6_dataset)\n",
    "\n",
    "print (Xtr.shape)\n",
    "print (Str.shape)\n",
    "print (Xts.shape)\n",
    "print (Yts.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NpzImageDataset(Dataset):\n",
    "    def __init__(self, X, y, is_cifar=False):\n",
    "        self.X = X.astype(np.float32)\n",
    "        self.y = y.astype(np.int64)\n",
    "        self.is_cifar = is_cifar\n",
    "\n",
    "        # Normalize to [0,1]\n",
    "        self.X = self.X / 255.0 if self.X.max() > 1.0 else self.X\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x = self.X[idx]\n",
    "        if x.ndim == 1:\n",
    "            # flat; try to infer shape 28x28 or 32x32x3\n",
    "            if x.size == 28*28:\n",
    "                x = x.reshape(1, 28, 28)\n",
    "            elif x.size == 32*32*3:\n",
    "                x = x.reshape(3, 32, 32)\n",
    "            else:\n",
    "                raise ValueError(\"Unknown flat image shape: {}\".format(x.shape))\n",
    "        else:\n",
    "            # (H,W) or (H,W,C)\n",
    "            if x.ndim == 2:\n",
    "                x = x[None, ...]  # to (1,H,W)\n",
    "            elif x.ndim == 3:\n",
    "                # assume HWC -> CHW\n",
    "                x = np.transpose(x, (2, 0, 1))\n",
    "            else:\n",
    "                raise ValueError(f\"Unexpected image dims: {x.shape}\")\n",
    "        return torch.from_numpy(x), torch.tensor(self.y[idx])\n",
    "\n",
    "\n",
    "def load_npz(path):\n",
    "    d = np.load(path)\n",
    "    Xtr, Str = d['Xtr'], d['Str']\n",
    "    Xts, Yts = d['Xts'], d['Yts']\n",
    "    return Xtr, Str, Xts, Yts\n",
    "\n",
    "\n",
    "def make_loaders(Xtr, Str, batch_size=128, seed=0, test_size=0.2):\n",
    "    # 80/20 split each repetition\n",
    "    X_tr, X_val, y_tr, y_val = train_test_split(\n",
    "        Xtr, Str, test_size=test_size, random_state=seed, stratify=Str\n",
    "    )\n",
    "\n",
    "    is_cifar = (X_tr.shape[-1] == 3) if X_tr.ndim == 4 else (X_tr.shape[-1] == 32*32*3)\n",
    "\n",
    "    train_ds = NpzImageDataset(X_tr, y_tr, is_cifar)\n",
    "    val_ds   = NpzImageDataset(X_val, y_val, is_cifar)\n",
    "\n",
    "    train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "    val_loader   = DataLoader(val_ds,   batch_size=batch_size, shuffle=False, num_workers=0)\n",
    "\n",
    "    return train_loader, val_loader, is_cifar\n",
    "\n",
    "\n",
    "def make_test_loader(Xts, Yts, batch_size=256):\n",
    "    is_cifar = (Xts.shape[-1] == 3) if Xts.ndim == 4 else (Xts.shape[-1] == 32*32*3)\n",
    "    test_ds = NpzImageDataset(Xts, Yts, is_cifar)\n",
    "    return DataLoader(test_ds, batch_size=batch_size, shuffle=False, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loaders for this split\n",
    "train_loader, val_loader, is_cifar = make_loaders(Xtr, Str, batch_size=512, seed=seed)\n",
    "test_loader = make_test_loader(Xts, Yts, batch_size=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0:\n",
      "x shape: torch.Size([512, 1, 28, 28])\n",
      "y shape: torch.Size([512])\n",
      "x[0]: tensor([[[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.2980, 0.4314, 0.3451, 0.4431, 0.4235, 0.4471, 0.4667,\n",
      "          0.4627, 0.4510, 0.4510, 0.4471, 0.4431, 0.4314, 0.4275, 0.4314,\n",
      "          0.4275, 0.4314, 0.4353, 0.4353, 0.4314, 0.4314, 0.4275, 0.4275,\n",
      "          0.4588, 0.4118, 0.4000, 0.3176],\n",
      "         [0.0000, 0.6667, 0.6941, 0.4980, 0.6510, 0.6000, 0.6000, 0.6000,\n",
      "          0.6118, 0.6157, 0.6078, 0.6039, 0.5961, 0.6078, 0.6118, 0.6078,\n",
      "          0.6078, 0.6078, 0.6000, 0.6000, 0.6039, 0.6078, 0.6000, 0.5922,\n",
      "          0.6235, 0.5765, 0.6471, 0.5176],\n",
      "         [0.0000, 0.6118, 0.8510, 0.6157, 0.6902, 0.6549, 0.6314, 0.6353,\n",
      "          0.6196, 0.6118, 0.6078, 0.6000, 0.5922, 0.6000, 0.6000, 0.6000,\n",
      "          0.6000, 0.5961, 0.5804, 0.5804, 0.5961, 0.6157, 0.6392, 0.6353,\n",
      "          0.7098, 0.5725, 0.7725, 0.6980],\n",
      "         [0.0000, 0.6353, 0.9098, 0.6784, 0.6510, 0.7412, 0.7137, 0.6863,\n",
      "          0.6941, 0.6784, 0.6784, 0.6667, 0.6588, 0.6667, 0.6667, 0.6667,\n",
      "          0.6667, 0.6667, 0.6745, 0.6706, 0.6824, 0.6902, 0.6980, 0.7412,\n",
      "          0.6863, 0.5686, 0.9216, 0.6275],\n",
      "         [0.0000, 0.6980, 0.9333, 0.7843, 0.4980, 0.6824, 0.7255, 0.6941,\n",
      "          0.6980, 0.6980, 0.6941, 0.6902, 0.6941, 0.6824, 0.6784, 0.6784,\n",
      "          0.6745, 0.6706, 0.6784, 0.6863, 0.6941, 0.7020, 0.6863, 0.7255,\n",
      "          0.5373, 0.6353, 0.9882, 0.5804],\n",
      "         [0.0000, 0.8000, 0.9294, 0.8745, 0.6431, 0.6745, 0.7216, 0.7059,\n",
      "          0.7216, 0.7216, 0.7176, 0.7137, 0.6980, 0.6824, 0.6784, 0.6745,\n",
      "          0.6706, 0.6784, 0.7020, 0.7137, 0.7176, 0.7098, 0.7059, 0.6902,\n",
      "          0.5804, 0.8235, 0.9490, 0.5490],\n",
      "         [0.0000, 0.7098, 0.9216, 0.8863, 0.7882, 0.5333, 0.7255, 0.7294,\n",
      "          0.7059, 0.7216, 0.7137, 0.7333, 0.8118, 0.8588, 0.8627, 0.8471,\n",
      "          0.8314, 0.7765, 0.6902, 0.7098, 0.7098, 0.7294, 0.7451, 0.6157,\n",
      "          0.6784, 0.8824, 0.9294, 0.5255],\n",
      "         [0.0000, 0.6667, 0.9216, 0.8549, 0.9490, 0.6392, 0.6510, 0.7373,\n",
      "          0.7098, 0.7098, 0.7216, 0.7373, 0.4706, 0.4353, 0.4431, 0.4588,\n",
      "          0.4000, 0.5333, 0.7490, 0.6902, 0.6902, 0.7333, 0.7059, 0.5804,\n",
      "          0.8118, 0.8314, 0.9098, 0.5216],\n",
      "         [0.0000, 0.6549, 0.9216, 0.8510, 0.9294, 0.8000, 0.5765, 0.7882,\n",
      "          0.7529, 0.7294, 0.7804, 0.7961, 0.3529, 0.2784, 0.3137, 0.3216,\n",
      "          0.2824, 0.4863, 0.7961, 0.7294, 0.7373, 0.7725, 0.6902, 0.7020,\n",
      "          0.9294, 0.7961, 0.9294, 0.5255],\n",
      "         [0.0000, 0.6431, 0.9216, 0.8627, 0.9176, 0.9216, 0.6706, 0.3608,\n",
      "          0.7176, 0.6392, 0.6314, 0.6824, 0.8392, 0.8627, 0.8549, 0.8392,\n",
      "          0.8510, 0.7882, 0.7020, 0.6039, 0.7216, 0.5333, 0.4745, 0.8902,\n",
      "          0.8588, 0.8196, 0.9020, 0.5216],\n",
      "         [0.0000, 0.6275, 0.9255, 0.8706, 0.8824, 0.9529, 0.7843, 0.6902,\n",
      "          0.7412, 0.7333, 0.7294, 0.7333, 0.7137, 0.7020, 0.6941, 0.7020,\n",
      "          0.6784, 0.7020, 0.7490, 0.6941, 0.7255, 0.7529, 0.7020, 0.9333,\n",
      "          0.8314, 0.7922, 0.9451, 0.5294],\n",
      "         [0.0000, 0.5922, 0.9216, 0.8510, 0.9137, 0.8235, 0.6431, 0.8235,\n",
      "          0.8118, 0.8392, 0.8353, 0.8353, 0.8392, 0.8431, 0.8392, 0.8431,\n",
      "          0.8353, 0.8431, 0.8549, 0.8706, 0.8392, 0.8431, 0.8157, 0.7922,\n",
      "          0.8275, 0.7608, 1.0000, 0.4471],\n",
      "         [0.0000, 0.6157, 0.8863, 0.8353, 0.8745, 0.6000, 0.4627, 0.8078,\n",
      "          0.7490, 0.7569, 0.7647, 0.7686, 0.7608, 0.7608, 0.7647, 0.7608,\n",
      "          0.7569, 0.7569, 0.7647, 0.7647, 0.7608, 0.8706, 0.4314, 0.6000,\n",
      "          0.8667, 0.7922, 0.9255, 0.4078],\n",
      "         [0.0000, 0.5765, 0.8980, 0.8039, 0.6039, 0.6824, 0.8196, 0.7882,\n",
      "          0.7647, 0.7608, 0.7686, 0.7647, 0.7569, 0.7647, 0.7686, 0.7647,\n",
      "          0.7608, 0.7608, 0.7647, 0.7725, 0.7647, 0.7725, 0.8314, 0.7137,\n",
      "          0.6235, 0.7765, 0.9725, 0.4078],\n",
      "         [0.0000, 0.6157, 0.8314, 0.7098, 0.6667, 0.7804, 0.7490, 0.7490,\n",
      "          0.7686, 0.7608, 0.7647, 0.7569, 0.7529, 0.7490, 0.7451, 0.7451,\n",
      "          0.7412, 0.7333, 0.7333, 0.7569, 0.7490, 0.7451, 0.7569, 0.7686,\n",
      "          0.6667, 0.6824, 0.8353, 0.3725],\n",
      "         [0.0000, 0.5922, 0.6745, 0.7569, 0.7804, 0.7882, 0.8000, 0.7922,\n",
      "          0.8039, 0.8078, 0.8118, 0.8118, 0.8039, 0.8000, 0.8000, 0.7961,\n",
      "          0.7961, 0.7922, 0.7843, 0.7725, 0.7608, 0.7647, 0.7569, 0.7569,\n",
      "          0.7529, 0.7294, 0.8157, 0.2510],\n",
      "         [0.0000, 0.7333, 0.5529, 0.7725, 0.7725, 0.9020, 0.9020, 0.9020,\n",
      "          0.8941, 0.8902, 0.8941, 0.8784, 0.8588, 0.8353, 0.8353, 0.8392,\n",
      "          0.8392, 0.8392, 0.8392, 0.8353, 0.8353, 0.8353, 0.8353, 0.8196,\n",
      "          0.7843, 0.6863, 0.6784, 0.0000],\n",
      "         [0.0000, 0.0549, 0.0039, 0.0118, 0.0039, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000]]])\n",
      "y[0]: tensor(0)\n",
      "Batch 1:\n",
      "x shape: torch.Size([512, 1, 28, 28])\n",
      "y shape: torch.Size([512])\n",
      "x[0]: tensor([[[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0667, 0.4627,\n",
      "          0.4314, 0.2588, 0.1961, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0510, 0.2039, 0.2392, 0.3686, 0.2863, 0.0667, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.1451, 0.6941, 0.6588, 0.6118,\n",
      "          0.7216, 0.7412, 0.7725, 0.8549, 0.7216, 0.5804, 0.6471, 0.6941,\n",
      "          0.6471, 0.6118, 0.5843, 0.5373, 0.5843, 0.5725, 0.5098, 0.0314,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.6980, 0.6902, 0.4941, 0.4627,\n",
      "          0.7294, 0.5333, 0.5020, 0.5255, 0.4902, 0.5412, 0.5098, 0.4471,\n",
      "          0.4314, 0.4235, 0.4627, 0.4118, 0.4706, 0.3804, 0.5686, 0.3059,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.1608, 0.6667, 0.5882, 0.5961, 0.4706,\n",
      "          0.7922, 0.5333, 0.5373, 0.5647, 0.4902, 0.5020, 0.5020, 0.4627,\n",
      "          0.5176, 0.4627, 0.5059, 0.4314, 0.4784, 0.3922, 0.4627, 0.4392,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.6588, 0.6667, 0.5725, 0.7686, 0.4902,\n",
      "          0.7843, 0.5216, 0.5373, 0.5373, 0.4902, 0.5020, 0.4902, 0.4745,\n",
      "          0.5098, 0.4588, 0.4902, 0.4392, 0.4157, 0.4392, 0.4549, 0.5647,\n",
      "          0.1137, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.5882, 0.6510, 0.6667, 0.6471, 0.8549, 0.5059,\n",
      "          0.7725, 0.5216, 0.5373, 0.5098, 0.4863, 0.4941, 0.4941, 0.5098,\n",
      "          0.5176, 0.4627, 0.4784, 0.4314, 0.3686, 0.4784, 0.4706, 0.4471,\n",
      "          0.5412, 0.1608, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.2902, 0.6941, 0.6784, 0.7412, 0.8392, 0.5529,\n",
      "          0.7373, 0.5333, 0.5373, 0.4941, 0.4902, 0.5020, 0.5020, 0.5333,\n",
      "          0.5333, 0.4745, 0.4745, 0.4275, 0.3490, 0.4902, 0.5176, 0.5176,\n",
      "          0.3843, 0.0667, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.1804, 0.7098, 0.8627, 0.8824, 0.6157,\n",
      "          0.7137, 0.5490, 0.4902, 0.4784, 0.4941, 0.4941, 0.5059, 0.5373,\n",
      "          0.5373, 0.5216, 0.4431, 0.4314, 0.3490, 0.4706, 0.6627, 0.4784,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.1765, 0.9020, 0.9176, 0.6941,\n",
      "          0.7608, 0.5333, 0.4549, 0.4627, 0.4706, 0.4902, 0.5020, 0.5255,\n",
      "          0.5255, 0.5176, 0.4392, 0.3647, 0.3059, 0.5882, 0.5725, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.4863, 0.9961, 0.6667,\n",
      "          0.7686, 0.5490, 0.4588, 0.4902, 0.4784, 0.4784, 0.4941, 0.5255,\n",
      "          0.5216, 0.5020, 0.4275, 0.3020, 0.4157, 0.5686, 0.0000, 0.0000,\n",
      "          0.0039, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.7882, 0.7451,\n",
      "          0.7098, 0.5569, 0.4627, 0.5059, 0.4902, 0.4784, 0.4902, 0.5255,\n",
      "          0.5176, 0.5059, 0.4588, 0.3059, 0.5843, 0.2863, 0.0000, 0.0078,\n",
      "          0.0039, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.3529, 0.7922,\n",
      "          0.6745, 0.5569, 0.4706, 0.5020, 0.4941, 0.4941, 0.4784, 0.5255,\n",
      "          0.5333, 0.5216, 0.4784, 0.3020, 0.5843, 0.1725, 0.0000, 0.0039,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.6941,\n",
      "          0.6902, 0.5569, 0.4706, 0.5059, 0.5059, 0.4941, 0.4784, 0.5255,\n",
      "          0.5333, 0.5176, 0.4784, 0.3059, 0.5686, 0.0392, 0.0000, 0.0039,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.6902,\n",
      "          0.6980, 0.5373, 0.4706, 0.5098, 0.5176, 0.4902, 0.4784, 0.5255,\n",
      "          0.5333, 0.5255, 0.4902, 0.3490, 0.4784, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.6510,\n",
      "          0.6902, 0.5333, 0.4784, 0.5176, 0.5216, 0.4902, 0.4784, 0.5255,\n",
      "          0.5216, 0.5373, 0.5098, 0.4000, 0.3765, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.7373,\n",
      "          0.6745, 0.5373, 0.5059, 0.5216, 0.5255, 0.4941, 0.4784, 0.5333,\n",
      "          0.5176, 0.5216, 0.5098, 0.4431, 0.2549, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.7608,\n",
      "          0.6510, 0.5373, 0.5059, 0.5255, 0.5373, 0.5059, 0.4745, 0.5255,\n",
      "          0.5255, 0.5176, 0.5216, 0.4863, 0.1725, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.6784,\n",
      "          0.6353, 0.5412, 0.5020, 0.5255, 0.5412, 0.5098, 0.4706, 0.5255,\n",
      "          0.5333, 0.5216, 0.5373, 0.5020, 0.1765, 0.0000, 0.0039, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0314, 0.7294,\n",
      "          0.6275, 0.5490, 0.5176, 0.5373, 0.5529, 0.5176, 0.4784, 0.5216,\n",
      "          0.5333, 0.5255, 0.5490, 0.5098, 0.2235, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0039, 0.0000, 0.1804, 0.7686,\n",
      "          0.6000, 0.5490, 0.5412, 0.5569, 0.5529, 0.5176, 0.5020, 0.5020,\n",
      "          0.5255, 0.5255, 0.5804, 0.5176, 0.2549, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.4157, 0.7843,\n",
      "          0.5529, 0.5490, 0.5490, 0.5725, 0.5647, 0.5216, 0.5020, 0.5059,\n",
      "          0.5255, 0.5373, 0.6275, 0.4745, 0.3608, 0.0000, 0.0000, 0.0078,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.6784, 0.7451,\n",
      "          0.5412, 0.5373, 0.5569, 0.5961, 0.5804, 0.5333, 0.5059, 0.5059,\n",
      "          0.5255, 0.5216, 0.6902, 0.4784, 0.4118, 0.0549, 0.0000, 0.0157,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0039, 0.0000, 0.0000, 0.6745, 0.7098,\n",
      "          0.5490, 0.5176, 0.5569, 0.5882, 0.5882, 0.5373, 0.5098, 0.5020,\n",
      "          0.5255, 0.4941, 0.7216, 0.4941, 0.4157, 0.1647, 0.0000, 0.0078,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0078, 0.0000, 0.1137, 0.7451, 0.6941,\n",
      "          0.5686, 0.5216, 0.5686, 0.5804, 0.5804, 0.5490, 0.5176, 0.5020,\n",
      "          0.5216, 0.4941, 0.7412, 0.5098, 0.4118, 0.2863, 0.0000, 0.0039,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0039, 0.0000, 0.3137, 0.7451, 0.6784,\n",
      "          0.5647, 0.5216, 0.6000, 0.6039, 0.5686, 0.5490, 0.5216, 0.5098,\n",
      "          0.5176, 0.5333, 0.7098, 0.5216, 0.3765, 0.4745, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.6118, 0.7059, 0.6431,\n",
      "          0.5647, 0.4745, 0.5961, 0.6157, 0.5490, 0.5373, 0.5059, 0.4941,\n",
      "          0.4941, 0.5333, 0.7098, 0.4941, 0.2980, 0.5333, 0.0667, 0.0000,\n",
      "          0.0157, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0039, 0.0000, 0.0000, 0.6510, 0.6941, 0.6353,\n",
      "          0.6000, 0.5804, 0.7059, 0.7569, 0.6745, 0.6196, 0.6000, 0.5961,\n",
      "          0.5843, 0.7294, 0.7725, 0.6039, 0.3804, 0.6627, 0.3922, 0.0000,\n",
      "          0.0353, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0039, 0.0000, 0.0000, 0.5020, 0.4157, 0.3843,\n",
      "          0.3216, 0.2392, 0.3137, 0.4314, 0.4549, 0.3961, 0.3529, 0.3137,\n",
      "          0.2824, 0.3451, 0.3529, 0.1725, 0.1451, 0.2745, 0.1137, 0.0000,\n",
      "          0.0039, 0.0000, 0.0000, 0.0000]]])\n",
      "y[0]: tensor(2)\n"
     ]
    }
   ],
   "source": [
    "for batch_idx, (x, y) in enumerate(train_loader):\n",
    "    print(f\"Batch {batch_idx}:\")\n",
    "    print(\"x shape:\", x.shape)\n",
    "    print(\"y shape:\", y.shape)\n",
    "    print(\"x[0]:\", x[0])\n",
    "    print(\"y[0]:\", y[0])\n",
    "    if batch_idx == 1:  # stop after 2 batches\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ForwardCorrectedCE(nn.Module):\n",
    "    \"\"\"\n",
    "    Forward loss correction: minimizes CE between T^T p and noisy labels.\n",
    "    T: class-transition matrix where T[i,j] = P(S=j | Y=i). Shape [C,C].\n",
    "    \"\"\"\n",
    "    def __init__(self, T):\n",
    "        super().__init__()\n",
    "        self.register_buffer('T', T)  # [C,C]\n",
    "\n",
    "    def forward(self, logits, y_noisy):\n",
    "        # logits -> p(y|x)\n",
    "        p = F.softmax(logits, dim=1)  # [B,C]\n",
    "        # mix via T^T\n",
    "        mixed = torch.clamp(p @ self.T.t(), 1e-6, 1.0)\n",
    "        log_mixed = torch.log(mixed)\n",
    "        return F.nll_loss(log_mixed, y_noisy)\n",
    "\n",
    "\n",
    "class GeneralizedCrossEntropy(nn.Module):\n",
    "    \"\"\"\n",
    "    GCE loss: L_q(p, y) = (1 - p_y^q) / q, with q in (0,1].\n",
    "    q→1 recovers CE; smaller q is more robust to label noise.\n",
    "    \"\"\"\n",
    "    def __init__(self, q=0.7):\n",
    "        super().__init__()\n",
    "        assert 0 < q <= 1\n",
    "        self.q = q\n",
    "\n",
    "    def forward(self, logits, y):\n",
    "        p = F.softmax(logits, dim=1)\n",
    "        p_y = p.gather(1, y.view(-1,1)).clamp(min=1e-6, max=1.0)\n",
    "        if self.q == 1.0:\n",
    "            return -torch.log(p_y).mean()\n",
    "        return ((1 - p_y.pow(self.q)) / self.q).mean()\n",
    "    \n",
    "def rre_fse(T, T_prime, eps=1e-12): \n",
    "  \"\"\"\n",
    "  Return relative reconstruction frobenious error\n",
    "\n",
    "  Args:\n",
    "    V_hat: Clean data matrix (no-noise)\n",
    "    W: Basis matrix\n",
    "    H: Encoding matrix\n",
    "    eps: epsilon value to prevent division by zeo / stabilize the computation\n",
    "  \"\"\"\n",
    "  FE = np.linalg.norm( T - T_prime, ord='fro') # Calculating numerator value from fse error\n",
    "  denom = np.linalg.norm(T, ord='fro')  # Calculating denominator value\n",
    "  RRE = FE / (denom + eps)  # RRE calculation\n",
    "  return RRE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_block(cin, cout):\n",
    "    return nn.Sequential(\n",
    "        nn.Conv2d(cin, cout, 3, padding=1),\n",
    "        nn.BatchNorm2d(cout),\n",
    "        nn.ReLU(inplace=True),\n",
    "        nn.MaxPool2d(2)\n",
    "    )\n",
    "\n",
    "\n",
    "class SmallCNN28(nn.Module):\n",
    "    \"\"\"For 1×28×28 images.\"\"\"\n",
    "    def __init__(self, num_classes=3):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            conv_block(1, 32),  # 14x14\n",
    "            conv_block(32, 64), # 7x7\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(64*7*7, 128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(128, num_classes)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "\n",
    "class SmallCNNCifar(nn.Module):\n",
    "    \"\"\"For 3×32×32 images.\"\"\"\n",
    "    def __init__(self, num_classes=3):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            conv_block(3, 32),   # 16x16\n",
    "            conv_block(32, 64),  # 8x8\n",
    "            conv_block(64, 128), # 4x4\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(128*4*4, 256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(256, num_classes)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "\n",
    "def make_model(is_cifar, num_classes=3):\n",
    "    return SmallCNNCifar(num_classes) if is_cifar else SmallCNN28(num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def predict_proba(model, loader, device):\n",
    "    model.eval()\n",
    "    probs = []\n",
    "    ys = []\n",
    "    for xb, yb in loader:\n",
    "        xb = xb.to(device)\n",
    "        logits = model(xb)\n",
    "        p = F.softmax(logits, dim=1).cpu().numpy()\n",
    "        probs.append(p)\n",
    "        ys.append(yb.numpy())\n",
    "    return np.concatenate(probs), np.concatenate(ys)\n",
    "\n",
    "\n",
    "def estimate_transition_anchor(t, train_loader, is_cifar, num_classes=3, device='cpu', epochs=5):\n",
    "    \"\"\"\n",
    "    Simple anchor/confident-example estimator (Patrini et al., 2017 style):\n",
    "    1) Train a base classifier on noisy data.\n",
    "    2) Get p(y|x) on training set.\n",
    "    3) For each clean class i, find indices whose predicted argmax == noisy label == i and with high confidence.\n",
    "    4) For those indices, estimate column i of T as average of empirical noisy label distribution given model predicts i.\n",
    "    Here: since we only have noisy labels S, we approximate T[:, i] ≈ E[ onehot(S) | argmax p = i, p_i >= τ ].\n",
    "    Normalize columns to sum to 1.\n",
    "    \"\"\"\n",
    "    import torch.optim as optim\n",
    "\n",
    "\n",
    "    device = torch.device(device)\n",
    "    model = make_model(is_cifar, num_classes=num_classes).to(device)\n",
    "\n",
    "    optimizer = optim.Adam(model.parameters(), lr=1e-3, weight_decay=1e-4)\n",
    "\n",
    "    # quick warmup training on noisy labels\n",
    "    for _ in range(epochs):\n",
    "        train_one_epoch(model, train_loader, optimizer, criterion=None, device=device)  # default CE inside\n",
    "\n",
    "    # collect probs & noisy labels\n",
    "    probs, y_noisy = predict_proba(model, train_loader, device)\n",
    "    preds = probs.argmax(axis=1)\n",
    "    maxp = probs.max(axis=1)\n",
    "\n",
    "    C = num_classes\n",
    "    T = t\n",
    "    # choose class-wise thresholds based on quantiles for stability\n",
    "    for i in range(C):\n",
    "        idx = np.where(preds == i)[0]\n",
    "        if idx.size == 0:\n",
    "            T[:, i] = np.ones(C) / C\n",
    "            continue\n",
    "        # high-confidence subset (top 30% by p_i)\n",
    "        conf = maxp[idx]\n",
    "        if conf.size > 50:\n",
    "            tau = np.quantile(conf, 0.7)\n",
    "        else:\n",
    "            tau = np.min(conf)  # keep all if tiny\n",
    "        keep = idx[conf >= tau]\n",
    "        if keep.size == 0:\n",
    "            keep = idx\n",
    "        # empirical distribution of noisy labels in this confident set\n",
    "        hist = np.bincount(y_noisy[keep], minlength=C).astype(np.float64)\n",
    "        if hist.sum() == 0:\n",
    "            T[:, i] = np.ones(C) / C\n",
    "        else:\n",
    "            T[:, i] = hist / hist.sum()\n",
    "\n",
    "    # column-normalize\n",
    "    colsum = T.sum(axis=0, keepdims=True)\n",
    "    T = np.divide(T, np.maximum(colsum, 1e-8))\n",
    "    return T.astype(np.float32)\n",
    "\n",
    "\n",
    "def estimate_transition_trevision(T_init,train_loader, is_cifar, num_classes=3, device=\"cpu\", epochs=5, lambda_reg=0.01, lr_t=1e-4):\n",
    "    \"\"\"\n",
    "    T-Revision version of transition-matrix estimation.\n",
    "\n",
    "    Args:\n",
    "        T_init (np.ndarray or torch.Tensor): initial transition matrix [C, C]\n",
    "        train_loader: noisy training dataloader\n",
    "        is_cifar (bool): model type selector\n",
    "        num_classes (int): number of classes\n",
    "        device (str): 'cpu', 'cuda', or 'mps'\n",
    "        epochs (int): warm-up epochs\n",
    "        lambda_reg (float): regularization to keep T close to T_init\n",
    "        lr_t (float): learning rate for T refinement\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: refined transition matrix T' (shape [C,C])\n",
    "    \"\"\"\n",
    "\n",
    "    device = torch.device(device)\n",
    "    C = num_classes\n",
    "\n",
    "    # ---------------------------\n",
    "    # 1. Base model setup\n",
    "    # ---------------------------\n",
    "    model = make_model(is_cifar, num_classes=C).to(device)\n",
    "    opt_model = optim.Adam(model.parameters(), lr=1e-3, weight_decay=1e-4)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    # quick warmup to get p(y|x)\n",
    "    for _ in range(epochs):\n",
    "        train_one_epoch(model, train_loader, opt_model, criterion=criterion, device=device)\n",
    "\n",
    "    # ---------------------------\n",
    "    # 2. Get predicted probabilities & noisy labels\n",
    "    # ---------------------------\n",
    "    probs, y_noisy = predict_proba(model, train_loader, device)\n",
    "    preds = probs.argmax(axis=1)\n",
    "    maxp = probs.max(axis=1)\n",
    "\n",
    "    # ---------------------------\n",
    "    # 3. Initialize learnable ΔT (T-Revision)\n",
    "    # ---------------------------\n",
    "\n",
    "    T_init_torch = torch.tensor(T_init, dtype=torch.float32, device=device)\n",
    "    \n",
    "    delta_T = nn.Parameter(torch.zeros_like(T_init_torch))\n",
    "    optimizer_T = optim.Adam([delta_T], lr=lr_t)\n",
    "\n",
    "    # ---------------------------\n",
    "    # 4. Optimize ΔT using forward-corrected loss\n",
    "    # ---------------------------\n",
    "    for _ in range(epochs):\n",
    "        optimizer_T.zero_grad()\n",
    "        T_prime = T_init_torch + delta_T\n",
    "\n",
    "        # enforce nonnegativity & normalization\n",
    "        #T_prime = torch.clamp(T_prime, min=1e-6)\n",
    "        #T_prime = T_prime / T_prime.sum(dim=0, keepdim=True)\n",
    "\n",
    "        # predicted noisy-label probs\n",
    "        p = torch.tensor(probs, dtype=torch.float32, device=device)\n",
    "        noisy_pred = torch.clamp(p @ T_prime.t(), 1e-6, 1.0)\n",
    "        log_noisy = torch.log(noisy_pred)\n",
    "\n",
    "        y_t = torch.tensor(y_noisy, dtype=torch.long, device=device)\n",
    "        loss_ce = nn.NLLLoss()(log_noisy, y_t)\n",
    "        reg = lambda_reg * torch.norm(T_prime - T_init_torch, p=\"fro\")\n",
    "        loss = loss_ce + reg\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer_T.step()\n",
    "        print(delta_T.grad.abs().mean().item())\n",
    "\n",
    "    # normalize final T\n",
    "    T_final = T_init_torch + delta_T.data\n",
    "    T_final = torch.clamp(T_final, min=1e-6)\n",
    "    T_final = T_final / T_final.sum(dim=0, keepdim=True)\n",
    "\n",
    "    return T_final.detach().cpu().numpy().astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "\n",
    "def estimate_transition_trevision(\n",
    "    T_init,\n",
    "    train_loader,\n",
    "    is_cifar,\n",
    "    num_classes=3,\n",
    "    device=\"cpu\",\n",
    "    epochs=5,\n",
    "    lambda_reg=1e-4,\n",
    "    lr_t=5e-3,\n",
    "    lr_model=1e-3,\n",
    "    warmup_epochs=3,\n",
    "    log_every=1\n",
    "):\n",
    "    \"\"\"\n",
    "    Refine transition matrix using T-Revision method (Patrini et al. style).\n",
    "\n",
    "    Args:\n",
    "        T_init (np.ndarray or torch.Tensor): initial transition matrix [C, C]\n",
    "        train_loader: noisy dataloader (x, y_noisy)\n",
    "        is_cifar (bool): dataset selector for model architecture\n",
    "        num_classes (int): number of classes\n",
    "        device (str): 'cpu', 'cuda', or 'mps'\n",
    "        epochs (int): number of refinement epochs for ΔT\n",
    "        lambda_reg (float): regularization to keep T close to T_init\n",
    "        lr_t (float): learning rate for ΔT\n",
    "        lr_model (float): learning rate for model warm-up\n",
    "        warmup_epochs (int): number of model warm-up epochs\n",
    "        log_every (int): print interval\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: refined transition matrix [C, C]\n",
    "    \"\"\"\n",
    "\n",
    "    device = torch.device(device)\n",
    "    C = num_classes\n",
    "\n",
    "    # ---------------------------\n",
    "    # 1. Base model setup\n",
    "    # ---------------------------\n",
    "    model = make_model(is_cifar, num_classes=C).to(device)\n",
    "    opt_model = optim.Adam(model.parameters(), lr=lr_model, weight_decay=1e-4)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    # Warm-up (train classifier on noisy labels)\n",
    "    print(f\"[Warm-up] training base classifier for {warmup_epochs} epochs...\")\n",
    "    for e in range(warmup_epochs):\n",
    "        train_one_epoch(model, train_loader, opt_model, criterion=criterion, device=device)\n",
    "        print(f\"  done epoch {e+1}/{warmup_epochs}\")\n",
    "\n",
    "    # ---------------------------\n",
    "    # 2. Get predicted probabilities\n",
    "    # ---------------------------\n",
    "    probs, y_noisy = predict_proba(model, train_loader, device)\n",
    "    p = torch.tensor(probs, dtype=torch.float32, device=device)\n",
    "    y_t = torch.tensor(y_noisy, dtype=torch.long, device=device)\n",
    "\n",
    "    # ---------------------------\n",
    "    # 3. Initialize learnable ΔT (T-Revision)\n",
    "    # ---------------------------\n",
    "    T_init_torch = torch.tensor(T_init, dtype=torch.float32, device=device)\n",
    "    delta_T = nn.Parameter(torch.zeros_like(T_init_torch))\n",
    "    optimizer_T = optim.Adam([delta_T], lr=lr_t)\n",
    "\n",
    "    print(f\"[Optimization] refining transition matrix for {epochs} epochs...\")\n",
    "\n",
    "    # ---------------------------\n",
    "    # 4. Optimize ΔT\n",
    "    # ---------------------------\n",
    "    for ep in range(epochs):\n",
    "        optimizer_T.zero_grad()\n",
    "\n",
    "        # Proposed transition\n",
    "        T_prime = T_init_torch + delta_T\n",
    "        T_prime = torch.clamp(T_prime, min=1e-6)\n",
    "\n",
    "        # Forward correction: p(y_noisy | x) = p(y|x) * T'\n",
    "        noisy_pred = torch.clamp(p @ T_prime.t(), 1e-6, 1.0)\n",
    "        log_noisy = torch.log(noisy_pred)\n",
    "\n",
    "        # Loss = NLL + regularization\n",
    "        loss_ce = nn.NLLLoss()(log_noisy, y_t)\n",
    "        reg = lambda_reg * torch.norm(T_prime - T_init_torch, p=\"fro\")\n",
    "        loss = loss_ce + reg\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer_T.step()\n",
    "\n",
    "        if (ep + 1) % log_every == 0:\n",
    "            grad_norm = delta_T.grad.abs().mean().item() if delta_T.grad is not None else 0\n",
    "            print(f\"Epoch {ep+1}/{epochs} | loss={loss.item():.5f} | grad={grad_norm:.5e}\")\n",
    "\n",
    "    # ---------------------------\n",
    "    # 5. Normalize once at the end\n",
    "    # ---------------------------\n",
    "    T_final = T_init_torch + delta_T.data\n",
    "    T_final = torch.clamp(T_final, min=1e-6)\n",
    "    T_final = T_final / T_final.sum(dim=0, keepdim=True)\n",
    "\n",
    "    print(\"\\n[Done] Refined Transition Matrix:\")\n",
    "    print(T_final.detach().cpu().numpy())\n",
    "\n",
    "    return T_final.detach().cpu().numpy().astype(np.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_transition_trevision(\n",
    "    T_init,\n",
    "    train_loader,\n",
    "    is_cifar,\n",
    "    num_classes=3,\n",
    "    device=\"cpu\",\n",
    "    warmup_epochs=15,\n",
    "    refine_epochs=60,\n",
    "    lr_model=1e-3,\n",
    "    lr_t=5e-3,\n",
    "    lambda_reg=1e-4,\n",
    "    q=0.7,\n",
    "):\n",
    "    \"\"\"\n",
    "    Improved T-Revision: row-stochastic, stable, no T_init overwrite.\n",
    "    \"\"\"\n",
    "\n",
    "    device = torch.device(device)\n",
    "    C = num_classes\n",
    "\n",
    "    # ----- 1. Warm-up base model -----\n",
    "    model = make_model(is_cifar, num_classes=C).to(device)\n",
    "    opt_model = optim.Adam(model.parameters(), lr=lr_model, weight_decay=1e-4)\n",
    "    criterion = GeneralizedCrossEntropy(q=q)\n",
    "    print(f\"[Warm-up] {warmup_epochs} epochs...\")\n",
    "    for e in range(warmup_epochs):\n",
    "        train_one_epoch(model, train_loader, opt_model, criterion=criterion, device=device)\n",
    "\n",
    "    # ----- 2. Predict posteriors -----\n",
    "    probs, y_noisy = predict_proba(model, train_loader, device)\n",
    "    p = torch.tensor(probs, dtype=torch.float32, device=device)\n",
    "    y_t = torch.tensor(y_noisy, dtype=torch.long, device=device)\n",
    "\n",
    "    # ----- 3. Learnable ΔT in logit-space -----\n",
    "    logits_T = nn.Parameter(torch.log(torch.tensor(T_init + 1e-6, dtype=torch.float32, device=device)))\n",
    "    optT = optim.Adam([logits_T], lr=lr_t)\n",
    "\n",
    "    print(f\"[Refinement] {refine_epochs} epochs...\")\n",
    "    for ep in range(refine_epochs):\n",
    "        optT.zero_grad()\n",
    "        T_prime = torch.softmax(logits_T, dim=1)          # rows sum to 1\n",
    "        noisy_pred = torch.clamp(p @ T_prime, 1e-8, 1.0)  # no transpose\n",
    "        loss_ce = nn.NLLLoss()(torch.log(noisy_pred), y_t)\n",
    "        reg = lambda_reg * torch.norm(T_prime - torch.tensor(T_init, device=device), p='fro')\n",
    "        loss = loss_ce + reg\n",
    "        loss.backward(); optT.step()\n",
    "        if (ep + 1) % 10 == 0:\n",
    "            print(f\"Epoch {ep+1}: loss={loss.item():.5f}\")\n",
    "\n",
    "    T_final = torch.softmax(logits_T, dim=1).detach().cpu().numpy()\n",
    "    return T_final.astype(np.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(model, loader, device):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for xb, yb in loader:\n",
    "            xb, yb = xb.to(device), yb.to(device)\n",
    "            logits = model(xb)\n",
    "            pred = logits.argmax(dim=1)\n",
    "            correct += (pred == yb).sum().item()\n",
    "            total += yb.numel()\n",
    "    return correct / max(total, 1)\n",
    "\n",
    "\n",
    "def train_one_epoch(model, loader, optimizer, criterion=None, device='cpu'):\n",
    "    model.train()\n",
    "    if criterion is None:\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "    total_loss = 0.0\n",
    "    for xb, yb in loader:\n",
    "        xb, yb = xb.to(device), yb.to(device)\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        logits = model(xb)\n",
    "        loss = criterion(logits, yb)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item() * yb.size(0)\n",
    "    return total_loss / len(loader.dataset)\n",
    "\n",
    "\n",
    "def fit_model(model, train_loader, val_loader, device, loss_name='ce', T=None, q=0.7, beta=0.2, epochs=10, lr=1e-3):\n",
    "    import torch.optim as optim\n",
    "\n",
    "    if loss_name == 'forward':\n",
    "        assert T is not None, \"Forward correction requires known/estimated T\"\n",
    "        criterion = ForwardCorrectedCE(torch.tensor(T, dtype=torch.float32, device=device))\n",
    "    elif loss_name == 'gce':\n",
    "        criterion = GeneralizedCrossEntropy(q=q)\n",
    "    else:\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=1e-4)\n",
    "\n",
    "    best_val = -np.inf\n",
    "    best_state = None\n",
    "\n",
    "    for _ in range(epochs):\n",
    "        train_one_epoch(model, train_loader, optimizer, criterion, device)\n",
    "        # early stopping on val accuracy (cheap)\n",
    "        val_acc = accuracy(model, val_loader, device)\n",
    "        if val_acc > best_val:\n",
    "            best_val = val_acc\n",
    "            best_state = {k: v.detach().cpu().clone() for k, v in model.state_dict().items()}\n",
    "\n",
    "    if best_state is not None:\n",
    "        model.load_state_dict(best_state)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed):\n",
    "    import random\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "DATA_PATHS = {\n",
    "    'fashion03': os.path.join('data', 'FashionMNIST0.3.npz'),\n",
    "    'fashion06': os.path.join('data', 'FashionMNIST0.6.npz'),\n",
    "    'cifar':     os.path.join('data', 'CIFAR.npz'),\n",
    "}\n",
    "def known_T_fashion_03():\n",
    "    # T = [[0.7,0.3,0.0],[0.0,0.7,0.3],[0.0,0.0,0.7]]\n",
    "    return np.array([[0.7,0.3,0.0],\n",
    "                     [0.0,0.7,0.3],\n",
    "                     [0.0,0.0,0.7]], dtype=np.float32)\n",
    "\n",
    "def known_T_fashion_06():\n",
    "    # [[0.3,0.4,0.3],[0.4,0.3,0.3],[0.3,0.3,0.4]]\n",
    "    return np.array([[0.3,0.4,0.3],\n",
    "                     [0.4,0.3,0.3],\n",
    "                     [0.3,0.3,0.4]], dtype=np.float32)\n",
    "\n",
    "\n",
    "def pick_known_T(tag):\n",
    "    if tag == 'fashion03':\n",
    "        return known_T_fashion_03()\n",
    "    elif tag == 'fashion06':\n",
    "        return known_T_fashion_06()\n",
    "    else:\n",
    "        return None\n",
    "C = 3\n",
    "\n",
    "def run_once(args, seed):\n",
    "    set_seed(seed)\n",
    "    device = torch.device('mps')\n",
    "\n",
    "    # load data\n",
    "    Xtr, Str, Xts, Yts = load_npz(DATA_PATHS[args['dataset']])\n",
    "\n",
    "    # loaders for this split\n",
    "    train_loader, val_loader, is_cifar = make_loaders(Xtr, Str, batch_size=args['batch_size'], seed=seed)\n",
    "    test_loader = make_test_loader(Xts, Yts, batch_size=512)\n",
    "  \n",
    "    # choose model\n",
    "    model = make_model(is_cifar).to(device)\n",
    "\n",
    "    # Transition matrix\n",
    "    T = None\n",
    "    if args['loss'] == 'forward' or args['loss'] == 'gce':\n",
    "        if args['estimate_T'] or args['dataset']=='cifar':\n",
    "            print('estimating matrix')\n",
    "            T = np.zeros((C, C), dtype=np.float64)\n",
    "            T = estimate_transition_anchor(T, train_loader, is_cifar, device=device, epochs=args['est_epochs'])\n",
    "            T = estimate_transition_trevision(T, train_loader, is_cifar, device=device)\n",
    "            print(T)\n",
    "        else:\n",
    "            T = pick_known_T(args['dataset'])\n",
    "            if T is None:\n",
    "                raise ValueError(\"Forward loss selected but no known T for this dataset; use --estimate_T.\")\n",
    "            \n",
    "\n",
    "\n",
    "    # fit\n",
    "    model = fit_model(\n",
    "        model,\n",
    "        train_loader,\n",
    "        val_loader,\n",
    "        device,\n",
    "        loss_name=args['loss'],\n",
    "        T=T,\n",
    "        q=args['q'],\n",
    "        beta=args['beta'],\n",
    "        epochs=args['epochs'],\n",
    "        lr=args['lr'],\n",
    "    )\n",
    "\n",
    "    # evaluate on clean test set\n",
    "    test_acc = accuracy(model, test_loader, device)\n",
    "    return float(test_acc), (T.tolist() if T is not None else None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = ['forward','gce']\n",
    "datasets = ['cifar', \"fashion03\", \"fashion06\"]\n",
    "base = {\n",
    "    \"runs\":10,\n",
    "    \"epochs\": 10,\n",
    "    \"estimate_T\":True,\n",
    "    \"loss\":'forward',\n",
    "    \"batch_size\":4096,\n",
    "    \"q\":0.6,\n",
    "    \"est_epochs\":5,\n",
    "    \"beta\":0.2,\n",
    "    \"lr\":1e-3,\n",
    "    \"num_classes\":3,\n",
    "    \"device\":'mps'\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'runs': 10, 'epochs': 10, 'estimate_T': True, 'loss': 'forward', 'batch_size': 4096, 'q': 0.6, 'est_epochs': 5, 'beta': 0.2, 'lr': 0.001, 'num_classes': 3, 'device': 'mps', 'dataset': 'cifar', 'out': 'results_forward_2025-10-28 22:11:44_0.json'}, {'runs': 10, 'epochs': 10, 'estimate_T': True, 'loss': 'gce', 'batch_size': 4096, 'q': 0.6, 'est_epochs': 5, 'beta': 0.2, 'lr': 0.001, 'num_classes': 3, 'device': 'mps', 'dataset': 'cifar', 'out': 'results_gce_2025-10-28 22:11:44_0.json'}, {'runs': 10, 'epochs': 10, 'estimate_T': True, 'loss': 'forward', 'batch_size': 4096, 'q': 0.6, 'est_epochs': 5, 'beta': 0.2, 'lr': 0.001, 'num_classes': 3, 'device': 'mps', 'dataset': 'fashion03', 'out': 'results_forward_2025-10-28 22:11:44_1.json'}, {'runs': 10, 'epochs': 10, 'estimate_T': True, 'loss': 'gce', 'batch_size': 4096, 'q': 0.6, 'est_epochs': 5, 'beta': 0.2, 'lr': 0.001, 'num_classes': 3, 'device': 'mps', 'dataset': 'fashion03', 'out': 'results_gce_2025-10-28 22:11:44_1.json'}, {'runs': 10, 'epochs': 10, 'estimate_T': True, 'loss': 'forward', 'batch_size': 4096, 'q': 0.6, 'est_epochs': 5, 'beta': 0.2, 'lr': 0.001, 'num_classes': 3, 'device': 'mps', 'dataset': 'fashion06', 'out': 'results_forward_2025-10-28 22:11:44_2.json'}, {'runs': 10, 'epochs': 10, 'estimate_T': True, 'loss': 'gce', 'batch_size': 4096, 'q': 0.6, 'est_epochs': 5, 'beta': 0.2, 'lr': 0.001, 'num_classes': 3, 'device': 'mps', 'dataset': 'fashion06', 'out': 'results_gce_2025-10-28 22:11:44_2.json'}]\n"
     ]
    }
   ],
   "source": [
    "now = datetime.now()\n",
    "now = now.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "# create each cfg\n",
    "configs = []\n",
    "for i, ds in enumerate(datasets):\n",
    "    for loss in losses:\n",
    "        cfg = {**base, \"dataset\": ds, \"out\": 'results_'+loss+'_'+now+'_'+str(i)+'.json', \"loss\":loss}\n",
    "        configs.append(cfg)\n",
    "\n",
    "print(configs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "estimating matrix\n",
      "[Warm-up] 15 epochs...\n",
      "[Refinement] 60 epochs...\n",
      "Epoch 10: loss=1.09633\n",
      "Epoch 20: loss=1.09464\n",
      "Epoch 30: loss=1.09303\n",
      "Epoch 40: loss=1.09153\n",
      "Epoch 50: loss=1.09012\n",
      "Epoch 60: loss=1.08883\n",
      "[[0.3889811  0.354772   0.25624692]\n",
      " [0.22579259 0.5197434  0.254464  ]\n",
      " [0.2901467  0.2333899  0.47646347]]\n",
      "Run 01/10: test acc = 33.33%\n",
      "mps: 1 steps -> 39.54 sec | avg 39541.2 ms/step\n",
      "estimating matrix\n",
      "[Warm-up] 15 epochs...\n",
      "[Refinement] 60 epochs...\n",
      "Epoch 10: loss=1.09429\n",
      "Epoch 20: loss=1.09272\n",
      "Epoch 30: loss=1.09132\n",
      "Epoch 40: loss=1.09011\n",
      "Epoch 50: loss=1.08910\n",
      "Epoch 60: loss=1.08826\n",
      "[[0.45679858 0.2831707  0.26003075]\n",
      " [0.24708967 0.52224725 0.23066308]\n",
      " [0.2615049  0.21988928 0.51860577]]\n",
      "Run 02/10: test acc = 33.33%\n",
      "mps: 2 steps -> 35.94 sec | avg 17970.5 ms/step\n",
      "estimating matrix\n",
      "[Warm-up] 15 epochs...\n",
      "[Refinement] 60 epochs...\n",
      "Epoch 10: loss=1.09538\n",
      "Epoch 20: loss=1.09414\n",
      "Epoch 30: loss=1.09298\n",
      "Epoch 40: loss=1.09190\n",
      "Epoch 50: loss=1.09093\n",
      "Epoch 60: loss=1.09006\n",
      "[[0.43567258 0.30853808 0.25578934]\n",
      " [0.27391428 0.47243965 0.2536461 ]\n",
      " [0.2256974  0.27008212 0.5042205 ]]\n",
      "Run 03/10: test acc = 44.53%\n",
      "mps: 3 steps -> 35.26 sec | avg 11754.7 ms/step\n",
      "estimating matrix\n",
      "[Warm-up] 15 epochs...\n",
      "[Refinement] 60 epochs...\n",
      "Epoch 10: loss=1.09638\n",
      "Epoch 20: loss=1.09466\n",
      "Epoch 30: loss=1.09308\n",
      "Epoch 40: loss=1.09166\n",
      "Epoch 50: loss=1.09040\n",
      "Epoch 60: loss=1.08929\n",
      "[[0.4399707  0.31178963 0.24823974]\n",
      " [0.2615804  0.4744414  0.2639782 ]\n",
      " [0.25028482 0.2805025  0.46921265]]\n",
      "Run 04/10: test acc = 33.33%\n",
      "mps: 4 steps -> 35.23 sec | avg 8807.1 ms/step\n",
      "estimating matrix\n",
      "[Warm-up] 15 epochs...\n",
      "[Refinement] 60 epochs...\n",
      "Epoch 10: loss=1.09259\n",
      "Epoch 20: loss=1.09042\n",
      "Epoch 30: loss=1.08922\n",
      "Epoch 40: loss=1.08823\n",
      "Epoch 50: loss=1.08727\n",
      "Epoch 60: loss=1.08642\n",
      "[[0.49515253 0.2861289  0.21871854]\n",
      " [0.22975577 0.5504545  0.21978971]\n",
      " [0.21488686 0.31104955 0.47406363]]\n",
      "Run 05/10: test acc = 33.33%\n",
      "mps: 5 steps -> 35.35 sec | avg 7070.1 ms/step\n",
      "estimating matrix\n",
      "[Warm-up] 15 epochs...\n",
      "[Refinement] 60 epochs...\n",
      "Epoch 10: loss=1.09581\n",
      "Epoch 20: loss=1.09465\n",
      "Epoch 30: loss=1.09364\n",
      "Epoch 40: loss=1.09275\n",
      "Epoch 50: loss=1.09197\n",
      "Epoch 60: loss=1.09129\n",
      "[[0.440075   0.30890623 0.25101873]\n",
      " [0.25146163 0.4614195  0.28711888]\n",
      " [0.23872337 0.25247246 0.5088042 ]]\n",
      "Run 06/10: test acc = 41.83%\n",
      "mps: 6 steps -> 36.30 sec | avg 6050.3 ms/step\n",
      "estimating matrix\n",
      "[Warm-up] 15 epochs...\n",
      "[Refinement] 60 epochs...\n",
      "Epoch 10: loss=1.10054\n",
      "Epoch 20: loss=1.09883\n",
      "Epoch 30: loss=1.09734\n",
      "Epoch 40: loss=1.09604\n",
      "Epoch 50: loss=1.09493\n",
      "Epoch 60: loss=1.09398\n",
      "[[0.48364022 0.22429559 0.29206422]\n",
      " [0.2704306  0.448242   0.2813274 ]\n",
      " [0.24649137 0.34461898 0.4088897 ]]\n",
      "Run 07/10: test acc = 47.63%\n",
      "mps: 7 steps -> 35.68 sec | avg 5097.7 ms/step\n",
      "estimating matrix\n",
      "[Warm-up] 15 epochs...\n",
      "[Refinement] 60 epochs...\n",
      "Epoch 10: loss=1.09677\n",
      "Epoch 20: loss=1.09592\n",
      "Epoch 30: loss=1.09515\n",
      "Epoch 40: loss=1.09450\n",
      "Epoch 50: loss=1.09394\n",
      "Epoch 60: loss=1.09349\n",
      "[[0.49250805 0.2580602  0.24943176]\n",
      " [0.24840495 0.47581732 0.2757778 ]\n",
      " [0.25513843 0.26162833 0.48323333]]\n",
      "Run 08/10: test acc = 51.70%\n",
      "mps: 8 steps -> 34.77 sec | avg 4345.9 ms/step\n",
      "estimating matrix\n",
      "[Warm-up] 15 epochs...\n",
      "[Refinement] 60 epochs...\n",
      "Epoch 10: loss=1.09621\n",
      "Epoch 20: loss=1.09452\n",
      "Epoch 30: loss=1.09295\n",
      "Epoch 40: loss=1.09152\n",
      "Epoch 50: loss=1.09022\n",
      "Epoch 60: loss=1.08905\n",
      "[[0.45052335 0.29200312 0.25747347]\n",
      " [0.26725358 0.43432897 0.29841745]\n",
      " [0.22883876 0.29731742 0.47384378]]\n",
      "Run 09/10: test acc = 48.40%\n",
      "mps: 9 steps -> 34.52 sec | avg 3835.2 ms/step\n",
      "estimating matrix\n",
      "[Warm-up] 15 epochs...\n",
      "[Refinement] 60 epochs...\n",
      "Epoch 10: loss=1.09698\n",
      "Epoch 20: loss=1.09535\n",
      "Epoch 30: loss=1.09444\n",
      "Epoch 40: loss=1.09356\n",
      "Epoch 50: loss=1.09270\n",
      "Epoch 60: loss=1.09190\n",
      "[[0.42067134 0.33489582 0.24443288]\n",
      " [0.23963991 0.4230344  0.33732572]\n",
      " [0.2467529  0.27073777 0.48250937]]\n",
      "Run 10/10: test acc = 39.20%\n",
      "mps: 10 steps -> 33.84 sec | avg 3384.0 ms/step\n",
      "========================================================================\n",
      "cifar | forward | mean±std over 10 runs: 40.66±6.81%\n",
      "Saved summary to results_forward_2025-10-28 22:11:44_0.json\n",
      "estimating matrix\n",
      "[Warm-up] 15 epochs...\n",
      "[Refinement] 60 epochs...\n",
      "Epoch 10: loss=1.09633\n",
      "Epoch 20: loss=1.09464\n",
      "Epoch 30: loss=1.09303\n",
      "Epoch 40: loss=1.09153\n",
      "Epoch 50: loss=1.09012\n",
      "Epoch 60: loss=1.08883\n",
      "[[0.3889811  0.354772   0.25624692]\n",
      " [0.22579259 0.5197434  0.254464  ]\n",
      " [0.2901467  0.2333899  0.47646347]]\n",
      "Run 01/10: test acc = 41.40%\n",
      "mps: 1 steps -> 34.83 sec | avg 34830.8 ms/step\n",
      "estimating matrix\n",
      "[Warm-up] 15 epochs...\n",
      "[Refinement] 60 epochs...\n",
      "Epoch 10: loss=1.09429\n",
      "Epoch 20: loss=1.09272\n",
      "Epoch 30: loss=1.09132\n",
      "Epoch 40: loss=1.09011\n",
      "Epoch 50: loss=1.08910\n",
      "Epoch 60: loss=1.08826\n",
      "[[0.45679858 0.2831707  0.26003075]\n",
      " [0.24708967 0.52224725 0.23066308]\n",
      " [0.2615049  0.21988928 0.51860577]]\n",
      "Run 02/10: test acc = 55.63%\n",
      "mps: 2 steps -> 35.21 sec | avg 17607.2 ms/step\n",
      "estimating matrix\n",
      "[Warm-up] 15 epochs...\n",
      "[Refinement] 60 epochs...\n",
      "Epoch 10: loss=1.09538\n",
      "Epoch 20: loss=1.09414\n",
      "Epoch 30: loss=1.09298\n",
      "Epoch 40: loss=1.09190\n",
      "Epoch 50: loss=1.09093\n",
      "Epoch 60: loss=1.09006\n",
      "[[0.43567258 0.30853808 0.25578934]\n",
      " [0.27391428 0.47243965 0.2536461 ]\n",
      " [0.2256974  0.27008212 0.5042205 ]]\n",
      "Run 03/10: test acc = 48.27%\n",
      "mps: 3 steps -> 34.48 sec | avg 11492.3 ms/step\n",
      "estimating matrix\n",
      "[Warm-up] 15 epochs...\n",
      "[Refinement] 60 epochs...\n",
      "Epoch 10: loss=1.09638\n",
      "Epoch 20: loss=1.09466\n",
      "Epoch 30: loss=1.09308\n",
      "Epoch 40: loss=1.09166\n",
      "Epoch 50: loss=1.09040\n",
      "Epoch 60: loss=1.08929\n",
      "[[0.4399707  0.31178963 0.24823974]\n",
      " [0.2615804  0.4744414  0.2639782 ]\n",
      " [0.25028482 0.2805025  0.46921265]]\n",
      "Run 04/10: test acc = 36.90%\n",
      "mps: 4 steps -> 34.56 sec | avg 8639.1 ms/step\n",
      "estimating matrix\n",
      "[Warm-up] 15 epochs...\n",
      "[Refinement] 60 epochs...\n",
      "Epoch 10: loss=1.09259\n",
      "Epoch 20: loss=1.09042\n",
      "Epoch 30: loss=1.08922\n",
      "Epoch 40: loss=1.08823\n",
      "Epoch 50: loss=1.08727\n",
      "Epoch 60: loss=1.08642\n",
      "[[0.49515253 0.2861289  0.21871854]\n",
      " [0.22975577 0.5504545  0.21978971]\n",
      " [0.21488686 0.31104955 0.47406363]]\n",
      "Run 05/10: test acc = 37.77%\n",
      "mps: 5 steps -> 36.21 sec | avg 7241.9 ms/step\n",
      "estimating matrix\n",
      "[Warm-up] 15 epochs...\n",
      "[Refinement] 60 epochs...\n",
      "Epoch 10: loss=1.09581\n",
      "Epoch 20: loss=1.09465\n",
      "Epoch 30: loss=1.09364\n",
      "Epoch 40: loss=1.09275\n",
      "Epoch 50: loss=1.09197\n",
      "Epoch 60: loss=1.09129\n",
      "[[0.440075   0.30890623 0.25101873]\n",
      " [0.25146163 0.4614195  0.28711888]\n",
      " [0.23872337 0.25247246 0.5088042 ]]\n",
      "Run 06/10: test acc = 33.77%\n",
      "mps: 6 steps -> 34.96 sec | avg 5827.4 ms/step\n",
      "estimating matrix\n",
      "[Warm-up] 15 epochs...\n",
      "[Refinement] 60 epochs...\n",
      "Epoch 10: loss=1.10054\n",
      "Epoch 20: loss=1.09883\n",
      "Epoch 30: loss=1.09734\n",
      "Epoch 40: loss=1.09604\n",
      "Epoch 50: loss=1.09493\n",
      "Epoch 60: loss=1.09398\n",
      "[[0.48364022 0.22429559 0.29206422]\n",
      " [0.2704306  0.448242   0.2813274 ]\n",
      " [0.24649137 0.34461898 0.4088897 ]]\n",
      "Run 07/10: test acc = 47.83%\n",
      "mps: 7 steps -> 34.54 sec | avg 4934.8 ms/step\n",
      "estimating matrix\n",
      "[Warm-up] 15 epochs...\n",
      "[Refinement] 60 epochs...\n",
      "Epoch 10: loss=1.09677\n",
      "Epoch 20: loss=1.09592\n",
      "Epoch 30: loss=1.09515\n",
      "Epoch 40: loss=1.09450\n",
      "Epoch 50: loss=1.09394\n",
      "Epoch 60: loss=1.09349\n",
      "[[0.49250805 0.2580602  0.24943176]\n",
      " [0.24840495 0.47581732 0.2757778 ]\n",
      " [0.25513843 0.26162833 0.48323333]]\n",
      "Run 08/10: test acc = 51.57%\n",
      "mps: 8 steps -> 26.55 sec | avg 3318.5 ms/step\n",
      "estimating matrix\n",
      "[Warm-up] 15 epochs...\n",
      "[Refinement] 60 epochs...\n",
      "Epoch 10: loss=1.09621\n",
      "Epoch 20: loss=1.09452\n",
      "Epoch 30: loss=1.09295\n",
      "Epoch 40: loss=1.09152\n",
      "Epoch 50: loss=1.09022\n",
      "Epoch 60: loss=1.08905\n",
      "[[0.45052335 0.29200312 0.25747347]\n",
      " [0.26725358 0.43432897 0.29841745]\n",
      " [0.22883876 0.29731742 0.47384378]]\n",
      "Run 09/10: test acc = 47.43%\n",
      "mps: 9 steps -> 22.89 sec | avg 2543.1 ms/step\n",
      "estimating matrix\n",
      "[Warm-up] 15 epochs...\n",
      "[Refinement] 60 epochs...\n",
      "Epoch 10: loss=1.09698\n",
      "Epoch 20: loss=1.09535\n",
      "Epoch 30: loss=1.09444\n",
      "Epoch 40: loss=1.09356\n",
      "Epoch 50: loss=1.09270\n",
      "Epoch 60: loss=1.09190\n",
      "[[0.42067134 0.33489582 0.24443288]\n",
      " [0.23963991 0.4230344  0.33732572]\n",
      " [0.2467529  0.27073777 0.48250937]]\n",
      "Run 10/10: test acc = 46.47%\n",
      "mps: 10 steps -> 22.83 sec | avg 2282.8 ms/step\n",
      "========================================================================\n",
      "cifar | gce | mean±std over 10 runs: 44.70±6.63%\n",
      "Saved summary to results_gce_2025-10-28 22:11:44_0.json\n",
      "estimating matrix\n",
      "[Warm-up] 15 epochs...\n",
      "[Refinement] 60 epochs...\n",
      "Epoch 10: loss=1.80319\n",
      "Epoch 20: loss=1.78622\n",
      "Epoch 30: loss=1.77125\n",
      "Epoch 40: loss=1.75808\n",
      "Epoch 50: loss=1.74641\n",
      "Epoch 60: loss=1.73595\n",
      "[[8.0789989e-01 1.1850477e-06 1.9209895e-01]\n",
      " [1.9292520e-01 7.9943871e-01 7.6361098e-03]\n",
      " [6.8786810e-03 2.2489445e-01 7.6822692e-01]]\n",
      "Run 01/10: test acc = 98.20%\n",
      "mps: 1 steps -> 21.26 sec | avg 21261.6 ms/step\n",
      "estimating matrix\n",
      "[Warm-up] 15 epochs...\n",
      "[Refinement] 60 epochs...\n",
      "Epoch 10: loss=1.78534\n",
      "Epoch 20: loss=1.76962\n",
      "Epoch 30: loss=1.75568\n",
      "Epoch 40: loss=1.74311\n",
      "Epoch 50: loss=1.73162\n",
      "Epoch 60: loss=1.72105\n",
      "[[8.0601150e-01 1.1893391e-06 1.9398737e-01]\n",
      " [1.9402227e-01 8.0429453e-01 1.6831890e-03]\n",
      " [1.2243634e-02 2.5018108e-01 7.3757529e-01]]\n",
      "Run 02/10: test acc = 97.20%\n",
      "mps: 2 steps -> 18.30 sec | avg 9148.3 ms/step\n",
      "estimating matrix\n",
      "[Warm-up] 15 epochs...\n",
      "[Refinement] 60 epochs...\n",
      "Epoch 10: loss=1.94103\n",
      "Epoch 20: loss=1.92299\n",
      "Epoch 30: loss=1.90686\n",
      "Epoch 40: loss=1.89221\n",
      "Epoch 50: loss=1.87868\n",
      "Epoch 60: loss=1.86610\n",
      "[[7.8781134e-01 1.1700240e-06 2.1218751e-01]\n",
      " [1.9708517e-01 8.0291361e-01 1.1899234e-06]\n",
      " [1.0022139e-02 2.5272420e-01 7.3725361e-01]]\n",
      "Run 03/10: test acc = 97.97%\n",
      "mps: 3 steps -> 18.58 sec | avg 6192.9 ms/step\n",
      "estimating matrix\n",
      "[Warm-up] 15 epochs...\n",
      "[Refinement] 60 epochs...\n",
      "Epoch 10: loss=1.85213\n",
      "Epoch 20: loss=1.83594\n",
      "Epoch 30: loss=1.82166\n",
      "Epoch 40: loss=1.80908\n",
      "Epoch 50: loss=1.79787\n",
      "Epoch 60: loss=1.78776\n",
      "[[8.0060190e-01 1.1942760e-06 1.9939692e-01]\n",
      " [1.9428454e-01 8.0263048e-01 3.0849956e-03]\n",
      " [1.5285537e-02 2.2801742e-01 7.5669700e-01]]\n",
      "Run 04/10: test acc = 98.27%\n",
      "mps: 4 steps -> 18.30 sec | avg 4575.1 ms/step\n",
      "estimating matrix\n",
      "[Warm-up] 15 epochs...\n",
      "[Refinement] 60 epochs...\n",
      "Epoch 10: loss=1.79834\n",
      "Epoch 20: loss=1.77957\n",
      "Epoch 30: loss=1.76271\n",
      "Epoch 40: loss=1.74766\n",
      "Epoch 50: loss=1.73422\n",
      "Epoch 60: loss=1.72215\n",
      "[[8.1077206e-01 1.1550899e-06 1.8922676e-01]\n",
      " [1.7848153e-01 7.9596072e-01 2.5557702e-02]\n",
      " [9.1282427e-03 2.1366240e-01 7.7720934e-01]]\n",
      "Run 05/10: test acc = 98.23%\n",
      "mps: 5 steps -> 18.29 sec | avg 3658.2 ms/step\n",
      "estimating matrix\n",
      "[Warm-up] 15 epochs...\n",
      "[Refinement] 60 epochs...\n",
      "Epoch 10: loss=1.67330\n",
      "Epoch 20: loss=1.65477\n",
      "Epoch 30: loss=1.63819\n",
      "Epoch 40: loss=1.62337\n",
      "Epoch 50: loss=1.61003\n",
      "Epoch 60: loss=1.59786\n",
      "[[7.9967833e-01 1.1599705e-06 2.0032048e-01]\n",
      " [1.8810320e-01 8.1189555e-01 1.2022929e-06]\n",
      " [1.1852784e-02 2.2708426e-01 7.6106298e-01]]\n",
      "Run 06/10: test acc = 97.63%\n",
      "mps: 6 steps -> 17.84 sec | avg 2973.5 ms/step\n",
      "estimating matrix\n",
      "[Warm-up] 15 epochs...\n",
      "[Refinement] 60 epochs...\n",
      "Epoch 10: loss=1.77343\n",
      "Epoch 20: loss=1.75587\n",
      "Epoch 30: loss=1.74006\n",
      "Epoch 40: loss=1.72552\n",
      "Epoch 50: loss=1.71200\n",
      "Epoch 60: loss=1.69939\n",
      "[[8.0231249e-01 1.1809141e-06 1.9768637e-01]\n",
      " [1.9579890e-01 8.0419993e-01 1.1934960e-06]\n",
      " [7.2771879e-03 2.5605032e-01 7.3667240e-01]]\n",
      "Run 07/10: test acc = 97.40%\n",
      "mps: 7 steps -> 17.71 sec | avg 2529.7 ms/step\n",
      "estimating matrix\n",
      "[Warm-up] 15 epochs...\n",
      "[Refinement] 60 epochs...\n",
      "Epoch 10: loss=1.66254\n",
      "Epoch 20: loss=1.64557\n",
      "Epoch 30: loss=1.62974\n",
      "Epoch 40: loss=1.61506\n",
      "Epoch 50: loss=1.60146\n",
      "Epoch 60: loss=1.58884\n",
      "[[7.9677838e-01 1.1497867e-06 2.0322047e-01]\n",
      " [1.8516701e-01 8.1377906e-01 1.0539099e-03]\n",
      " [2.1733051e-02 2.8807649e-01 6.9019043e-01]]\n",
      "Run 08/10: test acc = 97.63%\n",
      "mps: 8 steps -> 17.61 sec | avg 2201.3 ms/step\n",
      "estimating matrix\n",
      "[Warm-up] 15 epochs...\n",
      "[Refinement] 60 epochs...\n",
      "Epoch 10: loss=1.83567\n",
      "Epoch 20: loss=1.81881\n",
      "Epoch 30: loss=1.80394\n",
      "Epoch 40: loss=1.79081\n",
      "Epoch 50: loss=1.77906\n",
      "Epoch 60: loss=1.76840\n",
      "[[7.9913920e-01 1.1625009e-06 2.0085970e-01]\n",
      " [1.8967153e-01 8.1032723e-01 1.2060971e-06]\n",
      " [8.9820568e-03 2.3424080e-01 7.5677711e-01]]\n",
      "Run 09/10: test acc = 97.37%\n",
      "mps: 9 steps -> 17.71 sec | avg 1967.8 ms/step\n",
      "estimating matrix\n",
      "[Warm-up] 15 epochs...\n",
      "[Refinement] 60 epochs...\n",
      "Epoch 10: loss=1.79747\n",
      "Epoch 20: loss=1.77943\n",
      "Epoch 30: loss=1.76258\n",
      "Epoch 40: loss=1.74688\n",
      "Epoch 50: loss=1.73226\n",
      "Epoch 60: loss=1.71861\n",
      "[[8.0146766e-01 1.1714552e-06 1.9853118e-01]\n",
      " [1.9096236e-01 8.0903649e-01 1.2008879e-06]\n",
      " [2.0070996e-02 3.0020234e-01 6.7972672e-01]]\n",
      "Run 10/10: test acc = 96.97%\n",
      "mps: 10 steps -> 17.62 sec | avg 1762.2 ms/step\n",
      "========================================================================\n",
      "fashion03 | forward | mean±std over 10 runs: 97.69±0.44%\n",
      "Saved summary to results_forward_2025-10-28 22:11:44_1.json\n",
      "estimating matrix\n",
      "[Warm-up] 15 epochs...\n",
      "[Refinement] 60 epochs...\n",
      "Epoch 10: loss=1.80319\n",
      "Epoch 20: loss=1.78622\n",
      "Epoch 30: loss=1.77125\n",
      "Epoch 40: loss=1.75808\n",
      "Epoch 50: loss=1.74641\n",
      "Epoch 60: loss=1.73595\n",
      "[[8.0789989e-01 1.1850477e-06 1.9209895e-01]\n",
      " [1.9292520e-01 7.9943871e-01 7.6361098e-03]\n",
      " [6.8786810e-03 2.2489445e-01 7.6822692e-01]]\n",
      "Run 01/10: test acc = 96.43%\n",
      "mps: 1 steps -> 18.51 sec | avg 18513.6 ms/step\n",
      "estimating matrix\n",
      "[Warm-up] 15 epochs...\n",
      "[Refinement] 60 epochs...\n",
      "Epoch 10: loss=1.78534\n",
      "Epoch 20: loss=1.76962\n",
      "Epoch 30: loss=1.75568\n",
      "Epoch 40: loss=1.74311\n",
      "Epoch 50: loss=1.73162\n",
      "Epoch 60: loss=1.72105\n",
      "[[8.0601150e-01 1.1893391e-06 1.9398737e-01]\n",
      " [1.9402227e-01 8.0429453e-01 1.6831890e-03]\n",
      " [1.2243634e-02 2.5018108e-01 7.3757529e-01]]\n",
      "Run 02/10: test acc = 97.03%\n",
      "mps: 2 steps -> 17.62 sec | avg 8807.7 ms/step\n",
      "estimating matrix\n",
      "[Warm-up] 15 epochs...\n",
      "[Refinement] 60 epochs...\n",
      "Epoch 10: loss=1.94103\n",
      "Epoch 20: loss=1.92299\n",
      "Epoch 30: loss=1.90686\n",
      "Epoch 40: loss=1.89221\n",
      "Epoch 50: loss=1.87868\n",
      "Epoch 60: loss=1.86610\n",
      "[[7.8781134e-01 1.1700240e-06 2.1218751e-01]\n",
      " [1.9708517e-01 8.0291361e-01 1.1899234e-06]\n",
      " [1.0022139e-02 2.5272420e-01 7.3725361e-01]]\n",
      "Run 03/10: test acc = 96.30%\n",
      "mps: 3 steps -> 17.96 sec | avg 5985.5 ms/step\n",
      "estimating matrix\n",
      "[Warm-up] 15 epochs...\n",
      "[Refinement] 60 epochs...\n",
      "Epoch 10: loss=1.85213\n",
      "Epoch 20: loss=1.83594\n",
      "Epoch 30: loss=1.82166\n",
      "Epoch 40: loss=1.80908\n",
      "Epoch 50: loss=1.79787\n",
      "Epoch 60: loss=1.78776\n",
      "[[8.0060190e-01 1.1942760e-06 1.9939692e-01]\n",
      " [1.9428454e-01 8.0263048e-01 3.0849956e-03]\n",
      " [1.5285537e-02 2.2801742e-01 7.5669700e-01]]\n",
      "Run 04/10: test acc = 97.03%\n",
      "mps: 4 steps -> 18.09 sec | avg 4522.2 ms/step\n",
      "estimating matrix\n",
      "[Warm-up] 15 epochs...\n",
      "[Refinement] 60 epochs...\n",
      "Epoch 10: loss=1.79834\n",
      "Epoch 20: loss=1.77957\n",
      "Epoch 30: loss=1.76271\n",
      "Epoch 40: loss=1.74766\n",
      "Epoch 50: loss=1.73422\n",
      "Epoch 60: loss=1.72215\n",
      "[[8.1077206e-01 1.1550899e-06 1.8922676e-01]\n",
      " [1.7848153e-01 7.9596072e-01 2.5557702e-02]\n",
      " [9.1282427e-03 2.1366240e-01 7.7720934e-01]]\n",
      "Run 05/10: test acc = 97.53%\n",
      "mps: 5 steps -> 18.65 sec | avg 3730.7 ms/step\n",
      "estimating matrix\n",
      "[Warm-up] 15 epochs...\n",
      "[Refinement] 60 epochs...\n",
      "Epoch 10: loss=1.67330\n",
      "Epoch 20: loss=1.65477\n",
      "Epoch 30: loss=1.63819\n",
      "Epoch 40: loss=1.62337\n",
      "Epoch 50: loss=1.61003\n",
      "Epoch 60: loss=1.59786\n",
      "[[7.9967833e-01 1.1599705e-06 2.0032048e-01]\n",
      " [1.8810320e-01 8.1189555e-01 1.2022929e-06]\n",
      " [1.1852784e-02 2.2708426e-01 7.6106298e-01]]\n",
      "Run 06/10: test acc = 95.90%\n",
      "mps: 6 steps -> 18.32 sec | avg 3054.0 ms/step\n",
      "estimating matrix\n",
      "[Warm-up] 15 epochs...\n",
      "[Refinement] 60 epochs...\n",
      "Epoch 10: loss=1.77343\n",
      "Epoch 20: loss=1.75587\n",
      "Epoch 30: loss=1.74006\n",
      "Epoch 40: loss=1.72552\n",
      "Epoch 50: loss=1.71200\n",
      "Epoch 60: loss=1.69939\n",
      "[[8.0231249e-01 1.1809141e-06 1.9768637e-01]\n",
      " [1.9579890e-01 8.0419993e-01 1.1934960e-06]\n",
      " [7.2771879e-03 2.5605032e-01 7.3667240e-01]]\n",
      "Run 07/10: test acc = 97.73%\n",
      "mps: 7 steps -> 18.27 sec | avg 2609.9 ms/step\n",
      "estimating matrix\n",
      "[Warm-up] 15 epochs...\n",
      "[Refinement] 60 epochs...\n",
      "Epoch 10: loss=1.66254\n",
      "Epoch 20: loss=1.64557\n",
      "Epoch 30: loss=1.62974\n",
      "Epoch 40: loss=1.61506\n",
      "Epoch 50: loss=1.60146\n",
      "Epoch 60: loss=1.58884\n",
      "[[7.9677838e-01 1.1497867e-06 2.0322047e-01]\n",
      " [1.8516701e-01 8.1377906e-01 1.0539099e-03]\n",
      " [2.1733051e-02 2.8807649e-01 6.9019043e-01]]\n",
      "Run 08/10: test acc = 96.77%\n",
      "mps: 8 steps -> 18.02 sec | avg 2253.0 ms/step\n",
      "estimating matrix\n",
      "[Warm-up] 15 epochs...\n",
      "[Refinement] 60 epochs...\n",
      "Epoch 10: loss=1.83567\n",
      "Epoch 20: loss=1.81881\n",
      "Epoch 30: loss=1.80394\n",
      "Epoch 40: loss=1.79081\n",
      "Epoch 50: loss=1.77906\n",
      "Epoch 60: loss=1.76840\n",
      "[[7.9913920e-01 1.1625009e-06 2.0085970e-01]\n",
      " [1.8967153e-01 8.1032723e-01 1.2060971e-06]\n",
      " [8.9820568e-03 2.3424080e-01 7.5677711e-01]]\n",
      "Run 09/10: test acc = 97.63%\n",
      "mps: 9 steps -> 18.58 sec | avg 2063.9 ms/step\n",
      "estimating matrix\n",
      "[Warm-up] 15 epochs...\n",
      "[Refinement] 60 epochs...\n",
      "Epoch 10: loss=1.79747\n",
      "Epoch 20: loss=1.77943\n",
      "Epoch 30: loss=1.76258\n",
      "Epoch 40: loss=1.74688\n",
      "Epoch 50: loss=1.73226\n",
      "Epoch 60: loss=1.71861\n",
      "[[8.0146766e-01 1.1714552e-06 1.9853118e-01]\n",
      " [1.9096236e-01 8.0903649e-01 1.2008879e-06]\n",
      " [2.0070996e-02 3.0020234e-01 6.7972672e-01]]\n",
      "Run 10/10: test acc = 96.43%\n",
      "mps: 10 steps -> 18.48 sec | avg 1848.3 ms/step\n",
      "========================================================================\n",
      "fashion03 | gce | mean±std over 10 runs: 96.88±0.59%\n",
      "Saved summary to results_gce_2025-10-28 22:11:44_1.json\n",
      "estimating matrix\n",
      "[Warm-up] 15 epochs...\n",
      "[Refinement] 60 epochs...\n",
      "Epoch 10: loss=1.09374\n",
      "Epoch 20: loss=1.09214\n",
      "Epoch 30: loss=1.09080\n",
      "Epoch 40: loss=1.08967\n",
      "Epoch 50: loss=1.08871\n",
      "Epoch 60: loss=1.08791\n",
      "[[0.5545196  0.28542125 0.16005912]\n",
      " [0.24006483 0.46435776 0.2955775 ]\n",
      " [0.21198219 0.24000481 0.54801303]]\n",
      "Run 01/10: test acc = 83.70%\n",
      "mps: 1 steps -> 18.35 sec | avg 18354.4 ms/step\n",
      "estimating matrix\n",
      "[Warm-up] 15 epochs...\n",
      "[Refinement] 60 epochs...\n",
      "Epoch 10: loss=1.09411\n",
      "Epoch 20: loss=1.09118\n",
      "Epoch 30: loss=1.08851\n",
      "Epoch 40: loss=1.08610\n",
      "Epoch 50: loss=1.08396\n",
      "Epoch 60: loss=1.08209\n",
      "[[0.4804538  0.25562984 0.26391634]\n",
      " [0.2525786  0.50190824 0.24551316]\n",
      " [0.2754342  0.25633535 0.4682304 ]]\n",
      "Run 02/10: test acc = 86.07%\n",
      "mps: 2 steps -> 18.54 sec | avg 9270.2 ms/step\n",
      "estimating matrix\n",
      "[Warm-up] 15 epochs...\n",
      "[Refinement] 60 epochs...\n",
      "Epoch 10: loss=1.09428\n",
      "Epoch 20: loss=1.09195\n",
      "Epoch 30: loss=1.08984\n",
      "Epoch 40: loss=1.08797\n",
      "Epoch 50: loss=1.08635\n",
      "Epoch 60: loss=1.08498\n",
      "[[0.48145947 0.24750157 0.27103895]\n",
      " [0.24597254 0.5087396  0.2452879 ]\n",
      " [0.27445444 0.25092572 0.47461987]]\n",
      "Run 03/10: test acc = 90.67%\n",
      "mps: 3 steps -> 18.32 sec | avg 6106.1 ms/step\n",
      "estimating matrix\n",
      "[Warm-up] 15 epochs...\n",
      "[Refinement] 60 epochs...\n",
      "Epoch 10: loss=1.09794\n",
      "Epoch 20: loss=1.09584\n",
      "Epoch 30: loss=1.09394\n",
      "Epoch 40: loss=1.09225\n",
      "Epoch 50: loss=1.09079\n",
      "Epoch 60: loss=1.08955\n",
      "[[0.44452298 0.29281285 0.26266417]\n",
      " [0.21347913 0.49572307 0.2907978 ]\n",
      " [0.332091   0.2278125  0.44009644]]\n",
      "Run 04/10: test acc = 87.73%\n",
      "mps: 4 steps -> 18.18 sec | avg 4546.0 ms/step\n",
      "estimating matrix\n",
      "[Warm-up] 15 epochs...\n",
      "[Refinement] 60 epochs...\n",
      "Epoch 10: loss=1.09359\n",
      "Epoch 20: loss=1.09174\n",
      "Epoch 30: loss=1.09011\n",
      "Epoch 40: loss=1.08872\n",
      "Epoch 50: loss=1.08757\n",
      "Epoch 60: loss=1.08663\n",
      "[[0.4949418  0.26486018 0.24019805]\n",
      " [0.26282284 0.48539644 0.25178072]\n",
      " [0.2343789  0.25908092 0.5065402 ]]\n",
      "Run 05/10: test acc = 89.83%\n",
      "mps: 5 steps -> 18.29 sec | avg 3658.7 ms/step\n",
      "estimating matrix\n",
      "[Warm-up] 15 epochs...\n",
      "[Refinement] 60 epochs...\n",
      "Epoch 10: loss=1.09265\n",
      "Epoch 20: loss=1.09090\n",
      "Epoch 30: loss=1.08939\n",
      "Epoch 40: loss=1.08810\n",
      "Epoch 50: loss=1.08703\n",
      "Epoch 60: loss=1.08615\n",
      "[[0.5169291  0.25787354 0.2251974 ]\n",
      " [0.25279436 0.4962804  0.25092518]\n",
      " [0.23095334 0.26230997 0.5067367 ]]\n",
      "Run 06/10: test acc = 88.03%\n",
      "mps: 6 steps -> 18.47 sec | avg 3078.6 ms/step\n",
      "estimating matrix\n",
      "[Warm-up] 15 epochs...\n",
      "[Refinement] 60 epochs...\n",
      "Epoch 10: loss=1.09083\n",
      "Epoch 20: loss=1.08948\n",
      "Epoch 30: loss=1.08839\n",
      "Epoch 40: loss=1.08753\n",
      "Epoch 50: loss=1.08689\n",
      "Epoch 60: loss=1.08642\n",
      "[[0.53278315 0.23388109 0.23333581]\n",
      " [0.2443217  0.5364919  0.21918646]\n",
      " [0.2334703  0.236662   0.5298677 ]]\n",
      "Run 07/10: test acc = 92.80%\n",
      "mps: 7 steps -> 18.29 sec | avg 2613.1 ms/step\n",
      "estimating matrix\n",
      "[Warm-up] 15 epochs...\n",
      "[Refinement] 60 epochs...\n",
      "Epoch 10: loss=1.09459\n",
      "Epoch 20: loss=1.09289\n",
      "Epoch 30: loss=1.09138\n",
      "Epoch 40: loss=1.09009\n",
      "Epoch 50: loss=1.08901\n",
      "Epoch 60: loss=1.08812\n",
      "[[0.49984455 0.2625482  0.23760723]\n",
      " [0.25189862 0.46687743 0.28122398]\n",
      " [0.2383568  0.26182202 0.4998212 ]]\n",
      "Run 08/10: test acc = 63.63%\n",
      "mps: 8 steps -> 18.60 sec | avg 2324.5 ms/step\n",
      "estimating matrix\n",
      "[Warm-up] 15 epochs...\n",
      "[Refinement] 60 epochs...\n",
      "Epoch 10: loss=1.09639\n",
      "Epoch 20: loss=1.09488\n",
      "Epoch 30: loss=1.09353\n",
      "Epoch 40: loss=1.09234\n",
      "Epoch 50: loss=1.09133\n",
      "Epoch 60: loss=1.09048\n",
      "[[0.4770446  0.255328   0.26762736]\n",
      " [0.25650263 0.48310778 0.26038957]\n",
      " [0.26875073 0.26006407 0.47118524]]\n",
      "Run 09/10: test acc = 63.37%\n",
      "mps: 9 steps -> 18.68 sec | avg 2075.3 ms/step\n",
      "estimating matrix\n",
      "[Warm-up] 15 epochs...\n",
      "[Refinement] 60 epochs...\n",
      "Epoch 10: loss=1.09549\n",
      "Epoch 20: loss=1.09354\n",
      "Epoch 30: loss=1.09183\n",
      "Epoch 40: loss=1.09040\n",
      "Epoch 50: loss=1.08922\n",
      "Epoch 60: loss=1.08829\n",
      "[[0.47418115 0.2535353  0.2722836 ]\n",
      " [0.25014868 0.4995491  0.25030226]\n",
      " [0.27218992 0.25969407 0.46811596]]\n",
      "Run 10/10: test acc = 85.97%\n",
      "mps: 10 steps -> 18.45 sec | avg 1844.6 ms/step\n",
      "========================================================================\n",
      "fashion06 | forward | mean±std over 10 runs: 83.18±10.14%\n",
      "Saved summary to results_forward_2025-10-28 22:11:44_2.json\n",
      "estimating matrix\n",
      "[Warm-up] 15 epochs...\n",
      "[Refinement] 60 epochs...\n",
      "Epoch 10: loss=1.09374\n",
      "Epoch 20: loss=1.09214\n",
      "Epoch 30: loss=1.09080\n",
      "Epoch 40: loss=1.08967\n",
      "Epoch 50: loss=1.08871\n",
      "Epoch 60: loss=1.08791\n",
      "[[0.5545196  0.28542125 0.16005912]\n",
      " [0.24006483 0.46435776 0.2955775 ]\n",
      " [0.21198219 0.24000481 0.54801303]]\n",
      "Run 01/10: test acc = 93.00%\n",
      "mps: 1 steps -> 18.45 sec | avg 18446.2 ms/step\n",
      "estimating matrix\n",
      "[Warm-up] 15 epochs...\n",
      "[Refinement] 60 epochs...\n",
      "Epoch 10: loss=1.09411\n",
      "Epoch 20: loss=1.09118\n",
      "Epoch 30: loss=1.08851\n",
      "Epoch 40: loss=1.08610\n",
      "Epoch 50: loss=1.08396\n",
      "Epoch 60: loss=1.08209\n",
      "[[0.4804538  0.25562984 0.26391634]\n",
      " [0.2525786  0.50190824 0.24551316]\n",
      " [0.2754342  0.25633535 0.4682304 ]]\n",
      "Run 02/10: test acc = 94.53%\n",
      "mps: 2 steps -> 18.30 sec | avg 9148.9 ms/step\n",
      "estimating matrix\n",
      "[Warm-up] 15 epochs...\n",
      "[Refinement] 60 epochs...\n",
      "Epoch 10: loss=1.09428\n",
      "Epoch 20: loss=1.09195\n",
      "Epoch 30: loss=1.08984\n",
      "Epoch 40: loss=1.08797\n",
      "Epoch 50: loss=1.08635\n",
      "Epoch 60: loss=1.08498\n",
      "[[0.48145947 0.24750157 0.27103895]\n",
      " [0.24597254 0.5087396  0.2452879 ]\n",
      " [0.27445444 0.25092572 0.47461987]]\n",
      "Run 03/10: test acc = 93.97%\n",
      "mps: 3 steps -> 18.57 sec | avg 6189.1 ms/step\n",
      "estimating matrix\n",
      "[Warm-up] 15 epochs...\n",
      "[Refinement] 60 epochs...\n",
      "Epoch 10: loss=1.09794\n",
      "Epoch 20: loss=1.09584\n",
      "Epoch 30: loss=1.09394\n",
      "Epoch 40: loss=1.09225\n",
      "Epoch 50: loss=1.09079\n",
      "Epoch 60: loss=1.08955\n",
      "[[0.44452298 0.29281285 0.26266417]\n",
      " [0.21347913 0.49572307 0.2907978 ]\n",
      " [0.332091   0.2278125  0.44009644]]\n",
      "Run 04/10: test acc = 90.50%\n",
      "mps: 4 steps -> 18.44 sec | avg 4609.4 ms/step\n",
      "estimating matrix\n",
      "[Warm-up] 15 epochs...\n",
      "[Refinement] 60 epochs...\n",
      "Epoch 10: loss=1.09359\n",
      "Epoch 20: loss=1.09174\n",
      "Epoch 30: loss=1.09011\n",
      "Epoch 40: loss=1.08872\n",
      "Epoch 50: loss=1.08757\n",
      "Epoch 60: loss=1.08663\n",
      "[[0.4949418  0.26486018 0.24019805]\n",
      " [0.26282284 0.48539644 0.25178072]\n",
      " [0.2343789  0.25908092 0.5065402 ]]\n",
      "Run 05/10: test acc = 92.13%\n",
      "mps: 5 steps -> 18.15 sec | avg 3630.5 ms/step\n",
      "estimating matrix\n",
      "[Warm-up] 15 epochs...\n",
      "[Refinement] 60 epochs...\n",
      "Epoch 10: loss=1.09265\n",
      "Epoch 20: loss=1.09090\n",
      "Epoch 30: loss=1.08939\n",
      "Epoch 40: loss=1.08810\n",
      "Epoch 50: loss=1.08703\n",
      "Epoch 60: loss=1.08615\n",
      "[[0.5169291  0.25787354 0.2251974 ]\n",
      " [0.25279436 0.4962804  0.25092518]\n",
      " [0.23095334 0.26230997 0.5067367 ]]\n",
      "Run 06/10: test acc = 88.17%\n",
      "mps: 6 steps -> 18.03 sec | avg 3005.1 ms/step\n",
      "estimating matrix\n",
      "[Warm-up] 15 epochs...\n",
      "[Refinement] 60 epochs...\n",
      "Epoch 10: loss=1.09083\n",
      "Epoch 20: loss=1.08948\n",
      "Epoch 30: loss=1.08839\n",
      "Epoch 40: loss=1.08753\n",
      "Epoch 50: loss=1.08689\n",
      "Epoch 60: loss=1.08642\n",
      "[[0.53278315 0.23388109 0.23333581]\n",
      " [0.2443217  0.5364919  0.21918646]\n",
      " [0.2334703  0.236662   0.5298677 ]]\n",
      "Run 07/10: test acc = 93.83%\n",
      "mps: 7 steps -> 17.97 sec | avg 2567.7 ms/step\n",
      "estimating matrix\n",
      "[Warm-up] 15 epochs...\n",
      "[Refinement] 60 epochs...\n",
      "Epoch 10: loss=1.09459\n",
      "Epoch 20: loss=1.09289\n",
      "Epoch 30: loss=1.09138\n",
      "Epoch 40: loss=1.09009\n",
      "Epoch 50: loss=1.08901\n",
      "Epoch 60: loss=1.08812\n",
      "[[0.49984455 0.2625482  0.23760723]\n",
      " [0.25189862 0.46687743 0.28122398]\n",
      " [0.2383568  0.26182202 0.4998212 ]]\n",
      "Run 08/10: test acc = 93.53%\n",
      "mps: 8 steps -> 18.55 sec | avg 2318.9 ms/step\n",
      "estimating matrix\n",
      "[Warm-up] 15 epochs...\n",
      "[Refinement] 60 epochs...\n",
      "Epoch 10: loss=1.09639\n",
      "Epoch 20: loss=1.09488\n",
      "Epoch 30: loss=1.09353\n",
      "Epoch 40: loss=1.09234\n",
      "Epoch 50: loss=1.09133\n",
      "Epoch 60: loss=1.09048\n",
      "[[0.4770446  0.255328   0.26762736]\n",
      " [0.25650263 0.48310778 0.26038957]\n",
      " [0.26875073 0.26006407 0.47118524]]\n",
      "Run 09/10: test acc = 92.87%\n",
      "mps: 9 steps -> 18.89 sec | avg 2099.3 ms/step\n",
      "estimating matrix\n",
      "[Warm-up] 15 epochs...\n",
      "[Refinement] 60 epochs...\n",
      "Epoch 10: loss=1.09549\n",
      "Epoch 20: loss=1.09354\n",
      "Epoch 30: loss=1.09183\n",
      "Epoch 40: loss=1.09040\n",
      "Epoch 50: loss=1.08922\n",
      "Epoch 60: loss=1.08829\n",
      "[[0.47418115 0.2535353  0.2722836 ]\n",
      " [0.25014868 0.4995491  0.25030226]\n",
      " [0.27218992 0.25969407 0.46811596]]\n",
      "Run 10/10: test acc = 92.13%\n",
      "mps: 10 steps -> 18.72 sec | avg 1872.4 ms/step\n",
      "========================================================================\n",
      "fashion06 | gce | mean±std over 10 runs: 92.47±1.80%\n",
      "Saved summary to results_gce_2025-10-28 22:11:44_2.json\n"
     ]
    }
   ],
   "source": [
    "for cfg in configs:\n",
    "    all_acc = []\n",
    "    last_T = None\n",
    "    t_arr = []\n",
    "    for r in range(cfg['runs']):\n",
    "\n",
    "        start = time.perf_counter()\n",
    "        acc, T = run_once(cfg, seed=1000+r)\n",
    "        all_acc.append(acc)\n",
    "        if cfg['estimate_T']:\n",
    "            t_arr.append(T)\n",
    "        last_T = T if T is not None else last_T\n",
    "        print(f\"Run {r+1:02d}/{cfg['runs']}: test acc = {acc*100:.2f}%\")\n",
    "        end = time.perf_counter()\n",
    "        print(f\"{cfg['device']}: {r+1} steps -> {end - start:.2f} sec | avg {1000*(end - start)/(r+1):.1f} ms/step\")\n",
    "    mean = float(np.mean(all_acc))\n",
    "    std  = float(np.std(all_acc))\n",
    "\n",
    "    summary = {\n",
    "        'dataset': cfg['dataset'],\n",
    "        'loss': cfg['loss'],\n",
    "        'estimate_T': bool(cfg['estimate_T']),\n",
    "        'epochs': cfg['epochs'],\n",
    "        'runs': cfg['runs'],\n",
    "        'mean_test_acc': mean,\n",
    "        'std_test_acc': std,\n",
    "        'last_estimated_T': last_T,\n",
    "        't_arr':t_arr,\n",
    "        'per_run_acc': all_acc,\n",
    "    }\n",
    "    print(\"=\"*72)\n",
    "    print(f\"{cfg['dataset']} | {cfg['loss']} | mean±std over {cfg['runs']} runs: {mean*100:.2f}±{std*100:.2f}%\")\n",
    "\n",
    "    with open(cfg['out'], 'w') as f:\n",
    "        json.dump(summary, f, indent=2)\n",
    "    print(f\"Saved summary to {cfg['out']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss tensor: tensor(0.9543, grad_fn=<MeanBackward0>)\n",
      "Loss value: 0.9543001651763916\n"
     ]
    }
   ],
   "source": [
    "criterion = GeneralizedCrossEntropy(q=0.7)\n",
    "\n",
    "# Dummy data\n",
    "logits = torch.randn(4, 3, requires_grad=True)  # model outputs\n",
    "y = torch.tensor([0, 2, 1, 0])                  # true labels\n",
    "\n",
    "# Compute loss\n",
    "loss = criterion(logits, y)\n",
    "print(\"Loss tensor:\", loss)\n",
    "print(\"Loss value:\", loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'results_forward_2025-10-28 22:11:44_1'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[44]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mresults_forward_2025-10-28 22:11:44_1\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mr\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[32m      2\u001b[39m     data = json.load(f)\n\u001b[32m      4\u001b[39m \u001b[38;5;28mprint\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Library/Python/3.11/lib/python/site-packages/IPython/core/interactiveshell.py:343\u001b[39m, in \u001b[36m_modified_open\u001b[39m\u001b[34m(file, *args, **kwargs)\u001b[39m\n\u001b[32m    336\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[32m0\u001b[39m, \u001b[32m1\u001b[39m, \u001b[32m2\u001b[39m}:\n\u001b[32m    337\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    338\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mIPython won\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m by default \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    339\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    340\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33myou can use builtins\u001b[39m\u001b[33m'\u001b[39m\u001b[33m open.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    341\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m343\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: 'results_forward_2025-10-28 22:11:44_1'"
     ]
    }
   ],
   "source": [
    "with open(\"results_forward_2025-10-28 22:11:44_1\", \"r\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "print()\n",
    "T_prime = np.array(data['last_estimated_T'])\n",
    "T_true = np.array([[0.7,0.3,0.0],\n",
    "                     [0.0,0.7,0.3],\n",
    "                     [0.3,0.0,0.7]], dtype=np.float32)\n",
    "\n",
    "#checking recreation performance\n",
    "print(T_prime)\n",
    "print(T_true)\n",
    "print(f\"Fro error: {np.linalg.norm(T_prime - T_true, 'fro')}\")\n",
    "print(f\"rre error: {np.linalg.norm(T_prime - T_true, 'fro') / np.linalg.norm(T_true, 'fro')}\")\n",
    "print(f\"mae error: {np.mean(np.abs(T_prime - T_true))}\")\n",
    "import scipy.stats as st\n",
    "\n",
    "corrs = [st.pearsonr(T_true[i], T_prime[i])[0] for i in range(C)]\n",
    "print(\"Per-row correlations:\", corrs)\n",
    "print(\"Mean:\", np.mean(corrs))\n",
    "\n",
    "\n",
    "with open(\"results_forward_2025-10-28 22:11:44_2.json\", \"r\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "print()\n",
    "T_prime = np.array(data['last_estimated_T'])\n",
    "T_true = np.array([[0.3,0.4,0.3],\n",
    "                     [0.4,0.3,0.3],\n",
    "                     [0.3,0.3,0.4]], dtype=np.float32)\n",
    "#checking recreation performance\n",
    "print(T_prime)\n",
    "print(T_true)\n",
    "print(f\"Fro error: {np.linalg.norm(T_prime - T_true, 'fro')}\")\n",
    "print(f\"rre error: {np.linalg.norm(T_prime - T_true, 'fro') / np.linalg.norm(T_true, 'fro')}\")\n",
    "print(f\"mae error: {np.mean(np.abs(T_prime - T_true))}\")\n",
    "import scipy.stats as st\n",
    "\n",
    "corrs = [st.pearsonr(T_true[i], T_prime[i])[0] for i in range(C)]\n",
    "print(\"Per-row correlations:\", corrs)\n",
    "print(\"Mean:\", np.mean(corrs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.74965894 0.         0.19647887]\n",
      " [0.24897681 0.80209059 0.        ]\n",
      " [0.00136426 0.19790941 0.80352116]]\n",
      "[[0.7 0.3 0. ]\n",
      " [0.  0.7 0.3]\n",
      " [0.  0.  0.7]]\n",
      "0.4561821482443322\n"
     ]
    }
   ],
   "source": [
    "#checking recreation performance\n",
    "print(est_t)\n",
    "print(known_t)\n",
    "print(rre_fse(known_t, est_t))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
