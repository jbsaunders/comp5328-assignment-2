{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9IrXtNd8l-9H"
   },
   "source": [
    "## COMP5328 - Advanced Machine Learning\n",
    "## Assignment 2: Title\n",
    "----------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Common imports\n",
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import json\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "# Ploting\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import os\n",
    "import torch.optim as optim\n",
    "import random\n",
    "import scipy.stats as st"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment variables\n",
    "# Common\n",
    "num_classes=3\n",
    "dataset_folder = 'data/'\n",
    "cifar_dataset = dataset_folder+'CIFAR.npz'\n",
    "MNISTO3_dataset = dataset_folder+'FashionMNIST0.3.npz'\n",
    "MNISTO6_dataset = dataset_folder+'FashionMNIST0.6.npz'\n",
    "\n",
    "\n",
    "DATA_PATHS = {\n",
    "    'fashion03': MNISTO3_dataset,\n",
    "    'fashion06': MNISTO6_dataset,\n",
    "    'cifar':     cifar_dataset\n",
    "}\n",
    "\n",
    "losses = ['forward','gce', 'forwardGCE']\n",
    "datasets = ['cifar', \"fashion03\", \"fashion06\"]\n",
    "base = {\n",
    "    \"runs\":10,\n",
    "    \"epochs\": 15,\n",
    "    \"loss\":'forward',\n",
    "    \"batch_size\":4096,\n",
    "    \"q\":0.6,\n",
    "    \"est_epochs\":10,\n",
    "    \"beta\":0.2,\n",
    "    \"lr\":1e-3,\n",
    "    \"device\":'mps'\n",
    "}\n",
    "\n",
    "\n",
    "known_T_fashion_03 = np.array(  [[0.7,0.3,0.0],\n",
    "                                [0.0,0.7,0.3],\n",
    "                                [0.3,0.0,0.7]], dtype=np.float32)\n",
    "\n",
    "known_T_fashion_06 = np.array(  [[0.4,0.3,0.3],\n",
    "                                [0.3,0.4,0.3],\n",
    "                                [0.3,0.3,0.4]], dtype=np.float32)\n",
    "\n",
    "def pick_known_T(tag):\n",
    "    if tag == 'fashion03':\n",
    "        return known_T_fashion_03\n",
    "    elif tag == 'fashion06':\n",
    "        return known_T_fashion_06\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def set_seed(seed=0):\n",
    "    import random\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "def sanity(T, name, dataset):\n",
    "    print(f\"\\n{name}\")\n",
    "    print(\"row sums:\", T.sum(axis=1))\n",
    "    print(\"col sums:\", T.sum(axis=0))\n",
    "    if dataset == 'fashion03':\n",
    "        T_true = known_T_fashion_03\n",
    "    elif dataset == 'fashion06':\n",
    "        T_true = known_T_fashion_06\n",
    "    if dataset != 'cifar':\n",
    "        # if you know T_true for this dataset:\n",
    "        print(\"Fro:\", np.linalg.norm(T - T_true, 'fro'))\n",
    "        print(\"MAE:\", np.mean(np.abs(T - T_true)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qYzWuiytl-9I"
   },
   "source": [
    "## 1. Load Dataset\n",
    "\n",
    "### 1.0 Data Folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unzip:  cannot find or open datasets.zip, datasets.zip.zip or datasets.zip.ZIP.\n"
     ]
    }
   ],
   "source": [
    "# Path to your dataset zip stored in Drive\n",
    "zip_path = \"datasets.zip\"\n",
    "\n",
    "# Unzip file\n",
    "!unzip -o -q \"$zip_path\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OxEr9ihznSqa",
    "outputId": "8b9fd2c9-b2f9-45f4-94aa-d0d473b7e140"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 172688\n",
      "-rw-r--r--@ 1 jamie.saunders  staff  55440974 Oct  4  2019 CIFAR.npz\n",
      "-rw-r--r--@ 1 jamie.saunders  staff  16485974 Oct 10  2021 FashionMNIST0.3.npz\n",
      "-rw-r--r--@ 1 jamie.saunders  staff  16485974 Oct 10  2021 FashionMNIST0.6.npz\n"
     ]
    }
   ],
   "source": [
    "# The structure of data folder.\n",
    "!ls -l data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_npz(path):\n",
    "    d = np.load(path)\n",
    "    Xtr, Str = d['Xtr'], d['Str']\n",
    "    Xts, Yts = d['Xts'], d['Yts']\n",
    "    return Xtr, Str, Xts, Yts\n",
    "\n",
    "# A helper class, it is used as an input of the DataLoader object.\n",
    "class DatasetArray(Dataset):\n",
    "    def __init__(self, data, labels=None, transform=None):\n",
    "        if labels != None:\n",
    "            self.data_arr = np.asarray(data).astype(np.float32)\n",
    "            self.label_arr = np.asarray(labels).astype(np.long)\n",
    "        else:\n",
    "            tmp_arr = np.asarray(data)\n",
    "            self.data_arr = tmp_arr[:,:-1].astype(np.float32)\n",
    "            self.label_arr = tmp_arr[:,-1].astype(np.long)\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data_arr)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "     \n",
    "        data = self.data_arr[index]\n",
    "        label = self.label_arr[index]\n",
    "        \n",
    "        if self.transform is not None:\n",
    "            data = self.transform(data)\n",
    "            \n",
    "        return (data, label)\n",
    "    \n",
    "    \n",
    "# Splitting the data into three parts.\n",
    "def train_val_test_random_split(data, fracs=[0.7,0.1,0.2]):\n",
    "    r\"\"\"Split the data into training, validation and test set.\n",
    "    Args:\n",
    "        fracs: a list of length three\n",
    "    \"\"\"\n",
    "    assert len(fracs) == 3\n",
    "    assert sum(fracs) == 1\n",
    "    assert all(frac > 0 for frac in fracs)\n",
    "    n = len(data)\n",
    "    subset_lens = [int(n*frac) for frac in fracs]\n",
    "    idxs = list(range(n))\n",
    "    random.shuffle(idxs)\n",
    "    data = np.array(data)\n",
    "    new_data = []\n",
    "    start_idx = 0\n",
    "    for subset_len in subset_lens:\n",
    "        end_idx = start_idx + subset_len\n",
    "        cur_idxs = idxs[start_idx:end_idx]\n",
    "        new_data.append(data[cur_idxs,:].tolist())\n",
    "        start_idx = end_idx\n",
    "    return new_data\n",
    "\n",
    "# Preparation of the data for training, validation and testing a pytorch network. \n",
    "# Note that the test data is not in use for this lab.\n",
    "def get_loader(batch_size =128, num_workers = 0, train_val_test_split = [0.7,0.1,0.2], data=None):\n",
    "    r\"\"\"This function is used to read the data file and split the data into three subsets, i.e, \n",
    "    train data, validation data and test data. Their corresponding DataLoader objects are returned.\"\"\"\n",
    "    \n",
    "    [train_data, val_data, test_data] = train_val_test_random_split(data, fracs = train_val_test_split)\n",
    "\n",
    "    train_data = DatasetArray(data = train_data)\n",
    "    val_data = DatasetArray(data = val_data)\n",
    "    test_data = DatasetArray(data = test_data)\n",
    "\n",
    "    #The pytorch built-in class DataLoader can help us to shuffle the data, draw mini-batch,\n",
    "    #do transformations, etc. \n",
    "    train_loader = DataLoader(\n",
    "        train_data,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=num_workers,\n",
    "    )\n",
    "\n",
    "    val_loader = DataLoader(\n",
    "        val_data,\n",
    "        batch_size=100,\n",
    "        shuffle=False,\n",
    "        num_workers=num_workers,\n",
    "    )\n",
    "\n",
    "    test_loader = DataLoader(\n",
    "        test_data,\n",
    "        batch_size=100,\n",
    "        num_workers=num_workers,\n",
    "        shuffle=False,\n",
    "    )\n",
    "    return train_loader, val_loader, test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NpzDataset(Dataset):\n",
    "    def __init__(self, X, y, is_cifar=False):\n",
    "        self.X = X.astype(np.float32)\n",
    "        self.y = y.astype(np.int64)\n",
    "        self.is_cifar = is_cifar\n",
    "\n",
    "        # Normalize to [0,1]\n",
    "        self.X = self.X / 255.0 if self.X.max() > 1.0 else self.X\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x = self.X[idx]\n",
    "        if x.ndim == 1:\n",
    "            # flat; try to infer shape 28x28 or 32x32x3\n",
    "            if x.size == 28*28:\n",
    "                x = x.reshape(1, 28, 28)\n",
    "            elif x.size == 32*32*3:\n",
    "                x = x.reshape(3, 32, 32)\n",
    "            else:\n",
    "                raise ValueError(\"Unknown flat image shape: {}\".format(x.shape))\n",
    "        else:\n",
    "            # (H,W) or (H,W,C)\n",
    "            if x.ndim == 2:\n",
    "                x = x[None, ...]  # to (1,H,W)\n",
    "            elif x.ndim == 3:\n",
    "                # assume HWC -> CHW\n",
    "                x = np.transpose(x, (2, 0, 1))\n",
    "            else:\n",
    "                raise ValueError(f\"Unexpected image dims: {x.shape}\")\n",
    "        return torch.from_numpy(x), torch.tensor(self.y[idx])\n",
    "\n",
    "\n",
    "def load_npz(path):\n",
    "    d = np.load(path)\n",
    "    Xtr, Str = d['Xtr'], d['Str']\n",
    "    Xts, Yts = d['Xts'], d['Yts']\n",
    "    return Xtr, Str, Xts, Yts\n",
    "\n",
    "\n",
    "def make_loaders(Xtr, Str, batch_size=128, seed=0, test_size=0.2):\n",
    "    # 80/20 split each repetition\n",
    "    X_tr, X_val, y_tr, y_val = train_test_split(\n",
    "        Xtr, Str, test_size=test_size, random_state=seed, stratify=Str\n",
    "    )\n",
    "\n",
    "    is_cifar = (X_tr.shape[-1] == 3) if X_tr.ndim == 4 else (X_tr.shape[-1] == 32*32*3)\n",
    "\n",
    "    train_ds = NpzDataset(X_tr, y_tr, is_cifar)\n",
    "    val_ds   = NpzDataset(X_val, y_val, is_cifar)\n",
    "\n",
    "    train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "    val_loader   = DataLoader(val_ds,   batch_size=batch_size, shuffle=False, num_workers=0)\n",
    "\n",
    "    return train_loader, val_loader, is_cifar\n",
    "\n",
    "def make_test_loader(Xts, Yts, batch_size=256):\n",
    "    is_cifar = (Xts.shape[-1] == 3) if Xts.ndim == 4 else (Xts.shape[-1] == 32*32*3)\n",
    "    test_ds = NpzDataset(Xts, Yts, is_cifar)\n",
    "    return DataLoader(test_ds, batch_size=batch_size, shuffle=False, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ForwardCorrectedCE(nn.Module):\n",
    "    \"\"\"\n",
    "    Forward loss correction: minimizes CE between T^T p and noisy labels.\n",
    "    T: class-transition matrix where T[i,j] = P(S=j | Y=i). Shape [C,C].\n",
    "    \"\"\"\n",
    "    def __init__(self, T):\n",
    "        super().__init__()\n",
    "        self.register_buffer('T', T)  # [C,C]\n",
    "\n",
    "    def forward(self, logits, y_noisy):\n",
    "        # logits -> p(y|x)\n",
    "        p = F.softmax(logits, dim=1)  # [B,C]\n",
    "        # mix via T^T\n",
    "        mixed = torch.clamp(p @ self.T.t(), 1e-6, 1.0)\n",
    "        log_mixed = torch.log(mixed)\n",
    "        return F.nll_loss(log_mixed, y_noisy)\n",
    "\n",
    "class ForwardCorrectedGCE(nn.Module):\n",
    "    def __init__(self, T, q=0.7):\n",
    "        super().__init__()\n",
    "        self.register_buffer('T', T)\n",
    "        self.q = q\n",
    "    def forward(self, logits, y_noisy):\n",
    "        p_noisy = torch.clamp(F.softmax(logits,1) @ self.T, 1e-6, 1.0)\n",
    "        p_s = p_noisy.gather(1, y_noisy.view(-1,1)).clamp(1e-6,1.0)\n",
    "        return (-(p_s.log()) if self.q==1.0 else (1 - p_s**self.q)/self.q).mean()\n",
    "\n",
    "\n",
    "class GeneralizedCrossEntropy(nn.Module):\n",
    "    \"\"\"\n",
    "    GCE loss: L_q(p, y) = (1 - p_y^q) / q, with q in (0,1].\n",
    "    q→1 recovers CE; smaller q is more robust to label noise.\n",
    "    \"\"\"\n",
    "    def __init__(self, q=0.7):\n",
    "        super().__init__()\n",
    "        assert 0 < q <= 1\n",
    "        self.q = q\n",
    "\n",
    "    def forward(self, logits, y):\n",
    "        p = F.softmax(logits, dim=1)\n",
    "        p_y = p.gather(1, y.view(-1,1)).clamp(min=1e-6, max=1.0)\n",
    "        if self.q == 1.0:\n",
    "            return -torch.log(p_y).mean()\n",
    "        return ((1 - p_y.pow(self.q)) / self.q).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_block(cin, cout):\n",
    "    return nn.Sequential(\n",
    "        nn.Conv2d(cin, cout, 3, padding=1),\n",
    "        nn.BatchNorm2d(cout),\n",
    "        nn.ReLU(inplace=True),\n",
    "        nn.MaxPool2d(2)\n",
    "    )\n",
    "\n",
    "class SmallCNN28(nn.Module):\n",
    "    \"\"\"For 1×28×28 images.\"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            conv_block(1, 32),  # 14x14\n",
    "            conv_block(32, 64), # 7x7\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(64*7*7, 128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(128, 3)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "\n",
    "class SmallCNNCifar(nn.Module):\n",
    "    \"\"\"For 3×32×32 images.\"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            conv_block(3, 32),   # 16x16\n",
    "            conv_block(32, 64),  # 8x8\n",
    "            conv_block(64, 128), # 4x4\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(128*4*4, 256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(256, 3)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "\n",
    "def make_model(is_cifar):\n",
    "    return SmallCNNCifar() if is_cifar else SmallCNN28()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def predict_proba(model, loader, device):\n",
    "    model.eval()\n",
    "    p_arr = []\n",
    "    ys = []\n",
    "    for xb, yb in loader:\n",
    "        xb = xb.to(device)\n",
    "        output = model(xb)\n",
    "        p = F.softmax(output, dim=1).cpu().numpy()\n",
    "        p_arr.append(p)\n",
    "        ys.append(yb.numpy())\n",
    "    return np.concatenate(p_arr), np.concatenate(ys)\n",
    "\n",
    "\n",
    "def estimate_transition_anchor(t, train_loader, is_cifar, q, device='cpu', epochs=5):\n",
    "    \"\"\"\n",
    "    Simple anchor/confident-example estimator (Patrini et al., 2017 style):\n",
    "    1) Train a base classifier on noisy data.\n",
    "    2) Get p(y|x) on training set.\n",
    "    3) For each clean class i, find indices whose predicted argmax == noisy label == i and with high confidence.\n",
    "    4) For those indices, estimate column i of T as average of empirical noisy label distribution given model predicts i.\n",
    "    Here: since we only have noisy labels S, we approximate T[:, i] ≈ E[ onehot(S) | argmax p = i, p_i >= τ ].\n",
    "    Normalize columns to sum to 1.\n",
    "    \"\"\"\n",
    "\n",
    "    device = torch.device(device)\n",
    "    model = make_model(is_cifar).to(device)\n",
    "\n",
    "    optimizer = optim.Adam(model.parameters(), lr=1e-3, weight_decay=1e-4)\n",
    "    criterion = GeneralizedCrossEntropy(q=q)\n",
    "    # quick warmup training on noisy labels\n",
    "    for _ in range(epochs):\n",
    "        train_one_epoch(model, train_loader, optimizer, criterion, device=device) \n",
    "\n",
    "    # collect probs & noisy labels\n",
    "    p_arr, y_noisy = predict_proba(model, train_loader, device)\n",
    "    preds = p_arr.argmax(axis=1)\n",
    "    maxp = p_arr.max(axis=1)\n",
    "\n",
    "    C = num_classes\n",
    "    T = t\n",
    "    # choose class-wise thresholds based on quantiles for stability\n",
    "    for i in range(C):\n",
    "        idx = np.where(preds == i)[0]\n",
    "        if idx.size == 0:\n",
    "            T[:, i] = np.ones(C) / C\n",
    "            continue\n",
    "        # high-confidence subset (top 30% by p_i)\n",
    "        conf = maxp[idx]\n",
    "        if conf.size > 50:\n",
    "            tau = np.quantile(conf, 0.7)\n",
    "        else:\n",
    "            tau = np.min(conf)  # keep all if tiny\n",
    "        keep = idx[conf >= tau]\n",
    "        if keep.size == 0:\n",
    "            keep = idx\n",
    "        # empirical distribution of noisy labels in this confident set\n",
    "        hist = np.bincount(y_noisy[keep], minlength=C).astype(np.float64)\n",
    "        if hist.sum() == 0:\n",
    "            T[:, i] = np.ones(C) / C\n",
    "        else:\n",
    "            T[:, i] = hist / hist.sum()\n",
    "\n",
    "    # column-normalize\n",
    "    colsum = T.sum(axis=0, keepdims=True)\n",
    "    T = np.divide(T, np.maximum(colsum, 1e-8))\n",
    "    return T.astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_transition_trevision(T_init, train_loader, is_cifar, q, device=\"cpu\", epochs=5, lambda_reg=1e-4, lr_t=5e-3, lr_model=1e-3, warmup_epochs=3, log_every=1\n",
    "):\n",
    "    \"\"\"\n",
    "    Refine transition matrix using T-Revision method (Patrini et al. style).\n",
    "\n",
    "    Args:\n",
    "        T_init (np.ndarray or torch.Tensor): initial transition matrix [C, C]\n",
    "        train_loader: noisy dataloader (x, y_noisy)\n",
    "        is_cifar (bool): dataset selector for model architecture\n",
    "        device (str): 'cpu', 'cuda', or 'mps'\n",
    "        epochs (int): number of refinement epochs for ΔT\n",
    "        lambda_reg (float): regularization to keep T close to T_init\n",
    "        lr_t (float): learning rate for ΔT\n",
    "        lr_model (float): learning rate for model warm-up\n",
    "        warmup_epochs (int): number of model warm-up epochs\n",
    "        log_every (int): print interval\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: refined transition matrix [C, C]\n",
    "    \"\"\"\n",
    "\n",
    "    device = torch.device(device)\n",
    "    C = 3\n",
    "\n",
    "    # ---------------------------\n",
    "    # 1. Base model setup\n",
    "    # ---------------------------\n",
    "    model = make_model(is_cifar).to(device)\n",
    "    opt_model = optim.Adam(model.parameters(), lr=lr_model, weight_decay=1e-4)\n",
    "    criterion = GeneralizedCrossEntropy(q=q)\n",
    "\n",
    "    # Warm-up (train classifier on noisy labels)\n",
    "    #print(f\"[Warm-up] training base classifier for {warmup_epochs} epochs...\")\n",
    "    for e in range(warmup_epochs):\n",
    "        train_one_epoch(model, train_loader, opt_model, criterion=criterion, device=device)\n",
    "        #print(f\"  done epoch {e+1}/{warmup_epochs}\")\n",
    "\n",
    "    # ---------------------------\n",
    "    # 2. Get predicted probabilities\n",
    "    # ---------------------------\n",
    "    probs, y_noisy = predict_proba(model, train_loader, device)\n",
    "    p = torch.tensor(probs, dtype=torch.float32, device=device)\n",
    "    y_t = torch.tensor(y_noisy, dtype=torch.long, device=device)\n",
    "\n",
    "    # ---------------------------\n",
    "    # 3. Initialize learnable ΔT (T-Revision)\n",
    "    # ---------------------------\n",
    "    T_init_torch = torch.tensor(T_init, dtype=torch.float32, device=device)\n",
    "    delta_T = nn.Parameter(torch.zeros_like(T_init_torch))\n",
    "    optimizer_T = optim.Adam([delta_T], lr=lr_t)\n",
    "\n",
    "    #print(f\"[Optimization] refining transition matrix for {epochs} epochs...\")\n",
    "\n",
    "    # ---------------------------\n",
    "    # 4. Optimize ΔT\n",
    "    # ---------------------------\n",
    "    for ep in range(epochs):\n",
    "        optimizer_T.zero_grad()\n",
    "\n",
    "        # Proposed transition\n",
    "        T_prime = T_init_torch + delta_T\n",
    "        T_prime = torch.clamp(T_prime, min=1e-6)\n",
    "\n",
    "        # Forward correction: p(y_noisy | x) = p(y|x) * T'\n",
    "        noisy_pred = torch.clamp(p @ T_prime.t(), 1e-6, 1.0)\n",
    "        log_noisy = torch.log(noisy_pred)\n",
    "\n",
    "        # Loss = NLL + regularization\n",
    "        loss_ce = nn.NLLLoss()(log_noisy, y_t)\n",
    "        reg = lambda_reg * torch.norm(T_prime - T_init_torch, p=\"fro\")\n",
    "        loss = loss_ce + reg\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer_T.step()\n",
    "\n",
    "        if (ep + 1) % log_every == 0:\n",
    "            grad_norm = delta_T.grad.abs().mean().item() if delta_T.grad is not None else 0\n",
    "            #print(f\"Epoch {ep+1}/{epochs} | loss={loss.item():.5f} | grad={grad_norm:.5e}\")\n",
    "\n",
    "    # ---------------------------\n",
    "    # 5. Normalize once at the end\n",
    "    # ---------------------------\n",
    "    T_final = T_init_torch + delta_T.data\n",
    "    T_final = torch.clamp(T_final, min=1e-6)\n",
    "    T_final = T_final / T_final.sum(dim=0, keepdim=True)\n",
    "\n",
    "    #print(\"\\n[Done] Refined Transition Matrix:\")\n",
    "    #print(T_final.detach().cpu().numpy())\n",
    "\n",
    "    return T_final.detach().cpu().numpy().astype(np.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def accuracy(model, loader, device):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for xb, yb in loader:\n",
    "        xb, yb = xb.to(device), yb.to(device)\n",
    "        logits = model(xb)\n",
    "        pred = logits.argmax(dim=1)\n",
    "        correct += (pred == yb).sum().item()\n",
    "        total += yb.numel()\n",
    "    return correct / max(total, 1)\n",
    "\n",
    "\n",
    "def train_one_epoch(model, loader, optimizer, criterion, device='mps'):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    for xb, yb in loader:\n",
    "        xb, yb = xb.to(device), yb.to(device)\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        logits = model(xb)\n",
    "        loss = criterion(logits, yb)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item() * yb.size(0)\n",
    "    return total_loss / len(loader.dataset)\n",
    "\n",
    "\n",
    "def fit_model(model, train_loader, val_loader, device, loss_name='gce', T=None, q=0.7, beta=0.2, epochs=10, lr=1e-3):\n",
    "\n",
    "    if loss_name == 'forward':\n",
    "        assert T is not None, \"Forward correction requires known/estimated T\"\n",
    "        print('forward loss')\n",
    "        criterion = ForwardCorrectedCE(torch.tensor(T, dtype=torch.float32, device=device))\n",
    "    elif loss_name == 'forwardGCE':\n",
    "        print('forwardGCE loss')\n",
    "        assert T is not None, \"Forward correction requires known/estimated T\"\n",
    "        criterion = ForwardCorrectedGCE(torch.tensor(T, dtype=torch.float32, device=device), q=q)\n",
    "    else:\n",
    "        print('GCE loss')\n",
    "        criterion = GeneralizedCrossEntropy(q=q)\n",
    "\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=1e-4)\n",
    "\n",
    "    best_val = -np.inf\n",
    "    best_state = None\n",
    "\n",
    "    for _ in range(epochs):\n",
    "        train_one_epoch(model, train_loader, optimizer, criterion, device)\n",
    "        # early stopping on val accuracy (cheap)\n",
    "        val_acc = accuracy(model, val_loader, device)\n",
    "        if val_acc > best_val:\n",
    "            best_val = val_acc\n",
    "            best_state = {k: v.detach().cpu().clone() for k, v in model.state_dict().items()}\n",
    "\n",
    "    if best_state is not None:\n",
    "        model.load_state_dict(best_state)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_once(args, seed):\n",
    "    set_seed(seed)\n",
    "    q = 0.7\n",
    "    if args['dataset']=='fashion03':\n",
    "        q = 0.3\n",
    "    elif args['dataset']=='fashion06':\n",
    "        q = 0.6\n",
    "        \n",
    "\n",
    "    device = torch.device('mps')\n",
    "    print(f'q:{q}')\n",
    "    # load data\n",
    "    Xtr, Str, Xts, Yts = load_npz(DATA_PATHS[args['dataset']])\n",
    "\n",
    "    # loaders for this split\n",
    "    train_loader, val_loader, is_cifar = make_loaders(Xtr, Str, batch_size=args['batch_size'], seed=seed)\n",
    "    test_loader = make_test_loader(Xts, Yts, batch_size=512)\n",
    "  \n",
    "    # choose model\n",
    "    model = make_model(is_cifar).to(device)\n",
    "\n",
    "    # Transition matrix\n",
    "    T = None\n",
    "    if args['estimate_T'] or args['dataset']=='cifar':\n",
    "        T = np.zeros((3, 3), dtype=np.float64)\n",
    "        T = estimate_transition_anchor(T, train_loader, is_cifar, q, device=device, epochs=args['est_epochs'])\n",
    "        sanity(T, 'Est T After Anchor point', args['dataset'])\n",
    "        T = estimate_transition_trevision(T, train_loader, is_cifar, q, device=device, epochs=args['est_epochs'])\n",
    "        sanity(T, 'Est T After T Revision', args['dataset'])\n",
    "    else:\n",
    "        T = pick_known_T(args['dataset'])\n",
    "        if T is None:\n",
    "            raise ValueError(\"Forward loss selected but no known T for this dataset; use --estimate_T.\")\n",
    "    print(q)\n",
    "    # fit\n",
    "    model = fit_model(\n",
    "        model,\n",
    "        train_loader,\n",
    "        val_loader,\n",
    "        device,\n",
    "        loss_name=args['loss'],\n",
    "        T=T,\n",
    "        q=q,\n",
    "        beta=args['beta'],\n",
    "        epochs=args['epochs'],\n",
    "        lr=args['lr'],\n",
    "    )\n",
    "\n",
    "    # evaluate on clean test set\n",
    "    test_acc = accuracy(model, test_loader, device)\n",
    "    return float(test_acc), (T.tolist() if T is not None else None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset: cifar, estimate_T:False, loss:forward\n",
      "dataset: cifar, estimate_T:False, loss:gce\n",
      "dataset: cifar, estimate_T:False, loss:forwardGCE\n",
      "dataset: fashion03, estimate_T:True, loss:forward\n",
      "dataset: fashion03, estimate_T:False, loss:forward\n",
      "dataset: fashion03, estimate_T:True, loss:gce\n",
      "dataset: fashion03, estimate_T:False, loss:gce\n",
      "dataset: fashion03, estimate_T:True, loss:forwardGCE\n",
      "dataset: fashion03, estimate_T:False, loss:forwardGCE\n",
      "dataset: fashion06, estimate_T:True, loss:forward\n",
      "dataset: fashion06, estimate_T:False, loss:forward\n",
      "dataset: fashion06, estimate_T:True, loss:gce\n",
      "dataset: fashion06, estimate_T:False, loss:gce\n",
      "dataset: fashion06, estimate_T:True, loss:forwardGCE\n",
      "dataset: fashion06, estimate_T:False, loss:forwardGCE\n"
     ]
    }
   ],
   "source": [
    "now = datetime.now()\n",
    "now = now.strftime(\"%Y-%m-%d-%H:%M\")\n",
    "folder = \"results\"+now\n",
    "if os.path.exists(folder) and os.path.isdir(folder):\n",
    "    os.rmdir(folder)\n",
    "    os.mkdir(folder)\n",
    "else:\n",
    "    os.mkdir(folder)\n",
    "# create each cfg\n",
    "estimate = [True, False]\n",
    "configs = []\n",
    "for i, ds in enumerate(datasets):\n",
    "    for loss in losses:\n",
    "        for t in estimate:\n",
    "            cfg = {**base, \"dataset\": ds, \"out\": folder+'/'+ds+'_'+loss+'_'+str(t)+'_'+now+'.json', \"loss\":loss, \"estimate_T\":t}\n",
    "            if t and ds !='cifar':\n",
    "                print(f\"dataset: {ds}, estimate_T:{t}, loss:{loss}\")\n",
    "                configs.append(cfg)\n",
    "            elif not t:\n",
    "                print(f\"dataset: {ds}, estimate_T:{t}, loss:{loss}\")\n",
    "                configs.append(cfg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "q:0.7\n",
      "\n",
      "Est T After Anchor point\n",
      "row sums: [1.0493636 1.015275  0.9353613]\n",
      "col sums: [1. 1. 1.]\n",
      "\n",
      "Est T After T Revision\n",
      "row sums: [1.0430155 1.0132998 0.9436846]\n",
      "col sums: [1.        1.0000001 1.       ]\n",
      "0.7\n",
      "forward loss\n",
      "Run 01/10: test acc = 33.33%\n",
      "mps: 1 steps -> 28.07 sec | avg 28070.7 ms/step\n",
      "q:0.7\n",
      "\n",
      "Est T After Anchor point\n",
      "row sums: [1.0348637 1.0296634 0.9354728]\n",
      "col sums: [1.         1.         0.99999994]\n",
      "\n",
      "Est T After T Revision\n",
      "row sums: [1.0304047 1.0258329 0.9437623]\n",
      "col sums: [0.99999994 1.         1.        ]\n",
      "0.7\n",
      "forward loss\n",
      "Run 02/10: test acc = 48.07%\n",
      "mps: 2 steps -> 22.44 sec | avg 11219.2 ms/step\n",
      "q:0.7\n",
      "\n",
      "Est T After Anchor point\n",
      "row sums: [0.96272004 0.9827586  1.0545214 ]\n",
      "col sums: [0.99999994 1.         1.        ]\n",
      "\n",
      "Est T After T Revision\n",
      "row sums: [0.96755826 0.98499274 1.047449  ]\n",
      "col sums: [1. 1. 1.]\n",
      "0.7\n",
      "forward loss\n",
      "Run 03/10: test acc = 49.13%\n",
      "mps: 3 steps -> 21.48 sec | avg 7158.6 ms/step\n",
      "q:0.7\n",
      "\n",
      "Est T After Anchor point\n",
      "row sums: [0.6970325  1.6332357  0.66973186]\n",
      "col sums: [1. 1. 1.]\n",
      "\n",
      "Est T After T Revision\n",
      "row sums: [0.6930304 1.6376605 0.669309 ]\n",
      "col sums: [1. 1. 1.]\n",
      "0.7\n",
      "forward loss\n",
      "Run 04/10: test acc = 47.67%\n",
      "mps: 4 steps -> 20.89 sec | avg 5223.6 ms/step\n",
      "q:0.7\n",
      "\n",
      "Est T After Anchor point\n",
      "row sums: [0.69094896 1.0551382  1.2539129 ]\n",
      "col sums: [1. 1. 1.]\n",
      "\n",
      "Est T After T Revision\n",
      "row sums: [0.68774015 1.0656562  1.2466038 ]\n",
      "col sums: [1. 1. 1.]\n",
      "0.7\n",
      "forward loss\n",
      "Run 05/10: test acc = 50.43%\n",
      "mps: 5 steps -> 20.96 sec | avg 4191.3 ms/step\n",
      "q:0.7\n",
      "\n",
      "Est T After Anchor point\n",
      "row sums: [0.9117851  1.1218823  0.96633255]\n",
      "col sums: [1.         0.99999994 1.        ]\n",
      "\n",
      "Est T After T Revision\n",
      "row sums: [0.92319953 1.1061213  0.9706793 ]\n",
      "col sums: [1. 1. 1.]\n",
      "0.7\n",
      "forward loss\n",
      "Run 06/10: test acc = 52.63%\n",
      "mps: 6 steps -> 20.86 sec | avg 3476.2 ms/step\n",
      "q:0.7\n",
      "\n",
      "Est T After Anchor point\n",
      "row sums: [0.9489744  0.99448526 1.0565403 ]\n",
      "col sums: [1. 1. 1.]\n",
      "\n",
      "Est T After T Revision\n",
      "row sums: [0.9555858  0.99518824 1.0492262 ]\n",
      "col sums: [1.0000001 1.        1.       ]\n",
      "0.7\n",
      "forward loss\n",
      "Run 07/10: test acc = 33.33%\n",
      "mps: 7 steps -> 22.81 sec | avg 3259.1 ms/step\n",
      "q:0.7\n",
      "\n",
      "Est T After Anchor point\n",
      "row sums: [1.035712  0.9308258 1.0334623]\n",
      "col sums: [1. 1. 1.]\n",
      "\n",
      "Est T After T Revision\n",
      "row sums: [1.0311024  0.93976533 1.0291321 ]\n",
      "col sums: [1.         1.         0.99999994]\n",
      "0.7\n",
      "forward loss\n",
      "Run 08/10: test acc = 33.33%\n",
      "mps: 8 steps -> 20.99 sec | avg 2623.6 ms/step\n",
      "q:0.7\n",
      "\n",
      "Est T After Anchor point\n",
      "row sums: [0.44296816 1.8249693  0.7320625 ]\n",
      "col sums: [1. 1. 1.]\n",
      "\n",
      "Est T After T Revision\n",
      "row sums: [0.47184944 1.8045658  0.7235847 ]\n",
      "col sums: [1. 1. 1.]\n",
      "0.7\n",
      "forward loss\n",
      "Run 09/10: test acc = 33.33%\n",
      "mps: 9 steps -> 20.72 sec | avg 2302.1 ms/step\n",
      "q:0.7\n",
      "\n",
      "Est T After Anchor point\n",
      "row sums: [1.0401804  0.95067805 1.0091416 ]\n",
      "col sums: [1. 1. 1.]\n",
      "\n",
      "Est T After T Revision\n",
      "row sums: [1.0349976 0.957055  1.0079474]\n",
      "col sums: [1.         1.0000001  0.99999994]\n",
      "0.7\n",
      "forward loss\n",
      "Run 10/10: test acc = 54.10%\n",
      "mps: 10 steps -> 20.83 sec | avg 2082.7 ms/step\n",
      "========================================================================\n",
      "cifar | forward | mean±std over 10 runs: 43.54±8.53%\n",
      "Saved summary to results2025-10-31-08:38/cifar_forward_False_2025-10-31-08:38.json\n",
      "q:0.7\n",
      "\n",
      "Est T After Anchor point\n",
      "row sums: [1.0493636 1.015275  0.9353613]\n",
      "col sums: [1. 1. 1.]\n",
      "\n",
      "Est T After T Revision\n",
      "row sums: [1.0430155 1.0132998 0.9436846]\n",
      "col sums: [1.        1.0000001 1.       ]\n",
      "0.7\n",
      "GCE loss\n",
      "Run 01/10: test acc = 41.27%\n",
      "mps: 1 steps -> 20.68 sec | avg 20678.7 ms/step\n",
      "q:0.7\n",
      "\n",
      "Est T After Anchor point\n",
      "row sums: [1.0348637 1.0296634 0.9354728]\n",
      "col sums: [1.         1.         0.99999994]\n",
      "\n",
      "Est T After T Revision\n",
      "row sums: [1.0304047 1.0258329 0.9437623]\n",
      "col sums: [0.99999994 1.         1.        ]\n",
      "0.7\n",
      "GCE loss\n",
      "Run 02/10: test acc = 50.70%\n",
      "mps: 2 steps -> 21.31 sec | avg 10652.8 ms/step\n",
      "q:0.7\n",
      "\n",
      "Est T After Anchor point\n",
      "row sums: [0.96272004 0.9827586  1.0545214 ]\n",
      "col sums: [0.99999994 1.         1.        ]\n",
      "\n",
      "Est T After T Revision\n",
      "row sums: [0.96755826 0.98499274 1.047449  ]\n",
      "col sums: [1. 1. 1.]\n",
      "0.7\n",
      "GCE loss\n",
      "Run 03/10: test acc = 52.10%\n",
      "mps: 3 steps -> 20.53 sec | avg 6843.6 ms/step\n",
      "q:0.7\n",
      "\n",
      "Est T After Anchor point\n",
      "row sums: [0.6970325  1.6332357  0.66973186]\n",
      "col sums: [1. 1. 1.]\n",
      "\n",
      "Est T After T Revision\n",
      "row sums: [0.6930304 1.6376605 0.669309 ]\n",
      "col sums: [1. 1. 1.]\n",
      "0.7\n",
      "GCE loss\n",
      "Run 04/10: test acc = 48.00%\n",
      "mps: 4 steps -> 21.19 sec | avg 5296.6 ms/step\n",
      "q:0.7\n",
      "\n",
      "Est T After Anchor point\n",
      "row sums: [0.69094896 1.0551382  1.2539129 ]\n",
      "col sums: [1. 1. 1.]\n",
      "\n",
      "Est T After T Revision\n",
      "row sums: [0.68774015 1.0656562  1.2466038 ]\n",
      "col sums: [1. 1. 1.]\n",
      "0.7\n",
      "GCE loss\n",
      "Run 05/10: test acc = 48.27%\n",
      "mps: 5 steps -> 20.91 sec | avg 4181.3 ms/step\n",
      "q:0.7\n",
      "\n",
      "Est T After Anchor point\n",
      "row sums: [0.9117851  1.1218823  0.96633255]\n",
      "col sums: [1.         0.99999994 1.        ]\n",
      "\n",
      "Est T After T Revision\n",
      "row sums: [0.92319953 1.1061213  0.9706793 ]\n",
      "col sums: [1. 1. 1.]\n",
      "0.7\n",
      "GCE loss\n",
      "Run 06/10: test acc = 53.17%\n",
      "mps: 6 steps -> 20.84 sec | avg 3473.5 ms/step\n",
      "q:0.7\n",
      "\n",
      "Est T After Anchor point\n",
      "row sums: [0.9489744  0.99448526 1.0565403 ]\n",
      "col sums: [1. 1. 1.]\n",
      "\n",
      "Est T After T Revision\n",
      "row sums: [0.9555858  0.99518824 1.0492262 ]\n",
      "col sums: [1.0000001 1.        1.       ]\n",
      "0.7\n",
      "GCE loss\n",
      "Run 07/10: test acc = 42.37%\n",
      "mps: 7 steps -> 20.68 sec | avg 2954.1 ms/step\n",
      "q:0.7\n",
      "\n",
      "Est T After Anchor point\n",
      "row sums: [1.035712  0.9308258 1.0334623]\n",
      "col sums: [1. 1. 1.]\n",
      "\n",
      "Est T After T Revision\n",
      "row sums: [1.0311024  0.93976533 1.0291321 ]\n",
      "col sums: [1.         1.         0.99999994]\n",
      "0.7\n",
      "GCE loss\n",
      "Run 08/10: test acc = 52.03%\n",
      "mps: 8 steps -> 21.66 sec | avg 2707.8 ms/step\n",
      "q:0.7\n",
      "\n",
      "Est T After Anchor point\n",
      "row sums: [0.44296816 1.8249693  0.7320625 ]\n",
      "col sums: [1. 1. 1.]\n",
      "\n",
      "Est T After T Revision\n",
      "row sums: [0.47184944 1.8045658  0.7235847 ]\n",
      "col sums: [1. 1. 1.]\n",
      "0.7\n",
      "GCE loss\n",
      "Run 09/10: test acc = 43.60%\n",
      "mps: 9 steps -> 21.74 sec | avg 2415.7 ms/step\n",
      "q:0.7\n",
      "\n",
      "Est T After Anchor point\n",
      "row sums: [1.0401804  0.95067805 1.0091416 ]\n",
      "col sums: [1. 1. 1.]\n",
      "\n",
      "Est T After T Revision\n",
      "row sums: [1.0349976 0.957055  1.0079474]\n",
      "col sums: [1.         1.0000001  0.99999994]\n",
      "0.7\n",
      "GCE loss\n",
      "Run 10/10: test acc = 35.10%\n",
      "mps: 10 steps -> 21.39 sec | avg 2138.8 ms/step\n",
      "========================================================================\n",
      "cifar | gce | mean±std over 10 runs: 46.66±5.59%\n",
      "Saved summary to results2025-10-31-08:38/cifar_gce_False_2025-10-31-08:38.json\n",
      "q:0.7\n",
      "\n",
      "Est T After Anchor point\n",
      "row sums: [1.0493636 1.015275  0.9353613]\n",
      "col sums: [1. 1. 1.]\n",
      "\n",
      "Est T After T Revision\n",
      "row sums: [1.0430155 1.0132998 0.9436846]\n",
      "col sums: [1.        1.0000001 1.       ]\n",
      "0.7\n",
      "forwardGCE loss\n",
      "Run 01/10: test acc = 33.33%\n",
      "mps: 1 steps -> 21.43 sec | avg 21426.9 ms/step\n",
      "q:0.7\n",
      "\n",
      "Est T After Anchor point\n",
      "row sums: [1.0348637 1.0296634 0.9354728]\n",
      "col sums: [1.         1.         0.99999994]\n",
      "\n",
      "Est T After T Revision\n",
      "row sums: [1.0304047 1.0258329 0.9437623]\n",
      "col sums: [0.99999994 1.         1.        ]\n",
      "0.7\n",
      "forwardGCE loss\n",
      "Run 02/10: test acc = 33.60%\n",
      "mps: 2 steps -> 21.39 sec | avg 10696.3 ms/step\n",
      "q:0.7\n",
      "\n",
      "Est T After Anchor point\n",
      "row sums: [0.96272004 0.9827586  1.0545214 ]\n",
      "col sums: [0.99999994 1.         1.        ]\n",
      "\n",
      "Est T After T Revision\n",
      "row sums: [0.96755826 0.98499274 1.047449  ]\n",
      "col sums: [1. 1. 1.]\n",
      "0.7\n",
      "forwardGCE loss\n",
      "Run 03/10: test acc = 33.33%\n",
      "mps: 3 steps -> 21.46 sec | avg 7152.8 ms/step\n",
      "q:0.7\n",
      "\n",
      "Est T After Anchor point\n",
      "row sums: [0.6970325  1.6332357  0.66973186]\n",
      "col sums: [1. 1. 1.]\n",
      "\n",
      "Est T After T Revision\n",
      "row sums: [0.6930304 1.6376605 0.669309 ]\n",
      "col sums: [1. 1. 1.]\n",
      "0.7\n",
      "forwardGCE loss\n",
      "Run 04/10: test acc = 33.33%\n",
      "mps: 4 steps -> 21.98 sec | avg 5495.4 ms/step\n",
      "q:0.7\n",
      "\n",
      "Est T After Anchor point\n",
      "row sums: [0.69094896 1.0551382  1.2539129 ]\n",
      "col sums: [1. 1. 1.]\n",
      "\n",
      "Est T After T Revision\n",
      "row sums: [0.68774015 1.0656562  1.2466038 ]\n",
      "col sums: [1. 1. 1.]\n",
      "0.7\n",
      "forwardGCE loss\n",
      "Run 05/10: test acc = 33.33%\n",
      "mps: 5 steps -> 20.94 sec | avg 4187.7 ms/step\n",
      "q:0.7\n",
      "\n",
      "Est T After Anchor point\n",
      "row sums: [0.9117851  1.1218823  0.96633255]\n",
      "col sums: [1.         0.99999994 1.        ]\n",
      "\n",
      "Est T After T Revision\n",
      "row sums: [0.92319953 1.1061213  0.9706793 ]\n",
      "col sums: [1. 1. 1.]\n",
      "0.7\n",
      "forwardGCE loss\n",
      "Run 06/10: test acc = 33.33%\n",
      "mps: 6 steps -> 22.29 sec | avg 3714.5 ms/step\n",
      "q:0.7\n",
      "\n",
      "Est T After Anchor point\n",
      "row sums: [0.9489744  0.99448526 1.0565403 ]\n",
      "col sums: [1. 1. 1.]\n",
      "\n",
      "Est T After T Revision\n",
      "row sums: [0.9555858  0.99518824 1.0492262 ]\n",
      "col sums: [1.0000001 1.        1.       ]\n",
      "0.7\n",
      "forwardGCE loss\n",
      "Run 07/10: test acc = 33.33%\n",
      "mps: 7 steps -> 21.65 sec | avg 3093.1 ms/step\n",
      "q:0.7\n",
      "\n",
      "Est T After Anchor point\n",
      "row sums: [1.035712  0.9308258 1.0334623]\n",
      "col sums: [1. 1. 1.]\n",
      "\n",
      "Est T After T Revision\n",
      "row sums: [1.0311024  0.93976533 1.0291321 ]\n",
      "col sums: [1.         1.         0.99999994]\n",
      "0.7\n",
      "forwardGCE loss\n",
      "Run 08/10: test acc = 39.97%\n",
      "mps: 8 steps -> 21.36 sec | avg 2670.3 ms/step\n",
      "q:0.7\n",
      "\n",
      "Est T After Anchor point\n",
      "row sums: [0.44296816 1.8249693  0.7320625 ]\n",
      "col sums: [1. 1. 1.]\n",
      "\n",
      "Est T After T Revision\n",
      "row sums: [0.47184944 1.8045658  0.7235847 ]\n",
      "col sums: [1. 1. 1.]\n",
      "0.7\n",
      "forwardGCE loss\n",
      "Run 09/10: test acc = 33.33%\n",
      "mps: 9 steps -> 20.98 sec | avg 2331.1 ms/step\n",
      "q:0.7\n",
      "\n",
      "Est T After Anchor point\n",
      "row sums: [1.0401804  0.95067805 1.0091416 ]\n",
      "col sums: [1. 1. 1.]\n",
      "\n",
      "Est T After T Revision\n",
      "row sums: [1.0349976 0.957055  1.0079474]\n",
      "col sums: [1.         1.0000001  0.99999994]\n",
      "0.7\n",
      "forwardGCE loss\n",
      "Run 10/10: test acc = 33.33%\n",
      "mps: 10 steps -> 21.40 sec | avg 2139.8 ms/step\n",
      "========================================================================\n",
      "cifar | forwardGCE | mean±std over 10 runs: 34.02±1.98%\n",
      "Saved summary to results2025-10-31-08:38/cifar_forwardGCE_False_2025-10-31-08:38.json\n",
      "q:0.3\n",
      "\n",
      "Est T After Anchor point\n",
      "row sums: [1.0159502 0.9755118 1.008538 ]\n",
      "col sums: [1. 1. 1.]\n",
      "Fro: 0.7002944\n",
      "MAE: 0.19909726\n",
      "\n",
      "Est T After T Revision\n",
      "row sums: [0.98345774 0.96573895 1.0508033 ]\n",
      "col sums: [1.        1.        1.0000001]\n",
      "Fro: 0.70211977\n",
      "MAE: 0.19250295\n",
      "0.3\n",
      "forward loss\n"
     ]
    }
   ],
   "source": [
    "for cfg in configs:\n",
    "    all_acc = []\n",
    "    last_T = None\n",
    "    t_arr = []\n",
    "    for r in range(cfg['runs']):\n",
    "        start = time.perf_counter()\n",
    "        acc, T = run_once(cfg, seed=1000+r)\n",
    "\n",
    "        all_acc.append(acc)\n",
    "        if cfg['estimate_T'] or cfg['dataset']=='cifar':\n",
    "            t_arr.append(T)\n",
    "        last_T = T if T is not None else last_T\n",
    "        print(f\"Run {r+1:02d}/{cfg['runs']}: test acc = {acc*100:.2f}%\")\n",
    "        end = time.perf_counter()\n",
    "        print(f\"{cfg['device']}: {r+1} steps -> {end - start:.2f} sec | avg {1000*(end - start)/(r+1):.1f} ms/step\")\n",
    "    mean = float(np.mean(all_acc))\n",
    "    std  = float(np.std(all_acc))\n",
    "\n",
    "    summary = {\n",
    "        'cfg':cfg,\n",
    "        'dataset': cfg['dataset'],\n",
    "        'loss': cfg['loss'],\n",
    "        'estimate_T': bool(cfg['estimate_T']),\n",
    "        'epochs': cfg['epochs'],\n",
    "        'runs': cfg['runs'],\n",
    "        'mean_test_acc': mean,\n",
    "        'std_test_acc': std,\n",
    "        'last_estimated_T': last_T,\n",
    "        't_arr':t_arr,\n",
    "        'per_run_acc': all_acc,\n",
    "    }\n",
    "    print(\"=\"*72)\n",
    "    print(f\"{cfg['dataset']} | {cfg['loss']} | mean±std over {cfg['runs']} runs: {mean*100:.2f}±{std*100:.2f}%\")\n",
    "\n",
    "    with open(cfg['out'], 'w') as f:\n",
    "        json.dump(summary, f, indent=2)\n",
    "    print(f\"Saved summary to {cfg['out']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fashion03_forward_True_2025-10-30-22:31.json:\n",
      "  dataset       = fashion03\n",
      "  loss          = forward\n",
      "  mean_test_acc = 0.9841\n",
      "  std_test_acc  = 0.0017\n",
      "------------------------------------------------------------\n",
      "fashion03_gce_True_2025-10-30-22:31.json:\n",
      "  dataset       = fashion03\n",
      "  loss          = gce\n",
      "  mean_test_acc = 0.9756\n",
      "  std_test_acc  = 0.0024\n",
      "------------------------------------------------------------\n",
      "fashion03_forwardGCE_True_2025-10-30-22:31.json:\n",
      "  dataset       = fashion03\n",
      "  loss          = forwardGCE\n",
      "  mean_test_acc = 0.7193\n",
      "  std_test_acc  = 0.0558\n",
      "------------------------------------------------------------\n",
      "fashion03_forward_False_2025-10-30-22:31.json:\n",
      "  dataset       = fashion03\n",
      "  loss          = forward\n",
      "  mean_test_acc = 0.5975\n",
      "  std_test_acc  = 0.0634\n",
      "------------------------------------------------------------\n",
      "fashion03_forwardGCE_False_2025-10-30-22:31.json:\n",
      "  dataset       = fashion03\n",
      "  loss          = forwardGCE\n",
      "  mean_test_acc = 0.9842\n",
      "  std_test_acc  = 0.0015\n",
      "------------------------------------------------------------\n",
      "fashion03_gce_False_2025-10-30-22:31.json:\n",
      "  dataset       = fashion03\n",
      "  loss          = gce\n",
      "  mean_test_acc = 0.9755\n",
      "  std_test_acc  = 0.0025\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for filename in os.listdir(folder):\n",
    "    if filename.endswith(\".json\"):\n",
    "        filepath = os.path.join(folder, filename)\n",
    "        try:\n",
    "            with open(filepath, \"r\", encoding=\"utf-8\") as f:\n",
    "                data = json.load(f)\n",
    "            \n",
    "            dataset = data.get(\"dataset\", \"N/A\")\n",
    "            loss = data.get(\"loss\", \"N/A\")\n",
    "            mean_acc = data.get(\"mean_test_acc\", None)\n",
    "            std_acc = data.get(\"std_test_acc\", None)\n",
    "\n",
    "            print(f\"{filename}:\")\n",
    "            print(f\"  dataset       = {dataset}\")\n",
    "            print(f\"  loss          = {loss}\")\n",
    "            print(f\"  mean_test_acc = {mean_acc:.4f}\" if mean_acc is not None else \"  mean_test_acc = N/A\")\n",
    "            print(f\"  std_test_acc  = {std_acc:.4f}\" if std_acc is not None else \"  std_test_acc = N/A\")\n",
    "            print(\"-\" * 60)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error reading {filename}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading: results2025-10-30-22:31/fashion03_forwardGCE_True_2025-10-30-22:31.json\n",
      "[[6.77233160e-01 9.09449057e-07 3.09233755e-01]\n",
      " [2.68797010e-01 6.96125269e-01 4.39298972e-02]\n",
      " [5.39697893e-02 3.03873837e-01 6.46836400e-01]]\n",
      "[[0.7 0.3 0. ]\n",
      " [0.  0.7 0.3]\n",
      " [0.3 0.  0.7]]\n",
      "Fro error: 0.6925883346021815\n",
      "rre error: 0.5250498579031176\n",
      "mae error: 0.19597879750514519\n",
      "Per-row correlations: [np.float64(0.6099112373696552), np.float64(0.7055756493819895), np.float64(0.641358232171299)]\n",
      "Mean: 0.6522817063076479\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mIndexError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[197]\u001b[39m\u001b[32m, line 35\u001b[39m\n\u001b[32m     31\u001b[39m files = \u001b[38;5;28msorted\u001b[39m(glob.glob(os.path.join(folder,\u001b[33m\"\u001b[39m\u001b[33mfashion06*True**.json\u001b[39m\u001b[33m\"\u001b[39m)))\n\u001b[32m     34\u001b[39m \u001b[38;5;66;03m# pick the first matching file\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m35\u001b[39m first_file = \u001b[43mfiles\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[32m     36\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mLoading:\u001b[39m\u001b[33m\"\u001b[39m, first_file)\n\u001b[32m     37\u001b[39m \u001b[38;5;66;03m# load the JSON contents\u001b[39;00m\n",
      "\u001b[31mIndexError\u001b[39m: list index out of range"
     ]
    }
   ],
   "source": [
    "C = 3\n",
    "\n",
    "# pattern for files starting with \"name\" and ending with \".json\"\n",
    "files = sorted(glob.glob(os.path.join(folder,\"fashion03*True*.json\")))\n",
    "\n",
    "# pick the first matching file\n",
    "first_file = files[0]\n",
    "print(\"Loading:\", first_file)\n",
    "\n",
    "# load the JSON contents\n",
    "with open(first_file, \"r\", encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "T_prime = np.array(data['last_estimated_T'])\n",
    "T_true = pick_known_T('fashion03')\n",
    "\n",
    "#checking recreation performance\n",
    "print(T_prime)\n",
    "print(T_true)\n",
    "print(f\"Fro error: {np.linalg.norm(T_prime - T_true, 'fro')}\")\n",
    "print(f\"rre error: {np.linalg.norm(T_prime - T_true, 'fro') / np.linalg.norm(T_true, 'fro')}\")\n",
    "print(f\"mae error: {np.mean(np.abs(T_prime - T_true))}\")\n",
    "\n",
    "\n",
    "corrs = [st.pearsonr(T_true[i], T_prime[i])[0] for i in range(C)]\n",
    "print(\"Per-row correlations:\", corrs)\n",
    "print(\"Mean:\", np.mean(corrs))\n",
    "\n",
    "\n",
    "# pattern for files starting with \"name\" and ending with \".json\"\n",
    "files = sorted(glob.glob(os.path.join(folder,\"fashion06*True**.json\")))\n",
    "\n",
    "\n",
    "# pick the first matching file\n",
    "first_file = files[0]\n",
    "print(\"\\nLoading:\", first_file)\n",
    "# load the JSON contents\n",
    "with open(first_file, \"r\", encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)\n",
    "print()\n",
    "T_prime = np.array(data['last_estimated_T'])\n",
    "T_true = pick_known_T('fashion06')\n",
    "\n",
    "#checking recreation performance\n",
    "print(T_prime)\n",
    "print(T_true)\n",
    "print(f\"Fro error: {np.linalg.norm(T_prime - T_true, 'fro')}\")\n",
    "print(f\"rre error: {np.linalg.norm(T_prime - T_true, 'fro') / np.linalg.norm(T_true, 'fro')}\")\n",
    "print(f\"mae error: {np.mean(np.abs(T_prime - T_true))}\")\n",
    "\n",
    "\n",
    "corrs = [st.pearsonr(T_true[i], T_prime[i])[0] for i in range(C)]\n",
    "print(\"Per-row correlations:\", corrs)\n",
    "print(\"Mean:\", np.mean(corrs))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
