{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9IrXtNd8l-9H"
   },
   "source": [
    "## COMP5328 - Advanced Machine Learning\n",
    "## Assignment 2: Title\n",
    "----------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 468,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Common imports\n",
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import json\n",
    "import time\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import tabulate\n",
    "\n",
    "# Ploting\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import os\n",
    "import torch.optim as optim\n",
    "import random\n",
    "import scipy.stats as st"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 469,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Using Apple MPS: mps\n"
     ]
    }
   ],
   "source": [
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "    print(\"✅ Using Apple MPS:\", device)\n",
    "elif torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"✅ Using NVIDIA CUDA:\", device)\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"⚠️ Using CPU:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 470,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment variables\n",
    "# Common\n",
    "num_classes=3\n",
    "dataset_folder = 'data/'\n",
    "\n",
    "cifar_dataset = dataset_folder+'CIFAR.npz'\n",
    "MNISTO3_dataset = dataset_folder+'FashionMNIST0.3.npz'\n",
    "MNISTO6_dataset = dataset_folder+'FashionMNIST0.6.npz'\n",
    "\n",
    "\n",
    "DATA_PATHS = {\n",
    "    'fashion03': MNISTO3_dataset,\n",
    "    'fashion06': MNISTO6_dataset,\n",
    "    'cifar':     cifar_dataset\n",
    "}\n",
    "\n",
    "losses = ['forward','gce', 'forwardGCE', 'trevision']\n",
    "losses = ['forwardGCE', 'trevision']\n",
    "datasets = ['cifar', \"fashion03\", \"fashion06\"]\n",
    "base = {\n",
    "    \"runs\":10,\n",
    "    \"epochs\": 10,\n",
    "    \"loss\":'forward',\n",
    "    \"batch_size\":256,\n",
    "    \"q\":0.6,\n",
    "    \"est_epochs\":25,\n",
    "    \"beta\":5e-4,\n",
    "    \"lr\": 1e-2,\n",
    "    \"device\":str(device)\n",
    "}\n",
    "\n",
    "known_T_fashion_03 = np.array(  [[0.7,0.3,0.0],\n",
    "                                [0.0,0.7,0.3],\n",
    "                                [0.3,0.0,0.7]], dtype=np.float32)\n",
    "\n",
    "known_T_fashion_06 = np.array(  [[0.4,0.3,0.3],\n",
    "                                [0.3,0.4,0.3],\n",
    "                                [0.3,0.3,0.4]], dtype=np.float32)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 471,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Common Functions\n",
    "def pick_known_T(tag):\n",
    "    if tag == 'fashion03':\n",
    "        return known_T_fashion_03\n",
    "    elif tag == 'fashion06':\n",
    "        return known_T_fashion_06\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def set_seed(seed=0):\n",
    "    import random\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "def sanity(T, name, dataset):\n",
    "    print(f\"\\n{name}\")\n",
    "\n",
    "    # ensure tensor and move to CPU for printing\n",
    "    if isinstance(T, np.ndarray):\n",
    "        T = torch.tensor(T, dtype=torch.float32)\n",
    "    T = T.detach().cpu()\n",
    "\n",
    "    print(\"row sums:\", T.sum(dim=1))\n",
    "    print(\"col sums:\", T.sum(dim=0))\n",
    "    # Check T is row-stochastic and reasonable\n",
    "\n",
    "    print(\"cond(T):\", float(torch.linalg.cond(T).item()))\n",
    "\n",
    "\n",
    "\n",
    "    # pick true transition\n",
    "    if dataset == 'fashion03':\n",
    "        T_true = torch.tensor(known_T_fashion_03, dtype=torch.float32)\n",
    "    elif dataset == 'fashion06':\n",
    "        T_true = torch.tensor(known_T_fashion_06, dtype=torch.float32)\n",
    "    else:\n",
    "        T_true = None\n",
    "\n",
    "    if T_true is not None:\n",
    "        T_true = T_true.to(T.device)\n",
    "        fro = torch.norm(T - T_true, p='fro').item()\n",
    "        mae = torch.mean(torch.abs(T - T_true)).item()\n",
    "        print(f\"Fro: {fro:.6f}\")\n",
    "        print(f\"MAE: {mae:.6f}\")\n",
    "\n",
    "\n",
    "def load_npz(path):\n",
    "    d = np.load(path)\n",
    "    Xtr, Str = d['Xtr'], d['Str']\n",
    "    Xts, Yts = d['Xts'], d['Yts']\n",
    "    return Xtr, Str, Xts, Yts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qYzWuiytl-9I"
   },
   "source": [
    "## 1. Load Dataset\n",
    "\n",
    "### 1.0 Data Folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 472,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OxEr9ihznSqa",
    "outputId": "8b9fd2c9-b2f9-45f4-94aa-d0d473b7e140"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 172688\n",
      "-rw-r--r--@ 1 jamie.saunders  staff  55440974 Oct  4  2019 CIFAR.npz\n",
      "-rw-r--r--@ 1 jamie.saunders  staff  16485974 Oct 10  2021 FashionMNIST0.3.npz\n",
      "-rw-r--r--@ 1 jamie.saunders  staff  16485974 Oct 10  2021 FashionMNIST0.6.npz\n"
     ]
    }
   ],
   "source": [
    "# The structure of data folder.\n",
    "!ls -l data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 473,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code taken from Week10 Tutorial\n",
    "\n",
    "# A helper class, it is used as an input of the DataLoader object.\n",
    "class DatasetArray(Dataset):\n",
    "    r\"\"\"This is a child class of the pytorch Dataset object.\"\"\"\n",
    "    def __init__(self, data, labels=None, transform=None):\n",
    "        if labels != None:\n",
    "            self.data_arr = np.asarray(data).astype(np.float32)\n",
    "            self.label_arr = np.asarray(labels).astype(np.long)\n",
    "        else:\n",
    "            tmp_arr = np.asarray(data)\n",
    "            self.data_arr = tmp_arr[:,:-1].astype(np.float32)\n",
    "            self.label_arr = tmp_arr[:,-1].astype(np.long)\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data_arr)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "     \n",
    "        data = self.data_arr[index]\n",
    "        label = self.label_arr[index]\n",
    "        \n",
    "        if self.transform is not None:\n",
    "            data = self.transform(data)\n",
    "            \n",
    "        return (data, label)\n",
    "    \n",
    "    \n",
    "# Splitting the data into three parts.\n",
    "def train_val_test_random_split(data, fracs=[0.7,0.1,0.2]):\n",
    "    r\"\"\"Split the data into training, validation and test set.\n",
    "    Args:\n",
    "        fracs: a list of length three\n",
    "    \"\"\"\n",
    "    assert len(fracs) == 3\n",
    "    assert sum(fracs) == 1\n",
    "    assert all(frac > 0 for frac in fracs)\n",
    "    n = len(data)\n",
    "    subset_lens = [int(n*frac) for frac in fracs]\n",
    "    idxs = list(range(n))\n",
    "    random.shuffle(idxs)\n",
    "    data = np.array(data)\n",
    "    new_data = []\n",
    "    start_idx = 0\n",
    "    for subset_len in subset_lens:\n",
    "        end_idx = start_idx + subset_len\n",
    "        cur_idxs = idxs[start_idx:end_idx]\n",
    "        new_data.append(data[cur_idxs,:].tolist())\n",
    "        start_idx = end_idx\n",
    "    return new_data\n",
    "\n",
    "# Preparation of the data for training, validation and testing a pytorch network. \n",
    "# Note that the test data is not in use for this lab.\n",
    "def get_loader(batch_size =128, num_workers = 0, train_val_test_split = [0.7,0.1,0.2], data=None):\n",
    "    r\"\"\"This function is used to read the data file and split the data into three subsets, i.e, \n",
    "    train data, validation data and test data. Their corresponding DataLoader objects are returned.\"\"\"\n",
    "    \n",
    "    [train_data, val_data, test_data] = train_val_test_random_split(data, fracs = train_val_test_split)\n",
    "\n",
    "    train_data = DatasetArray(data = train_data)\n",
    "    val_data = DatasetArray(data = val_data)\n",
    "    test_data = DatasetArray(data = test_data)\n",
    "\n",
    "    #The pytorch built-in class DataLoader can help us to shuffle the data, draw mini-batch,\n",
    "    #do transformations, etc. \n",
    "    train_loader = DataLoader(\n",
    "        train_data,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=num_workers,\n",
    "    )\n",
    "\n",
    "    val_loader = DataLoader(\n",
    "        val_data,\n",
    "        batch_size=100,\n",
    "        shuffle=False,\n",
    "        num_workers=num_workers,\n",
    "    )\n",
    "\n",
    "    test_loader = DataLoader(\n",
    "        test_data,\n",
    "        batch_size=100,\n",
    "        num_workers=num_workers,\n",
    "        shuffle=False,\n",
    "    )\n",
    "    return train_loader, val_loader, test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 474,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NpzDataset(Dataset):\n",
    "    def __init__(self, X, y, is_cifar=False):\n",
    "        self.X = X.astype(np.float32)\n",
    "        self.y = y.astype(np.int64)\n",
    "        self.is_cifar = is_cifar\n",
    "\n",
    "        # Normalize to [0,1]\n",
    "        self.X = self.X / 255.0 if self.X.max() > 1.0 else self.X\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x = self.X[idx]\n",
    "        if x.ndim == 1:\n",
    "            # flat; try to infer shape 28x28 or 32x32x3\n",
    "            if x.size == 28*28:\n",
    "                x = x.reshape(1, 28, 28)\n",
    "            elif x.size == 32*32*3:\n",
    "                x = x.reshape(3, 32, 32)\n",
    "            else:\n",
    "                raise ValueError(\"Unknown flat image shape: {}\".format(x.shape))\n",
    "        else:\n",
    "            # (H,W) or (H,W,C)\n",
    "            if x.ndim == 2:\n",
    "                x = x[None, ...]  # to (1,H,W)\n",
    "            elif x.ndim == 3:\n",
    "                # assume HWC -> CHW\n",
    "                x = np.transpose(x, (2, 0, 1))\n",
    "            else:\n",
    "                raise ValueError(f\"Unexpected image dims: {x.shape}\")\n",
    "        return torch.from_numpy(x), torch.tensor(self.y[idx])\n",
    "\n",
    "\n",
    "\n",
    "def make_loaders(Xtr, Str, batch_size=128, seed=0, test_size=0.2):\n",
    "    # 80/20 split each repetition\n",
    "    X_tr, X_val, y_tr, y_val = train_test_split(\n",
    "        Xtr, Str, test_size=test_size, random_state=seed, stratify=Str\n",
    "    )\n",
    "\n",
    "    is_cifar = (X_tr.shape[-1] == 3) if X_tr.ndim == 4 else (X_tr.shape[-1] == 32*32*3)\n",
    "\n",
    "    train_ds = NpzDataset(X_tr, y_tr, is_cifar)\n",
    "    val_ds   = NpzDataset(X_val, y_val, is_cifar)\n",
    "\n",
    "    train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "    val_loader   = DataLoader(val_ds,   batch_size=batch_size, shuffle=False, num_workers=0)\n",
    "\n",
    "    return train_loader, val_loader, is_cifar\n",
    "\n",
    "def make_test_loader(Xts, Yts, batch_size=256):\n",
    "    is_cifar = (Xts.shape[-1] == 3) if Xts.ndim == 4 else (Xts.shape[-1] == 32*32*3)\n",
    "    test_ds = NpzDataset(Xts, Yts, is_cifar)\n",
    "    return DataLoader(test_ds, batch_size=batch_size, shuffle=False, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 475,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ForwardCorrectedCE(nn.Module):\n",
    "    \"\"\"\n",
    "    Forward loss correction: minimizes CE between T^T p and noisy labels.\n",
    "    T: class-transition matrix where T[i,j] = P(S=j | Y=i). Shape [C,C].\n",
    "    \"\"\"\n",
    "    def __init__(self, T):\n",
    "        super().__init__()\n",
    "        self.register_buffer('T', T)  # [C,C]\n",
    "\n",
    "    def forward(self, logits, y_noisy):\n",
    "        # logits -> p(y|x)\n",
    "        p = F.softmax(logits, dim=1)  # [B,C]\n",
    "        # mix via T^T\n",
    "        mixed = torch.clamp(p @ self.T, 1e-6, 1.0)\n",
    "        log_mixed = torch.log(mixed)\n",
    "        return F.nll_loss(log_mixed, y_noisy)\n",
    "\n",
    "class ForwardCorrectedGCE(nn.Module):\n",
    "    def __init__(self, T, q):\n",
    "        super().__init__()\n",
    "        self.register_buffer('T', T)\n",
    "        self.q = q\n",
    "    def forward(self, logits, y_noisy):\n",
    "        p_noisy = torch.clamp(F.softmax(logits, dim=1) @ self.T, 1e-6, 1.0)\n",
    "        p_s = p_noisy.gather(1, y_noisy.view(-1,1)).clamp(1e-6,1.0)\n",
    "        return (-(p_s.log()) if self.q==1.0 else (1 - p_s**self.q)/self.q).mean()\n",
    "\n",
    "class GeneralizedCrossEntropy(nn.Module):\n",
    "    \"\"\"\n",
    "    GCE loss: L_q(p, y) = (1 - p_y^q) / q, with q in (0,1].\n",
    "    q→1 recovers CE; smaller q is more robust to label noise.\n",
    "    \"\"\"\n",
    "    def __init__(self, q):\n",
    "        super().__init__()\n",
    "        assert 0 < q <= 1\n",
    "        self.q = q\n",
    "\n",
    "    def forward(self, logits, y):\n",
    "        p = F.softmax(logits, dim=1)\n",
    "        p_y = p.gather(1, y.view(-1,1)).clamp(min=1e-6, max=1.0)\n",
    "        if self.q == 1.0:\n",
    "            return -torch.log(p_y).mean()\n",
    "        return ((1 - p_y.pow(self.q)) / self.q).mean()\n",
    "    \n",
    "    \n",
    "class TRevisionLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    Joint T-revision loss: learns transition matrix T along with model weights.\n",
    "    Combines forward correction with a regularization toward identity (or prior).\n",
    "    \"\"\"\n",
    "    def __init__(self, device, C=3, q=0.7, lambda_reg=1e-3, T0=None):\n",
    "        super().__init__()\n",
    "        self.q = q\n",
    "        self.lambda_reg = lambda_reg\n",
    "        eps = 1e-6\n",
    "        # ------------------------------\n",
    "        # Initialize T0\n",
    "        # ------------------------------\n",
    "        if T0 is None:\n",
    "            # Start from identity (clean labels)\n",
    "            T0 = torch.eye(C, device=device, dtype=torch.float32)\n",
    "        else:\n",
    "            # Safely handle both numpy arrays and tensors\n",
    "            T0 = torch.as_tensor(T0, dtype=torch.float32, device=device)\n",
    "            # Ensure it's detached and cloned to avoid warnings\n",
    "            T0 = T0.detach().clone()\n",
    "\n",
    "        # Clamp to ensure positivity and numerical stability\n",
    "        T0 = torch.clamp(T0, eps, 1.0)\n",
    "\n",
    "        # ------------------------------\n",
    "        # Logits parameterization\n",
    "        # ------------------------------\n",
    "        # Learn unnormalized log-values; softmax over columns later gives valid T\n",
    "        self.logits = nn.Parameter(torch.log(T0))\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, logits, y):\n",
    "        \"\"\"\n",
    "        logits: model outputs (B, C)\n",
    "        y: noisy labels (B,)\n",
    "        \"\"\"\n",
    "        C = logits.size(1)\n",
    "        T = torch.softmax(self.logits, dim=1)  # ensure column-stochastic\n",
    "        p = F.softmax(logits, dim=1)\n",
    "        p_noisy = torch.clamp(p @ T, 1e-6, 1.0)\n",
    "        p_s = p_noisy.gather(1, y.view(-1, 1)).clamp(1e-6, 1.0)\n",
    "\n",
    "        if self.q == 1.0:\n",
    "            loss = -torch.log(p_s)\n",
    "        else:\n",
    "            loss = (1 - p_s.pow(self.q)) / self.q\n",
    "        loss = loss.mean()\n",
    "\n",
    "        # regularize T toward identity or prior\n",
    "        T_prior = torch.eye(C, device=T.device)\n",
    "        reg = self.lambda_reg * torch.norm(T - T_prior, p='fro')\n",
    "        return loss + reg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 476,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_block(cin, cout):\n",
    "    return nn.Sequential(\n",
    "        nn.Conv2d(cin, cout, 3, padding=1),\n",
    "        nn.BatchNorm2d(cout),\n",
    "        nn.ReLU(inplace=True),\n",
    "        nn.MaxPool2d(2)\n",
    "    )\n",
    "\n",
    "class SmallCNN28(nn.Module):\n",
    "    \"\"\"For 1×28×28 images.\"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            conv_block(1, 32),  # 14x14\n",
    "            conv_block(32, 64), # 7x7\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(64*7*7, 128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(128, 3)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "\n",
    "class SmallCNNCifar(nn.Module):\n",
    "    \"\"\"Improved for CIFAR (3×32×32).\"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            conv_block(3, 64),     # 16x16\n",
    "            conv_block(64, 128),   # 8x8\n",
    "            conv_block(128, 256),  # 4x4\n",
    "            nn.Dropout(0.3)\n",
    "        )\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(256*4*4, 256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(256, 3)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.classifier(self.features(x))\n",
    "\n",
    "\n",
    "def make_model(is_cifar, device=None):\n",
    "    model = SmallCNNCifar() if is_cifar else SmallCNN28()\n",
    "    if device is not None:\n",
    "        model = model.to(device)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 477,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def accuracy(model, loader, device):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for xb, yb in loader:\n",
    "        xb, yb = xb.to(device), yb.to(device)\n",
    "        logits = model(xb)\n",
    "        pred = logits.argmax(dim=1)\n",
    "        correct += (pred == yb).sum().item()\n",
    "        total += yb.numel()\n",
    "    return correct / max(total, 1)\n",
    "\n",
    "\n",
    "def train_one_epoch(model, loader, optimizer, criterion, device=device):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    n_samples = 0\n",
    "    for xb, yb in loader:\n",
    "        xb, yb = xb.to(device), yb.to(device)\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        logits = model(xb)\n",
    "        loss = criterion(logits, yb)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        batch_size = yb.size(0)\n",
    "        total_loss += loss.detach().item() * batch_size\n",
    "        n_samples += batch_size\n",
    "\n",
    "    mean_loss = total_loss / n_samples\n",
    "    return mean_loss\n",
    "\n",
    "\n",
    "def fit_model(model, train_loader, val_loader, device, loss_name, T, q, beta=0.2, epochs=10, lr=1e-3):\n",
    "\n",
    "    model.to(device)\n",
    "    if T is not None:\n",
    "        if not isinstance(T, torch.Tensor):\n",
    "            T = torch.tensor(T, dtype=torch.float32, device=device)\n",
    "        else:\n",
    "            T = T.detach().clone().to(torch.float32).to(device)\n",
    "        \n",
    "    if loss_name == 'forward':\n",
    "        assert T is not None, \"Forward correction requires known/estimated T\"\n",
    "        criterion = ForwardCorrectedCE(T)\n",
    "    elif loss_name == 'forwardGCE':\n",
    "        assert T is not None, \"Forward correction requires known/estimated T\"\n",
    "        criterion = ForwardCorrectedGCE(T, q=q)\n",
    "    elif loss_name == 'trevision':\n",
    "        assert T is not None, \"T-revision requires known/estimated T\"\n",
    "        criterion = TRevisionLoss(C=3,device=device, q=q, lambda_reg=beta, T0=T).to(device)\n",
    "    else:\n",
    "        criterion = GeneralizedCrossEntropy(q=q)\n",
    "        \n",
    "    criterion = criterion.to(device)\n",
    "\n",
    "    if isinstance(criterion, TRevisionLoss):\n",
    "        optimizer = optim.Adam(\n",
    "            list(model.parameters()) + list(criterion.parameters()),\n",
    "            lr=lr, weight_decay=1e-4\n",
    "        )\n",
    "    else:\n",
    "        optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=1e-4)\n",
    "\n",
    "    best_val = -np.inf\n",
    "    best_state = None\n",
    "    loss_arr = []\n",
    "\n",
    "    for _ in range(epochs):\n",
    "        mean_loss, mean_acc, loss = train_one_epoch(model, train_loader, optimizer, criterion, device)\n",
    "        # early stopping on val accuracy (cheap)\n",
    "        val_acc = accuracy(model, val_loader, device)\n",
    "        if val_acc > best_val:\n",
    "            best_val = val_acc\n",
    "            best_state = {k: v.detach().cpu().clone() for k, v in model.state_dict().items()}\n",
    "        loss_arr.append(loss)\n",
    "        \n",
    "    if best_state is not None:\n",
    "        model.load_state_dict(best_state)\n",
    "\n",
    "    return model, loss_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 478,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def train_one_epoch(model, loader, optimizer, criterion=None, device=None):\n",
    "    # ---- device setup ----\n",
    "    if device is None:\n",
    "        device = (\n",
    "            \"cuda\" if torch.cuda.is_available()\n",
    "            else \"mps\" if torch.backends.mps.is_available()\n",
    "            else \"cpu\"\n",
    "        )\n",
    "    device = torch.device(device)\n",
    "\n",
    "    # move model to device\n",
    "    model.to(device)\n",
    "    model.train()\n",
    "\n",
    "    if criterion is None:\n",
    "        criterion = torch.nn.CrossEntropyLoss(reduction='none')  # per-sample losses\n",
    "\n",
    "    all_losses = []   # for histogram\n",
    "    batch_metrics = []  # (batch_idx, acc, mean_loss)\n",
    "\n",
    "    for batch_idx, (xb, yb) in enumerate(loader, 1):\n",
    "        xb, yb = xb.to(device), yb.to(device)\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "        logits = model(xb)\n",
    "        per_sample_loss = criterion(logits, yb)\n",
    "\n",
    "        # ensure scalar loss\n",
    "        if per_sample_loss.ndim > 0:\n",
    "            loss = per_sample_loss.mean()\n",
    "            all_losses.extend(per_sample_loss.detach().cpu().tolist())\n",
    "        else:\n",
    "            loss = per_sample_loss\n",
    "            all_losses.append(loss.detach().cpu().item())\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        preds = logits.argmax(dim=1)\n",
    "        acc = (preds == yb).float().mean().item()\n",
    "        batch_metrics.append((batch_idx, acc, loss.item()))\n",
    "        #print(f\"Batch {batch_idx:03d}: loss={loss.item():.4f}, acc={acc*100:.2f}%\")\n",
    "        \n",
    "\n",
    "    # ---- epoch summary ----\n",
    "    mean_loss = sum([l for _,_,l in batch_metrics]) / len(batch_metrics)\n",
    "    mean_acc = sum([a for _,a,_ in batch_metrics]) / len(batch_metrics)\n",
    "    print(f\"\\nEpoch summary: mean_loss={mean_loss:.4f}, mean_acc={mean_acc*100:.2f}%\")\n",
    "    with torch.no_grad():\n",
    "        total_norm = 0\n",
    "        for p in model.parameters():\n",
    "            total_norm += p.grad.data.norm(2).item() if p.grad is not None else 0\n",
    "    print(f\"Grad norm: {total_norm:.4f}\")\n",
    "\n",
    "    return mean_loss, mean_acc, all_losses\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 479,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def predict_proba(model, loader, device):\n",
    "\n",
    "    model.eval()\n",
    "    p_list, y_list = [], []\n",
    "    for xb, yb in loader:\n",
    "        xb = xb.to(device)\n",
    "        output = model(xb)\n",
    "        p = F.softmax(output, dim=1)\n",
    "        p_list.append(p)\n",
    "        y_list.append(yb.to(device))\n",
    "    return torch.cat(p_list), torch.cat(y_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 480,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def plot_run_performance(all_acc, all_mean_loss, loss_name, dataset):\n",
    "    runs = np.arange(1, len(all_acc) + 1)\n",
    "\n",
    "    fig, ax1 = plt.subplots(figsize=(8,5))\n",
    "    ax2 = ax1.twinx()\n",
    "\n",
    "    # Accuracy (left axis)\n",
    "    ax1.plot(runs, np.array(all_acc)*100, 'o-', color='tab:blue', label='Test Accuracy (%)')\n",
    "    ax1.set_xlabel('Run')\n",
    "    ax1.set_ylabel('Accuracy (%)', color='tab:blue')\n",
    "    ax1.tick_params(axis='y', labelcolor='tab:blue')\n",
    "    ax1.set_ylim(0, 100)\n",
    "\n",
    "    # Loss (right axis)\n",
    "    ax2.plot(runs, all_mean_loss, 's--', color='tab:red', label='Mean Train Loss')\n",
    "    ax2.set_ylabel('Loss', color='tab:red')\n",
    "    ax2.tick_params(axis='y', labelcolor='tab:red')\n",
    "\n",
    "    plt.title(f\"{dataset.upper()} — {loss_name} Performance Across Runs\")\n",
    "    fig.tight_layout()\n",
    "\n",
    "    # combine legends from both axes\n",
    "    lines, labels = ax1.get_legend_handles_labels()\n",
    "    lines2, labels2 = ax2.get_legend_handles_labels()\n",
    "    ax1.legend(lines + lines2, labels + labels2, loc='best')\n",
    "\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 481,
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_transition_anchor(t, train_loader, is_cifar, q, device, epochs):\n",
    "    \"\"\"\n",
    "    Simple anchor/confident-example estimator (Patrini et al., 2017 style):\n",
    "    1) Train a base classifier on noisy data.\n",
    "    2) Get p(y|x) on training set.\n",
    "    3) For each clean class i, find indices whose predicted argmax == noisy label == i and with high confidence.\n",
    "    4) For those indices, estimate column i of T as average of empirical noisy label distribution given model predicts i.\n",
    "    Here: since we only have noisy labels S, we approximate T[:, i] ≈ E[ onehot(S) | argmax p = i, p_i >= τ ].\n",
    "    Normalize columns to sum to 1.\n",
    "    \"\"\"\n",
    "\n",
    "    model = make_model(is_cifar).to(device)\n",
    "\n",
    "    optimizer = optim.Adam(model.parameters(), lr=1e-3, weight_decay=1e-4)\n",
    "    criterion = GeneralizedCrossEntropy(q=q)\n",
    "    # quick warmup training on noisy labels\n",
    "    for _ in range(epochs):\n",
    "        train_one_epoch(model, train_loader, optimizer, criterion, device=device) \n",
    "        \n",
    "    #print('devise - estimation')   \n",
    "    #print(next(model.parameters()).device)\n",
    "\n",
    "\n",
    "    # collect probs & noisy labels\n",
    "    p_arr, y_noisy = predict_proba(model, train_loader, device)\n",
    "\n",
    "    # ensure both are torch tensors on same device\n",
    "    if not torch.is_tensor(p_arr):\n",
    "        p_arr = torch.tensor(p_arr, dtype=torch.float32, device=device)\n",
    "    if not torch.is_tensor(y_noisy):\n",
    "        y_noisy = torch.tensor(y_noisy, dtype=torch.long, device=device)\n",
    "    else:\n",
    "        y_noisy = y_noisy.to(device, dtype=torch.long)\n",
    "\n",
    "    preds = p_arr.argmax(dim=1)\n",
    "    maxp = p_arr.max(dim=1).values\n",
    "\n",
    "    C = num_classes\n",
    "    T = t\n",
    "\n",
    "    if isinstance(T, np.ndarray):\n",
    "        T = torch.tensor(T, dtype=torch.float32, device=device)\n",
    "    elif torch.is_tensor(T):\n",
    "        T = T.to(device)\n",
    "        \n",
    "    # choose class-wise thresholds based on quantiles for stability\n",
    "    for i in range(C):\n",
    "        idx = (preds == i).nonzero(as_tuple=True)[0]\n",
    "        if idx.numel() == 0:\n",
    "            T[i, :] = torch.ones(C, device=device) / C\n",
    "            continue\n",
    "        # high-confidence subset (top 30% by p_i)\n",
    "        conf = maxp[idx]\n",
    "        if conf.numel() > 50:\n",
    "            tau = torch.quantile(conf, 0.7).item()\n",
    "        else:\n",
    "            tau = conf.min().item()\n",
    "        keep = idx[conf >= tau]\n",
    "        if keep.size == 0:\n",
    "            keep = idx\n",
    "        # empirical distribution of noisy labels among confident examples\n",
    "        hist = torch.bincount(y_noisy[keep], minlength=C).float()\n",
    "        if hist.sum() == 0:\n",
    "            T[i, :] = torch.ones(C, device=device) / C\n",
    "        else:\n",
    "            T[i, :] = hist / hist.sum()\n",
    "\n",
    "    # normalize rows to sum to 1\n",
    "    T = torch.clamp(T, min=1e-8)\n",
    "    T = T / T.sum(dim=1, keepdim=True)\n",
    "\n",
    "    return T.detach().to(device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 482,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_once(args, T, seed):\n",
    "    set_seed(seed)\n",
    "    q = args['q']\n",
    "    device = torch.device(args['device'])\n",
    "\n",
    "    C=3\n",
    "    # load data\n",
    "    Xtr, Str, Xts, Yts = load_npz(DATA_PATHS[args['dataset']])\n",
    "\n",
    "    # loaders for this split\n",
    "    train_loader, val_loader, is_cifar = make_loaders(Xtr, Str, batch_size=args['batch_size'], seed=seed)\n",
    "    test_loader = make_test_loader(Xts, Yts, batch_size=512)\n",
    "\n",
    "    xb, yb = next(iter(train_loader))\n",
    "    print(\"Batch shape:\", xb.shape, \"min/max:\", xb.min().item(), xb.max().item())\n",
    "  \n",
    "    # choose model\n",
    "    model = make_model(is_cifar).to(device)\n",
    "\n",
    "\n",
    "    # fit\n",
    "    model, loss_arr = fit_model(\n",
    "        model,\n",
    "        train_loader,\n",
    "        val_loader,\n",
    "        device,\n",
    "        loss_name=args['loss'],\n",
    "        T=T,\n",
    "        q=q,\n",
    "        beta=args['beta'],\n",
    "        epochs=args['epochs'],\n",
    "        lr=args['lr'],\n",
    "    )\n",
    "\n",
    "    all_loss_arrays = loss_arr\n",
    "    \n",
    "    # evaluate on clean test set\n",
    "    test_acc = accuracy(model, test_loader, device)\n",
    "    return float(test_acc), (T if T is not None else None), loss_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 483,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset: cifar, estimate_T:anchor, loss:forwardGCE, q:0.7\n",
      "Skipping dataset: cifar, estimate_T:None, loss:forwardGCE, q:0.7\n",
      "dataset: cifar, estimate_T:anchor, loss:trevision, q:0.7\n",
      "Skipping dataset: cifar, estimate_T:None, loss:trevision, q:0.7\n",
      "dataset: fashion03, estimate_T:anchor, loss:forwardGCE, q:0.7\n",
      "dataset: fashion03, estimate_T:None, loss:forwardGCE, q:0.7\n",
      "dataset: fashion03, estimate_T:anchor, loss:trevision, q:0.7\n",
      "dataset: fashion03, estimate_T:None, loss:trevision, q:0.7\n",
      "dataset: fashion06, estimate_T:anchor, loss:forwardGCE, q:0.7\n",
      "dataset: fashion06, estimate_T:None, loss:forwardGCE, q:0.7\n",
      "dataset: fashion06, estimate_T:anchor, loss:trevision, q:0.7\n",
      "dataset: fashion06, estimate_T:None, loss:trevision, q:0.7\n"
     ]
    }
   ],
   "source": [
    "# Define config\n",
    "now = datetime.now()\n",
    "now = now.strftime(\"%Y-%m-%d-%H_%M\")\n",
    "folder = \"results_\"+now\n",
    "if os.path.exists(folder) and os.path.isdir(folder):\n",
    "    os.rmdir(folder)\n",
    "    os.mkdir(folder)\n",
    "else:\n",
    "    os.mkdir(folder)\n",
    "# create each cfg\n",
    "estimate = ['anchor', 'None']\n",
    "qs = [0.7]\n",
    "configs = []\n",
    "for q in qs:\n",
    "    for i, ds in enumerate(datasets):\n",
    "        if ds == 'cifar':\n",
    "            is_cifar = True\n",
    "        else:\n",
    "            is_cifar = False\n",
    "        \n",
    "        for loss in losses:\n",
    "            for t in estimate:\n",
    "                cfg = {**base, \"dataset\": ds, \"loss\":loss, \"estimate_T\":t, \"q\":q, \"is_cifar\":is_cifar}\n",
    "                if t=='None' and ds=='cifar':\n",
    "                    print(f\"Skipping dataset: {ds}, estimate_T:{t}, loss:{loss}, q:{q}\")\n",
    "                else:\n",
    "                    print(f\"dataset: {ds}, estimate_T:{t}, loss:{loss}, q:{q}\")\n",
    "                    configs.append(cfg)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 484,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stabilize_T(T, alpha=0.7):\n",
    "    \"\"\"\n",
    "    Blend estimated T with identity to avoid near-singular/over-uniform matrices.\n",
    "    Keeps rows summing to 1.\n",
    "    \"\"\"\n",
    "    C = T.size(0)\n",
    "    I = torch.eye(C, device=T.device, dtype=T.dtype)\n",
    "    T = alpha * T + (1 - alpha) * I\n",
    "    T = torch.clamp(T, 1e-3, 0.999)\n",
    "    T = T / T.sum(dim=1, keepdim=True)\n",
    "    return T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch summary: mean_loss=0.8192, mean_acc=34.53%\n",
      "Grad norm: 0.8414\n",
      "\n",
      "Epoch summary: mean_loss=0.7613, mean_acc=37.11%\n",
      "Grad norm: 0.7713\n",
      "\n",
      "Epoch summary: mean_loss=0.7578, mean_acc=37.38%\n",
      "Grad norm: 0.7263\n",
      "\n",
      "Epoch summary: mean_loss=0.7551, mean_acc=37.51%\n",
      "Grad norm: 0.8353\n",
      "\n",
      "Epoch summary: mean_loss=0.7518, mean_acc=38.84%\n",
      "Grad norm: 0.9744\n",
      "\n",
      "Epoch summary: mean_loss=0.7508, mean_acc=38.79%\n",
      "Grad norm: 0.7969\n",
      "\n",
      "Epoch summary: mean_loss=0.7509, mean_acc=38.43%\n",
      "Grad norm: 1.0075\n",
      "\n",
      "Epoch summary: mean_loss=0.7486, mean_acc=38.48%\n",
      "Grad norm: 0.9003\n",
      "\n",
      "Epoch summary: mean_loss=0.7460, mean_acc=39.38%\n",
      "Grad norm: 0.8948\n",
      "\n",
      "Epoch summary: mean_loss=0.7461, mean_acc=39.11%\n",
      "Grad norm: 0.7852\n",
      "\n",
      "Epoch summary: mean_loss=0.7435, mean_acc=39.99%\n",
      "Grad norm: 0.9658\n",
      "\n",
      "Epoch summary: mean_loss=0.7406, mean_acc=40.13%\n",
      "Grad norm: 0.7633\n",
      "\n",
      "Epoch summary: mean_loss=0.7400, mean_acc=39.41%\n",
      "Grad norm: 0.6257\n",
      "\n",
      "Epoch summary: mean_loss=0.7398, mean_acc=40.50%\n",
      "Grad norm: 0.7333\n",
      "\n",
      "Epoch summary: mean_loss=0.7377, mean_acc=40.03%\n",
      "Grad norm: 0.6233\n",
      "\n",
      "Epoch summary: mean_loss=0.7315, mean_acc=41.70%\n",
      "Grad norm: 0.8682\n",
      "\n",
      "Epoch summary: mean_loss=0.7324, mean_acc=41.31%\n",
      "Grad norm: 0.7707\n",
      "\n",
      "Epoch summary: mean_loss=0.7258, mean_acc=41.48%\n",
      "Grad norm: 1.0464\n",
      "\n",
      "Epoch summary: mean_loss=0.7192, mean_acc=43.12%\n",
      "Grad norm: 0.9766\n",
      "\n",
      "Epoch summary: mean_loss=0.7127, mean_acc=42.74%\n",
      "Grad norm: 0.9673\n",
      "\n",
      "Epoch summary: mean_loss=0.7110, mean_acc=43.70%\n",
      "Grad norm: 1.1696\n",
      "\n",
      "Epoch summary: mean_loss=0.6975, mean_acc=45.25%\n",
      "Grad norm: 0.9672\n",
      "\n",
      "Epoch summary: mean_loss=0.6965, mean_acc=45.17%\n",
      "Grad norm: 1.3230\n",
      "\n",
      "Epoch summary: mean_loss=0.6801, mean_acc=46.89%\n",
      "Grad norm: 1.3057\n",
      "\n",
      "Epoch summary: mean_loss=0.6589, mean_acc=49.31%\n",
      "Grad norm: 1.3417\n",
      "\n",
      "Est T After Anchor point\n",
      "row sums: tensor([1., 1., 1.])\n",
      "col sums: tensor([0.9930, 1.0511, 0.9560])\n",
      "cond(T): 1.9361321926116943\n",
      "Estimated T:\n",
      "[[0.69933    0.1557789  0.14489113]\n",
      " [0.12020461 0.73401535 0.14578006]\n",
      " [0.17341977 0.16126418 0.66531605]]\n",
      "Batch shape: torch.Size([256, 3, 32, 32]) min/max: 0.0 1.0\n",
      "\n",
      "Epoch summary: mean_loss=0.8272, mean_acc=33.32%\n",
      "Grad norm: 0.0000\n",
      "\n",
      "Epoch summary: mean_loss=0.8282, mean_acc=33.32%\n",
      "Grad norm: 0.0000\n",
      "\n",
      "Epoch summary: mean_loss=0.8281, mean_acc=33.33%\n",
      "Grad norm: 0.0000\n",
      "\n",
      "Epoch summary: mean_loss=0.8193, mean_acc=33.44%\n",
      "Grad norm: 0.2136\n",
      "\n",
      "Epoch summary: mean_loss=0.7686, mean_acc=34.21%\n",
      "Grad norm: 0.1302\n",
      "\n",
      "Epoch summary: mean_loss=0.7639, mean_acc=35.65%\n",
      "Grad norm: 0.1788\n",
      "\n",
      "Epoch summary: mean_loss=0.7642, mean_acc=35.69%\n",
      "Grad norm: 0.1947\n",
      "\n",
      "Epoch summary: mean_loss=0.7636, mean_acc=35.51%\n",
      "Grad norm: 0.1549\n",
      "\n",
      "Epoch summary: mean_loss=0.7623, mean_acc=35.98%\n",
      "Grad norm: 0.2011\n",
      "\n",
      "Epoch summary: mean_loss=0.7626, mean_acc=36.25%\n",
      "Grad norm: 0.2140\n",
      "Batch shape: torch.Size([256, 3, 32, 32]) min/max: 0.0 1.0\n",
      "\n",
      "Epoch summary: mean_loss=0.8196, mean_acc=33.48%\n",
      "Grad norm: 0.0000\n",
      "\n",
      "Epoch summary: mean_loss=0.8213, mean_acc=33.33%\n",
      "Grad norm: 0.0000\n",
      "\n",
      "Epoch summary: mean_loss=0.8213, mean_acc=33.33%\n",
      "Grad norm: 0.0000\n",
      "\n",
      "Epoch summary: mean_loss=0.8212, mean_acc=33.34%\n",
      "Grad norm: 0.0000\n",
      "\n",
      "Epoch summary: mean_loss=0.8161, mean_acc=33.49%\n",
      "Grad norm: 0.3455\n",
      "\n",
      "Epoch summary: mean_loss=0.7700, mean_acc=35.14%\n",
      "Grad norm: 0.2515\n",
      "\n",
      "Epoch summary: mean_loss=0.7650, mean_acc=35.70%\n",
      "Grad norm: 0.1218\n",
      "\n",
      "Epoch summary: mean_loss=0.7632, mean_acc=35.73%\n",
      "Grad norm: 0.2369\n",
      "\n",
      "Epoch summary: mean_loss=0.7640, mean_acc=36.02%\n",
      "Grad norm: 0.3018\n",
      "\n",
      "Epoch summary: mean_loss=0.7621, mean_acc=36.91%\n",
      "Grad norm: 0.1936\n",
      "Batch shape: torch.Size([256, 3, 32, 32]) min/max: 0.0 1.0\n",
      "\n",
      "Epoch summary: mean_loss=0.8280, mean_acc=33.23%\n",
      "Grad norm: 0.0000\n",
      "\n",
      "Epoch summary: mean_loss=0.8281, mean_acc=33.33%\n",
      "Grad norm: 0.0000\n",
      "\n",
      "Epoch summary: mean_loss=0.8281, mean_acc=33.33%\n",
      "Grad norm: 0.0081\n",
      "\n",
      "Epoch summary: mean_loss=0.8150, mean_acc=33.49%\n",
      "Grad norm: 0.2431\n",
      "\n",
      "Epoch summary: mean_loss=0.7689, mean_acc=35.20%\n",
      "Grad norm: 0.2592\n",
      "\n",
      "Epoch summary: mean_loss=0.7659, mean_acc=35.43%\n",
      "Grad norm: 0.3001\n",
      "\n",
      "Epoch summary: mean_loss=0.7636, mean_acc=36.07%\n",
      "Grad norm: 0.2618\n",
      "\n",
      "Epoch summary: mean_loss=0.7611, mean_acc=36.47%\n",
      "Grad norm: 0.2243\n",
      "\n",
      "Epoch summary: mean_loss=0.7604, mean_acc=36.33%\n",
      "Grad norm: 0.2140\n",
      "\n",
      "Epoch summary: mean_loss=0.7590, mean_acc=37.12%\n",
      "Grad norm: 0.2532\n",
      "Batch shape: torch.Size([256, 3, 32, 32]) min/max: 0.0 1.0\n",
      "\n",
      "Epoch summary: mean_loss=0.8211, mean_acc=33.27%\n",
      "Grad norm: 0.0000\n",
      "\n",
      "Epoch summary: mean_loss=0.8212, mean_acc=33.34%\n",
      "Grad norm: 0.0000\n",
      "\n",
      "Epoch summary: mean_loss=0.8212, mean_acc=33.34%\n",
      "Grad norm: 0.0037\n",
      "\n",
      "Epoch summary: mean_loss=0.8211, mean_acc=33.34%\n",
      "Grad norm: 0.2343\n",
      "\n",
      "Epoch summary: mean_loss=0.7672, mean_acc=33.97%\n",
      "Grad norm: 0.1358\n",
      "\n",
      "Epoch summary: mean_loss=0.7658, mean_acc=35.97%\n",
      "Grad norm: 0.2890\n",
      "\n",
      "Epoch summary: mean_loss=0.7636, mean_acc=35.92%\n",
      "Grad norm: 0.1805\n",
      "\n",
      "Epoch summary: mean_loss=0.7626, mean_acc=36.19%\n",
      "Grad norm: 0.1986\n",
      "\n",
      "Epoch summary: mean_loss=0.7642, mean_acc=36.39%\n",
      "Grad norm: 0.1716\n",
      "\n",
      "Epoch summary: mean_loss=0.7614, mean_acc=36.81%\n",
      "Grad norm: 0.2470\n",
      "Batch shape: torch.Size([256, 3, 32, 32]) min/max: 0.0 1.0\n",
      "\n",
      "Epoch summary: mean_loss=0.8208, mean_acc=33.22%\n",
      "Grad norm: 0.0000\n",
      "\n",
      "Epoch summary: mean_loss=0.8214, mean_acc=33.32%\n",
      "Grad norm: 0.0000\n",
      "\n",
      "Epoch summary: mean_loss=0.8213, mean_acc=33.33%\n",
      "Grad norm: 0.0000\n",
      "\n",
      "Epoch summary: mean_loss=0.8013, mean_acc=34.06%\n",
      "Grad norm: 0.1346\n",
      "\n",
      "Epoch summary: mean_loss=0.7666, mean_acc=34.57%\n",
      "Grad norm: 0.0987\n",
      "\n",
      "Epoch summary: mean_loss=0.7664, mean_acc=35.36%\n",
      "Grad norm: 0.2515\n",
      "\n",
      "Epoch summary: mean_loss=0.7658, mean_acc=35.38%\n",
      "Grad norm: 0.2045\n",
      "\n",
      "Epoch summary: mean_loss=0.7634, mean_acc=36.06%\n",
      "Grad norm: 0.2330\n",
      "\n",
      "Epoch summary: mean_loss=0.7636, mean_acc=35.70%\n",
      "Grad norm: 0.1623\n",
      "\n",
      "Epoch summary: mean_loss=0.7630, mean_acc=35.52%\n",
      "Grad norm: 0.1466\n",
      "Batch shape: torch.Size([256, 3, 32, 32]) min/max: 0.0 1.0\n",
      "\n",
      "Epoch summary: mean_loss=0.8361, mean_acc=33.20%\n",
      "Grad norm: 0.0000\n",
      "\n",
      "Epoch summary: mean_loss=0.8358, mean_acc=33.33%\n",
      "Grad norm: 0.0000\n",
      "\n",
      "Epoch summary: mean_loss=0.8360, mean_acc=33.31%\n",
      "Grad norm: 0.0000\n",
      "\n",
      "Epoch summary: mean_loss=0.8359, mean_acc=33.32%\n",
      "Grad norm: 0.0259\n",
      "\n",
      "Epoch summary: mean_loss=0.7738, mean_acc=33.36%\n",
      "Grad norm: 0.1325\n",
      "\n",
      "Epoch summary: mean_loss=0.7677, mean_acc=33.43%\n",
      "Grad norm: 0.1391\n"
     ]
    }
   ],
   "source": [
    "results_rows = []\n",
    "for cfg in configs:\n",
    "      # collect summary rows for TSV\n",
    "    dataset_file = f\"{folder}/{cfg['dataset']}_results.json\"\n",
    "    if os.path.exists(dataset_file):\n",
    "        with open(dataset_file, 'r') as f:\n",
    "            try:\n",
    "                all_data = json.load(f)\n",
    "            except json.JSONDecodeError:\n",
    "                all_data = []\n",
    "    else:\n",
    "        all_data = []\n",
    "\n",
    "\n",
    "    all_acc = []\n",
    "    all_mean_loss = []\n",
    "    all_loss_arrays = []\n",
    "    last_T = None\n",
    "    t_arr = []\n",
    "\n",
    "        # load data\n",
    "    Xtr, Str, Xts, Yts = load_npz(DATA_PATHS[cfg['dataset']])\n",
    "        # Transition matrix\n",
    "    T = None\n",
    "    train_loader, val_loader, is_cifar = make_loaders(Xtr, Str, batch_size=cfg['batch_size'], seed=0)\n",
    "   \n",
    "    if cfg['estimate_T']=='anchor':\n",
    "        T = torch.zeros((C, C), dtype=torch.float32, device=device)\n",
    "        T = estimate_transition_anchor(T, train_loader, is_cifar, q, device=device, epochs=cfg['est_epochs'])\n",
    "        sanity(T, 'Est T After Anchor point', cfg['dataset'])\n",
    "        \n",
    "        print(\"Estimated T:\")\n",
    "        print(T.cpu().numpy())\n",
    "    else:\n",
    "        T = pick_known_T(cfg['dataset'])\n",
    "        if T is None:\n",
    "            raise ValueError(\"Forward loss selected but no known T for this dataset; use --estimate_T.\")\n",
    "            \n",
    "\n",
    "    if isinstance(T, np.ndarray):\n",
    "        T = torch.tensor(T, dtype=torch.float32, device=device)\n",
    "\n",
    "    if T is not None:\n",
    "        if not torch.is_tensor(T):\n",
    "            T = torch.tensor(T, dtype=torch.float32, device=device)\n",
    "        else:\n",
    "            T = T.to(device=device, dtype=torch.float32)\n",
    "\n",
    "        # Ensure row-stochastic\n",
    "        T = torch.clamp(T, 1e-6, 1.0)\n",
    "        T = T / T.sum(dim=1, keepdim=True)\n",
    "\n",
    "        # Stabilize\n",
    "        T = stabilize_T(T, alpha=0.7)\n",
    "        \n",
    "    # get true T if known\n",
    "    T_true = pick_known_T(cfg['dataset'])\n",
    "    has_T_true = T_true is not None\n",
    "\n",
    "    for r in range(cfg['runs']):\n",
    "        acc, T, loss_arr = run_once(cfg, T, seed=1000+r)\n",
    "\n",
    "        all_acc.append(acc)\n",
    "        all_loss_arrays.append(loss_arr)\n",
    "        all_mean_loss.append(np.mean(loss_arr))\n",
    "        \n",
    "        if cfg['estimate_T'] or cfg['dataset'] == 'cifar':\n",
    "            t_arr.append(T)\n",
    "        last_T = T if T is not None else last_T\n",
    "\n",
    "    plot_run_performance(all_acc, all_mean_loss, cfg['loss'], cfg['dataset'])\n",
    "    mean_acc = float(np.mean(all_acc))\n",
    "    std_acc  = float(np.std(all_acc))\n",
    "    mean_loss = float(np.mean(all_mean_loss))\n",
    "    std_loss  = float(np.std(all_mean_loss))\n",
    "\n",
    "    # compute metrics vs true T (if available)\n",
    "    fro_err = mae_err = rre_err = None\n",
    "    if has_T_true and last_T is not None:\n",
    "        T_true = np.array(T_true)\n",
    "        T_est = np.array(last_T)\n",
    "        fro_err = float(np.linalg.norm(T_est - T_true, 'fro'))\n",
    "        mae_err = float(np.mean(np.abs(T_est - T_true)))\n",
    "        rre_err = float(fro_err / np.linalg.norm(T_true, 'fro'))\n",
    "\n",
    "    # build summary object\n",
    "    summary = {\n",
    "        'cfg': cfg,\n",
    "        'dataset': cfg['dataset'],\n",
    "        'loss': cfg['loss'],\n",
    "        'estimate_T': cfg['estimate_T'],\n",
    "        'epochs': cfg['epochs'],\n",
    "        'runs': cfg['runs'],\n",
    "        'mean_test_acc': mean_acc,\n",
    "        'std_test_acc': std_acc,\n",
    "        'mean_train_loss': mean_loss,\n",
    "        'std_train_loss': std_loss,\n",
    "        'per_run_acc': all_acc,\n",
    "        'per_run_mean_loss': all_mean_loss,\n",
    "        'per_run_loss_arrays': all_loss_arrays,\n",
    "        'last_estimated_T': last_T,\n",
    "        't_arr': t_arr,\n",
    "        'fro_error': fro_err,\n",
    "        'mae_error': mae_err,\n",
    "        'rre_error': rre_err\n",
    "    }\n",
    "\n",
    "    print(\"=\" * 72)\n",
    "    print(f\"{cfg['dataset']} | {cfg['loss']} | mean±std acc: {mean_acc*100:.2f}±{std_acc*100:.2f}% | mean loss: {mean_loss:.4f}\")\n",
    "\n",
    "    if fro_err is not None:\n",
    "        print(f\"T-error: Fro {fro_err:.4f}, MAE {mae_err:.4f}, RRE {rre_err:.4f}\")\n",
    "\n",
    "    all_data.append(summary)\n",
    "\n",
    "    with open(dataset_file, 'w') as f:\n",
    "        json.dump(all_data, f, indent=2)\n",
    "    print(f\"Appended results for {cfg['loss']} → {dataset_file}\")\n",
    "\n",
    "    # ➕ Add a summary row for TSV\n",
    "    results_rows.append({\n",
    "        'dataset': cfg['dataset'],\n",
    "        'loss': cfg['loss'],\n",
    "        'estimate_T': cfg['estimate_T'],\n",
    "        'epochs': cfg['epochs'],\n",
    "        'runs': cfg['runs'],\n",
    "        'mean_test_acc': round(mean_acc * 100, 2),\n",
    "        'std_test_acc': round(std_acc * 100, 2),\n",
    "        'mean_train_loss': round(mean_loss, 4),\n",
    "        'std_train_loss': round(std_loss, 4),\n",
    "        'fro_error': None if fro_err is None else round(fro_err, 4),\n",
    "        'mae_error': None if mae_err is None else round(mae_err, 4),\n",
    "        'rre_error': None if rre_err is None else round(rre_err, 4)\n",
    "    })\n",
    "\n",
    "# After each config loop, write TSV summary\n",
    "if results_rows:\n",
    "    df = pd.DataFrame(results_rows)\n",
    "    tsv_path = os.path.join(folder, \"summary_results.tsv\")\n",
    "    df.to_csv(tsv_path, sep='\\t', index=False)\n",
    "    print(f\"\\n✅ Wrote table summary to {tsv_path}\")\n",
    "    print(df.to_markdown(index=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Wrote table summary to results_2025-11-05-16_43/summary_results.tsv\n",
      "| dataset   | loss       | estimate_T   |   epochs |   runs |   mean_test_acc |   std_test_acc |   mean_train_loss |   std_train_loss |   fro_error |   mae_error |   rre_error |\n",
      "|:----------|:-----------|:-------------|---------:|-------:|----------------:|---------------:|------------------:|-----------------:|------------:|------------:|------------:|\n",
      "| cifar     | forward    | anchor       |        5 |      5 |           33.41 |           0.16 |            1.047  |           0.0444 |    nan      |    nan      |    nan      |\n",
      "| cifar     | gce        | anchor       |        5 |      5 |           33.53 |           1.91 |            0.9311 |           0.0039 |    nan      |    nan      |    nan      |\n",
      "| cifar     | forwardGCE | anchor       |        5 |      5 |           33.33 |           0    |            0.897  |           0.0342 |    nan      |    nan      |    nan      |\n",
      "| cifar     | trevision  | anchor       |        5 |      5 |           34.48 |           2.04 |            1.0681 |           0.0071 |    nan      |    nan      |    nan      |\n",
      "| fashion03 | forward    | anchor       |        5 |      5 |           61.61 |           4.38 |            0.9175 |           0.0079 |      0.04   |      0.0105 |      0.0304 |\n",
      "| fashion03 | forward    | None         |        5 |      5 |           62.59 |           2.08 |            0.9175 |           0.0076 |      0      |      0      |      0      |\n",
      "| fashion03 | gce        | anchor       |        5 |      5 |           87.08 |           6.97 |            0.6914 |           0.0087 |    nan      |    nan      |    nan      |\n",
      "| fashion03 | gce        | None         |        5 |      5 |           87.08 |           6.97 |            0.6914 |           0.0087 |    nan      |    nan      |    nan      |\n",
      "| fashion03 | forwardGCE | anchor       |        5 |      5 |           65.42 |           1.52 |            0.7946 |           0.0064 |      0.04   |      0.0105 |      0.0304 |\n",
      "| fashion03 | forwardGCE | None         |        5 |      5 |           64.99 |           1.91 |            0.7949 |           0.0059 |      0      |      0      |      0      |\n",
      "| fashion03 | trevision  | anchor       |        5 |      5 |           93.1  |           3.44 |            0.77   |           0.0086 |      0.04   |      0.0105 |      0.0304 |\n",
      "| fashion03 | trevision  | None         |        5 |      5 |           93.11 |           3.34 |            0.7698 |           0.009  |      0      |      0      |      0      |\n",
      "| fashion06 | forward    | anchor       |        5 |      5 |           49.17 |          10.51 |            1.0861 |           0.0035 |      0.0213 |      0.0053 |      0.0211 |\n",
      "| fashion06 | forward    | None         |        5 |      5 |           93.93 |           1.92 |            1.0931 |           0.0003 |      0      |      0      |      0      |\n",
      "| fashion06 | gce        | anchor       |        5 |      5 |           76.51 |          12.19 |            0.9311 |           0.0014 |    nan      |    nan      |    nan      |\n",
      "| fashion06 | gce        | None         |        5 |      5 |           76.51 |          12.19 |            0.9311 |           0.0014 |    nan      |    nan      |    nan      |\n",
      "| fashion06 | forwardGCE | anchor       |        5 |      5 |           52.73 |          11.52 |            0.926  |           0.0023 |      0.0213 |      0.0053 |      0.0211 |\n",
      "| fashion06 | forwardGCE | None         |        5 |      5 |           91.47 |           6.9  |            0.9312 |           0.0003 |      0      |      0      |      0      |\n",
      "| fashion06 | trevision  | anchor       |        5 |      5 |           92.61 |           3.99 |            1.059  |           0.0035 |      0.0213 |      0.0053 |      0.0211 |\n",
      "| fashion06 | trevision  | None         |        5 |      5 |           91.47 |           6.94 |            1.0583 |           0.0003 |      0      |      0      |      0      |\n"
     ]
    }
   ],
   "source": [
    "if results_rows:\n",
    "    df = pd.DataFrame(results_rows)\n",
    "    tsv_path = os.path.join(folder, \"summary_results.tsv\")\n",
    "    df.to_csv(tsv_path, sep='\\t', index=False)\n",
    "    print(f\"\\n✅ Wrote table summary to {tsv_path}\")\n",
    "    print(df.to_markdown(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error reading fashion03_results.json: 'list' object has no attribute 'get'\n",
      "Error reading fashion06_results.json: 'list' object has no attribute 'get'\n",
      "Error reading cifar_results.json: 'list' object has no attribute 'get'\n"
     ]
    }
   ],
   "source": [
    "for filename in os.listdir(folder):\n",
    "    if filename.endswith(\".json\"):\n",
    "        filepath = os.path.join(folder, filename)\n",
    "        try:\n",
    "            with open(filepath, \"r\", encoding=\"utf-8\") as f:\n",
    "                data = json.load(f)\n",
    "            \n",
    "            dataset = data.get(\"dataset\", \"N/A\")\n",
    "            loss = data.get(\"loss\", \"N/A\")\n",
    "            mean_acc = data.get(\"mean_test_acc\", None)\n",
    "            std_acc = data.get(\"std_test_acc\", None)\n",
    "\n",
    "            print(f\"{filename}:\")\n",
    "            \n",
    "            print(f\"  dataset       = {dataset}\")\n",
    "            print(f\"  loss          = {loss}\")\n",
    "            print(f\"  mean_test_acc = {mean_acc:.4f}\" if mean_acc is not None else \"  mean_test_acc = N/A\")\n",
    "            print(f\"  std_test_acc  = {std_acc:.4f}\" if std_acc is not None else \"  std_test_acc = N/A\")\n",
    "            print(\"-\" * 60)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error reading {filename}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "results_2025-11-05-16_43\n",
      "['results_2025-11-05-16_43/fashion03_results.json']\n",
      "Loading: results_2025-11-05-16_43/fashion03_results.json\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "list indices must be integers or slices, not str",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[107]\u001b[39m\u001b[32m, line 19\u001b[39m\n\u001b[32m     16\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(first_file, \u001b[33m\"\u001b[39m\u001b[33mr\u001b[39m\u001b[33m\"\u001b[39m, encoding=\u001b[33m\"\u001b[39m\u001b[33mutf-8\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[32m     17\u001b[39m     data = json.load(f)\n\u001b[32m---> \u001b[39m\u001b[32m19\u001b[39m T_prime = np.array(\u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mlast_estimated_T\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m)\n\u001b[32m     20\u001b[39m T_true = pick_known_T(\u001b[33m'\u001b[39m\u001b[33mfashion03\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m     22\u001b[39m \u001b[38;5;66;03m#checking recreation performance\u001b[39;00m\n",
      "\u001b[31mTypeError\u001b[39m: list indices must be integers or slices, not str"
     ]
    }
   ],
   "source": [
    "C = 3\n",
    "files = os.listdir()\n",
    "files = sorted(glob.glob('results*'), reverse=True)\n",
    "folder = files[0]\n",
    "print(folder)\n",
    "\n",
    "# pattern for files starting with \"name\" and ending with \".json\"\n",
    "files = sorted(glob.glob(os.path.join(folder,\"fashion03*.json\")))\n",
    "print(files)\n",
    "\n",
    "# pick the first matching file\n",
    "first_file = files[0]\n",
    "print(\"Loading:\", first_file)\n",
    "\n",
    "# load the JSON contents\n",
    "with open(first_file, \"r\", encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "T_prime = np.array(data['last_estimated_T'])\n",
    "T_true = pick_known_T('fashion03')\n",
    "\n",
    "#checking recreation performance\n",
    "print(T_prime)\n",
    "print(T_true)\n",
    "print(f\"Fro error: {np.linalg.norm(T_prime - T_true, 'fro')}\")\n",
    "print(f\"rre error: {np.linalg.norm(T_prime - T_true, 'fro') / np.linalg.norm(T_true, 'fro')}\")\n",
    "print(f\"mae error: {np.mean(np.abs(T_prime - T_true))}\")\n",
    "\n",
    "\n",
    "corrs = [st.pearsonr(T_true[i], T_prime[i])[0] for i in range(C)]\n",
    "print(\"Per-row correlations:\", corrs)\n",
    "print(\"Mean:\", np.mean(corrs))\n",
    "\n",
    "\n",
    "# pattern for files starting with \"name\" and ending with \".json\"\n",
    "files = sorted(f for f in glob.glob(os.path.join(folder, \"fashion06*.json\")) if \"None\" not in f)\n",
    "\n",
    "\n",
    "# pick the first matching file\n",
    "first_file = files[0]\n",
    "print(\"\\nLoading:\", first_file)\n",
    "# load the JSON contents\n",
    "with open(first_file, \"r\", encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)\n",
    "print()\n",
    "T_prime = np.array(data['last_estimated_T'])\n",
    "\n",
    "T_true = pick_known_T('fashion06')\n",
    "\n",
    "#checking recreation performance\n",
    "print(T_prime)\n",
    "print(T_true)\n",
    "print(f\"Fro error: {np.linalg.norm(T_prime - T_true, 'fro')}\")\n",
    "print(f\"rre error: {np.linalg.norm(T_prime - T_true, 'fro') / np.linalg.norm(T_true, 'fro')}\")\n",
    "print(f\"mae error: {np.mean(np.abs(T_prime - T_true))}\")\n",
    "\n",
    "\n",
    "corrs = [st.pearsonr(T_true[i], T_prime[i])[0] for i in range(C)]\n",
    "print(\"Per-row correlations:\", corrs)\n",
    "print(\"Mean:\", np.mean(corrs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating 18 files for dataset: fashion03\n",
      "\n",
      "File: fashion03_forwardGCE_anchor_0.6_2025-11-04-09_11.json\n",
      "Fro error: 0.726604\n",
      "RRE error: 0.550837\n",
      "MAE error: 0.200319\n",
      "Per-row correlations: [0.6207 0.6522 0.6338] | Mean: 0.6356\n",
      "------------------------------------------------------------\n",
      "T_prime:\n",
      "[[0.7  0.   0.31]\n",
      " [0.29 0.69 0.  ]\n",
      " [0.02 0.31 0.69]]\n",
      "T_true:\n",
      "[[0.7 0.3 0. ]\n",
      " [0.  0.7 0.3]\n",
      " [0.3 0.  0.7]]\n",
      "T_true - T_prime:\n",
      "[[ 0.    0.3  -0.31]\n",
      " [-0.29  0.01  0.3 ]\n",
      " [ 0.28 -0.31  0.01]]\n",
      "File: fashion03_forwardGCE_anchor_0.8_2025-11-04-09_11.json\n",
      "Fro error: 0.730517\n",
      "RRE error: 0.553804\n",
      "MAE error: 0.201805\n",
      "Per-row correlations: [0.6207 0.6472 0.626 ] | Mean: 0.6313\n",
      "------------------------------------------------------------\n",
      "T_prime:\n",
      "[[0.7  0.   0.31]\n",
      " [0.29 0.69 0.  ]\n",
      " [0.02 0.31 0.69]]\n",
      "T_true:\n",
      "[[0.7 0.3 0. ]\n",
      " [0.  0.7 0.3]\n",
      " [0.3 0.  0.7]]\n",
      "T_true - T_prime:\n",
      "[[ 0.    0.3  -0.31]\n",
      " [-0.29  0.01  0.3 ]\n",
      " [ 0.28 -0.31  0.01]]\n",
      "File: fashion03_forwardGCE_anchor_0.9_2025-11-04-09_11.json\n",
      "Fro error: 0.731620\n",
      "RRE error: 0.554640\n",
      "MAE error: 0.202140\n",
      "Per-row correlations: [0.6209 0.6467 0.6232] | Mean: 0.6303\n",
      "------------------------------------------------------------\n",
      "T_prime:\n",
      "[[0.7  0.   0.31]\n",
      " [0.29 0.69 0.01]\n",
      " [0.01 0.31 0.68]]\n",
      "T_true:\n",
      "[[0.7 0.3 0. ]\n",
      " [0.  0.7 0.3]\n",
      " [0.3 0.  0.7]]\n",
      "T_true - T_prime:\n",
      "[[ 0.    0.3  -0.31]\n",
      " [-0.29  0.01  0.29]\n",
      " [ 0.29 -0.31  0.02]]\n",
      "File: fashion03_forwardGCE_trevision_0.6_2025-11-04-09_11.json\n",
      "Fro error: 0.000000\n",
      "RRE error: 0.000000\n",
      "MAE error: 0.000000\n",
      "Per-row correlations: [1. 1. 1.] | Mean: 1.0000\n",
      "------------------------------------------------------------\n",
      "T_prime:\n",
      "[[0.7 0.3 0. ]\n",
      " [0.  0.7 0.3]\n",
      " [0.3 0.  0.7]]\n",
      "T_true:\n",
      "[[0.7 0.3 0. ]\n",
      " [0.  0.7 0.3]\n",
      " [0.3 0.  0.7]]\n",
      "T_true - T_prime:\n",
      "[[0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]]\n",
      "File: fashion03_forwardGCE_trevision_0.8_2025-11-04-09_11.json\n",
      "Fro error: 0.000000\n",
      "RRE error: 0.000000\n",
      "MAE error: 0.000000\n",
      "Per-row correlations: [1. 1. 1.] | Mean: 1.0000\n",
      "------------------------------------------------------------\n",
      "T_prime:\n",
      "[[0.7 0.3 0. ]\n",
      " [0.  0.7 0.3]\n",
      " [0.3 0.  0.7]]\n",
      "T_true:\n",
      "[[0.7 0.3 0. ]\n",
      " [0.  0.7 0.3]\n",
      " [0.3 0.  0.7]]\n",
      "T_true - T_prime:\n",
      "[[0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]]\n",
      "File: fashion03_forwardGCE_trevision_0.9_2025-11-04-09_11.json\n",
      "Fro error: 0.000000\n",
      "RRE error: 0.000000\n",
      "MAE error: 0.000000\n",
      "Per-row correlations: [1. 1. 1.] | Mean: 1.0000\n",
      "------------------------------------------------------------\n",
      "T_prime:\n",
      "[[0.7 0.3 0. ]\n",
      " [0.  0.7 0.3]\n",
      " [0.3 0.  0.7]]\n",
      "T_true:\n",
      "[[0.7 0.3 0. ]\n",
      " [0.  0.7 0.3]\n",
      " [0.3 0.  0.7]]\n",
      "T_true - T_prime:\n",
      "[[0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]]\n",
      "File: fashion03_forward_anchor_0.6_2025-11-04-09_11.json\n",
      "Fro error: 0.726604\n",
      "RRE error: 0.550837\n",
      "MAE error: 0.200319\n",
      "Per-row correlations: [0.6207 0.6522 0.6338] | Mean: 0.6356\n",
      "------------------------------------------------------------\n",
      "T_prime:\n",
      "[[0.7  0.   0.31]\n",
      " [0.29 0.69 0.  ]\n",
      " [0.02 0.31 0.69]]\n",
      "T_true:\n",
      "[[0.7 0.3 0. ]\n",
      " [0.  0.7 0.3]\n",
      " [0.3 0.  0.7]]\n",
      "T_true - T_prime:\n",
      "[[ 0.    0.3  -0.31]\n",
      " [-0.29  0.01  0.3 ]\n",
      " [ 0.28 -0.31  0.01]]\n",
      "File: fashion03_forward_anchor_0.8_2025-11-04-09_11.json\n",
      "Fro error: 0.730517\n",
      "RRE error: 0.553804\n",
      "MAE error: 0.201805\n",
      "Per-row correlations: [0.6207 0.6472 0.626 ] | Mean: 0.6313\n",
      "------------------------------------------------------------\n",
      "T_prime:\n",
      "[[0.7  0.   0.31]\n",
      " [0.29 0.69 0.  ]\n",
      " [0.02 0.31 0.69]]\n",
      "T_true:\n",
      "[[0.7 0.3 0. ]\n",
      " [0.  0.7 0.3]\n",
      " [0.3 0.  0.7]]\n",
      "T_true - T_prime:\n",
      "[[ 0.    0.3  -0.31]\n",
      " [-0.29  0.01  0.3 ]\n",
      " [ 0.28 -0.31  0.01]]\n",
      "File: fashion03_forward_anchor_0.9_2025-11-04-09_11.json\n",
      "Fro error: 0.731620\n",
      "RRE error: 0.554640\n",
      "MAE error: 0.202140\n",
      "Per-row correlations: [0.6209 0.6467 0.6232] | Mean: 0.6303\n",
      "------------------------------------------------------------\n",
      "T_prime:\n",
      "[[0.7  0.   0.31]\n",
      " [0.29 0.69 0.01]\n",
      " [0.01 0.31 0.68]]\n",
      "T_true:\n",
      "[[0.7 0.3 0. ]\n",
      " [0.  0.7 0.3]\n",
      " [0.3 0.  0.7]]\n",
      "T_true - T_prime:\n",
      "[[ 0.    0.3  -0.31]\n",
      " [-0.29  0.01  0.29]\n",
      " [ 0.29 -0.31  0.02]]\n",
      "File: fashion03_forward_trevision_0.6_2025-11-04-09_11.json\n",
      "Fro error: 0.000000\n",
      "RRE error: 0.000000\n",
      "MAE error: 0.000000\n",
      "Per-row correlations: [1. 1. 1.] | Mean: 1.0000\n",
      "------------------------------------------------------------\n",
      "T_prime:\n",
      "[[0.7 0.3 0. ]\n",
      " [0.  0.7 0.3]\n",
      " [0.3 0.  0.7]]\n",
      "T_true:\n",
      "[[0.7 0.3 0. ]\n",
      " [0.  0.7 0.3]\n",
      " [0.3 0.  0.7]]\n",
      "T_true - T_prime:\n",
      "[[0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]]\n",
      "File: fashion03_forward_trevision_0.8_2025-11-04-09_11.json\n",
      "Fro error: 0.000000\n",
      "RRE error: 0.000000\n",
      "MAE error: 0.000000\n",
      "Per-row correlations: [1. 1. 1.] | Mean: 1.0000\n",
      "------------------------------------------------------------\n",
      "T_prime:\n",
      "[[0.7 0.3 0. ]\n",
      " [0.  0.7 0.3]\n",
      " [0.3 0.  0.7]]\n",
      "T_true:\n",
      "[[0.7 0.3 0. ]\n",
      " [0.  0.7 0.3]\n",
      " [0.3 0.  0.7]]\n",
      "T_true - T_prime:\n",
      "[[0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]]\n",
      "File: fashion03_forward_trevision_0.9_2025-11-04-09_11.json\n",
      "Fro error: 0.000000\n",
      "RRE error: 0.000000\n",
      "MAE error: 0.000000\n",
      "Per-row correlations: [1. 1. 1.] | Mean: 1.0000\n",
      "------------------------------------------------------------\n",
      "T_prime:\n",
      "[[0.7 0.3 0. ]\n",
      " [0.  0.7 0.3]\n",
      " [0.3 0.  0.7]]\n",
      "T_true:\n",
      "[[0.7 0.3 0. ]\n",
      " [0.  0.7 0.3]\n",
      " [0.3 0.  0.7]]\n",
      "T_true - T_prime:\n",
      "[[0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]]\n",
      "File: fashion03_gce_anchor_0.6_2025-11-04-09_11.json\n",
      "Fro error: 0.726604\n",
      "RRE error: 0.550837\n",
      "MAE error: 0.200319\n",
      "Per-row correlations: [0.6207 0.6522 0.6338] | Mean: 0.6356\n",
      "------------------------------------------------------------\n",
      "T_prime:\n",
      "[[0.7  0.   0.31]\n",
      " [0.29 0.69 0.  ]\n",
      " [0.02 0.31 0.69]]\n",
      "T_true:\n",
      "[[0.7 0.3 0. ]\n",
      " [0.  0.7 0.3]\n",
      " [0.3 0.  0.7]]\n",
      "T_true - T_prime:\n",
      "[[ 0.    0.3  -0.31]\n",
      " [-0.29  0.01  0.3 ]\n",
      " [ 0.28 -0.31  0.01]]\n",
      "File: fashion03_gce_anchor_0.8_2025-11-04-09_11.json\n",
      "Fro error: 0.730517\n",
      "RRE error: 0.553804\n",
      "MAE error: 0.201805\n",
      "Per-row correlations: [0.6207 0.6472 0.626 ] | Mean: 0.6313\n",
      "------------------------------------------------------------\n",
      "T_prime:\n",
      "[[0.7  0.   0.31]\n",
      " [0.29 0.69 0.  ]\n",
      " [0.02 0.31 0.69]]\n",
      "T_true:\n",
      "[[0.7 0.3 0. ]\n",
      " [0.  0.7 0.3]\n",
      " [0.3 0.  0.7]]\n",
      "T_true - T_prime:\n",
      "[[ 0.    0.3  -0.31]\n",
      " [-0.29  0.01  0.3 ]\n",
      " [ 0.28 -0.31  0.01]]\n",
      "File: fashion03_gce_anchor_0.9_2025-11-04-09_11.json\n",
      "Fro error: 0.731620\n",
      "RRE error: 0.554640\n",
      "MAE error: 0.202140\n",
      "Per-row correlations: [0.6209 0.6467 0.6232] | Mean: 0.6303\n",
      "------------------------------------------------------------\n",
      "T_prime:\n",
      "[[0.7  0.   0.31]\n",
      " [0.29 0.69 0.01]\n",
      " [0.01 0.31 0.68]]\n",
      "T_true:\n",
      "[[0.7 0.3 0. ]\n",
      " [0.  0.7 0.3]\n",
      " [0.3 0.  0.7]]\n",
      "T_true - T_prime:\n",
      "[[ 0.    0.3  -0.31]\n",
      " [-0.29  0.01  0.29]\n",
      " [ 0.29 -0.31  0.02]]\n",
      "File: fashion03_gce_trevision_0.6_2025-11-04-09_11.json\n",
      "Fro error: 0.000000\n",
      "RRE error: 0.000000\n",
      "MAE error: 0.000000\n",
      "Per-row correlations: [1. 1. 1.] | Mean: 1.0000\n",
      "------------------------------------------------------------\n",
      "T_prime:\n",
      "[[0.7 0.3 0. ]\n",
      " [0.  0.7 0.3]\n",
      " [0.3 0.  0.7]]\n",
      "T_true:\n",
      "[[0.7 0.3 0. ]\n",
      " [0.  0.7 0.3]\n",
      " [0.3 0.  0.7]]\n",
      "T_true - T_prime:\n",
      "[[0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]]\n",
      "File: fashion03_gce_trevision_0.8_2025-11-04-09_11.json\n",
      "Fro error: 0.000000\n",
      "RRE error: 0.000000\n",
      "MAE error: 0.000000\n",
      "Per-row correlations: [1. 1. 1.] | Mean: 1.0000\n",
      "------------------------------------------------------------\n",
      "T_prime:\n",
      "[[0.7 0.3 0. ]\n",
      " [0.  0.7 0.3]\n",
      " [0.3 0.  0.7]]\n",
      "T_true:\n",
      "[[0.7 0.3 0. ]\n",
      " [0.  0.7 0.3]\n",
      " [0.3 0.  0.7]]\n",
      "T_true - T_prime:\n",
      "[[0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]]\n",
      "File: fashion03_gce_trevision_0.9_2025-11-04-09_11.json\n",
      "Fro error: 0.000000\n",
      "RRE error: 0.000000\n",
      "MAE error: 0.000000\n",
      "Per-row correlations: [1. 1. 1.] | Mean: 1.0000\n",
      "------------------------------------------------------------\n",
      "T_prime:\n",
      "[[0.7 0.3 0. ]\n",
      " [0.  0.7 0.3]\n",
      " [0.3 0.  0.7]]\n",
      "T_true:\n",
      "[[0.7 0.3 0. ]\n",
      " [0.  0.7 0.3]\n",
      " [0.3 0.  0.7]]\n",
      "T_true - T_prime:\n",
      "[[0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]]\n",
      "\n",
      "✅ Best result: fashion03_forwardGCE_trevision_0.6_2025-11-04-09_11.json\n",
      "   Fro error: 0.000000, RRE: 0.000000, MAE: 0.000000, Corr: 1.0000\n",
      "No files found matching: fashion06*True*.json\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import json\n",
    "import numpy as np\n",
    "from scipy import stats as st\n",
    "\n",
    "def evaluate_T_matrices(folder, pattern, dataset_name, C=3):\n",
    "    \"\"\"\n",
    "    Loops over all JSON files matching pattern, computes error metrics \n",
    "    and correlations vs. known T, and returns best-performing result.\n",
    "    \"\"\"\n",
    "    files = sorted(glob.glob(os.path.join(folder, pattern)))\n",
    "    if not files:\n",
    "        print(f\"No files found matching: {pattern}\")\n",
    "        return None, None\n",
    "    \n",
    "    T_true = pick_known_T(dataset_name)\n",
    "    results = []\n",
    "\n",
    "    print(f\"Evaluating {len(files)} files for dataset: {dataset_name}\\n\")\n",
    "\n",
    "    for file in files:\n",
    "        with open(file, \"r\", encoding=\"utf-8\") as f:\n",
    "            data = json.load(f)\n",
    "        T_prime = np.array(data['last_estimated_T'])\n",
    "        \n",
    "        fro_err = np.linalg.norm(T_prime - T_true, 'fro')\n",
    "        rre_err = fro_err / np.linalg.norm(T_true, 'fro')\n",
    "        mae_err = np.mean(np.abs(T_prime - T_true))\n",
    "        corrs = [st.pearsonr(T_true[i], T_prime[i])[0] for i in range(C)]\n",
    "        mean_corr = np.mean(corrs)\n",
    "        \n",
    "        print(f\"File: {os.path.basename(file)}\")\n",
    "        print(f\"Fro error: {fro_err:.6f}\")\n",
    "        print(f\"RRE error: {rre_err:.6f}\")\n",
    "        print(f\"MAE error: {mae_err:.6f}\")\n",
    "        print(f\"Per-row correlations: {np.round(corrs, 4)} | Mean: {mean_corr:.4f}\")\n",
    "        print(\"-\" * 60)\n",
    "\n",
    "        #checking recreation performance\n",
    "        print(\"T_prime:\")\n",
    "        print(np.round(T_prime, 2))\n",
    "        print(\"T_true:\")\n",
    "        print(np.round(T_true, 2))\n",
    "        print(\"T_true - T_prime:\")\n",
    "        print(np.round(T_true - T_prime, 2))\n",
    "        \n",
    "        results.append({\n",
    "            'file': file,\n",
    "            'T_prime': T_prime,\n",
    "            'fro': fro_err,\n",
    "            'rre': rre_err,\n",
    "            'mae': mae_err,\n",
    "            'corr_mean': mean_corr\n",
    "        })\n",
    "    \n",
    "    # Choose the best result (lowest Frobenius error)\n",
    "    best = min(results, key=lambda x: x['fro'])\n",
    "    print(f\"\\n✅ Best result: {os.path.basename(best['file'])}\")\n",
    "    print(f\"   Fro error: {best['fro']:.6f}, RRE: {best['rre']:.6f}, MAE: {best['mae']:.6f}, Corr: {best['corr_mean']:.4f}\")\n",
    "    \n",
    "    return best['file'], best['T_prime']\n",
    "\n",
    "\n",
    "# Example usage\n",
    "\n",
    "best_file_03, best_T_03 = evaluate_T_matrices(folder, \"fashion03*.json\", \"fashion03\", C=3)\n",
    "best_file_06, best_T_06 = evaluate_T_matrices(folder, \"fashion06*True*.json\", \"fashion06\", C=3)\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
