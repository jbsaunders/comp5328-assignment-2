{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9IrXtNd8l-9H"
   },
   "source": [
    "## COMP5328 - Advanced Machine Learning\n",
    "## Assignment 2: Title\n",
    "----------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup\n",
    "\n",
    "### 1.1 Library Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Common imports\n",
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import json\n",
    "import time\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import tabulate\n",
    "import random\n",
    "import re\n",
    "\n",
    "# Ploting\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import os\n",
    "import torch.optim as optim\n",
    "import random\n",
    "import scipy.stats as st"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Notebook Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Torch device setup - to use gpu if available else cpu\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "    print(\"Apple MPS:\", device)\n",
    "elif torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"NVIDIA CUDA:\", device)\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"No GPU available, Using CPU:\", device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Checking data folder has data files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The structure of data folder.\n",
    "!ls -l data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Common variable and function setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment variables\n",
    "# Common\n",
    "num_classes=3\n",
    "dataset_folder = 'data/'\n",
    "\n",
    "cifar_dataset = dataset_folder+'CIFAR.npz'\n",
    "MNISTO3_dataset = dataset_folder+'FashionMNIST0.3.npz'\n",
    "MNISTO6_dataset = dataset_folder+'FashionMNIST0.6.npz'\n",
    "\n",
    "\n",
    "DATA_PATHS = {\n",
    "    'fashion03': MNISTO3_dataset,\n",
    "    'fashion06': MNISTO6_dataset,\n",
    "    'cifar':     cifar_dataset\n",
    "}\n",
    "\n",
    "losses = ['forward','gce', 'forwardGCE', 't-revision']\n",
    "losses = ['forward', 'forwardGCE', 't-revision']\n",
    "datasets = [ \"fashion03\", \"fashion06\", \"cifar\"]\n",
    "base = {\n",
    "    \"runs\":10,\n",
    "    \"epochs\": 15,\n",
    "    \"batch_size\":512,\n",
    "    \"q\":0.6,\n",
    "    \"est_epochs\":25,\n",
    "    \"beta\":5e-4,\n",
    "    \"lr\": 1e-4,\n",
    "    \"device\":str(device)\n",
    "}\n",
    "\n",
    "known_T_fashion_03 = np.array(  [[0.7,0.3,0.0],\n",
    "                                [0.0,0.7,0.3],\n",
    "                                [0.3,0.0,0.7]], dtype=np.float32)\n",
    "\n",
    "known_T_fashion_06 = np.array(  [[0.4,0.3,0.3],\n",
    "                                [0.3,0.4,0.3],\n",
    "                                [0.3,0.3,0.4]], dtype=np.float32)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define config\n",
    "now = datetime.now()\n",
    "now = now.strftime(\"%Y-%m-%d-%H_%M\")\n",
    "folder = \"results_\"+now\n",
    "\n",
    "if os.path.exists(folder) and os.path.isdir(folder):\n",
    "    os.rmdir(folder)\n",
    "    os.mkdir(folder)\n",
    "else:\n",
    "    os.mkdir(folder)\n",
    "\n",
    "# create each cfg\n",
    "estimate = ['anchor', 'None']\n",
    "qs = [0.6]\n",
    "configs = []\n",
    "for q in qs:\n",
    "    for i, ds in enumerate(datasets):\n",
    "        if ds == 'cifar':\n",
    "            is_cifar = True\n",
    "        else:\n",
    "            is_cifar = False\n",
    "        \n",
    "        for loss in losses:\n",
    "            for t in estimate:\n",
    "                cfg = {**base, \"dataset\": ds, \"loss\":loss, \"estimate_T\":t, \"q\":q, \"is_cifar\":is_cifar}\n",
    "                if t=='None' and ds=='cifar':\n",
    "                    print(f\"Skipping dataset without Known Transition Matrix: {ds}\")\n",
    "                else:\n",
    "                    print(f\"dataset: {ds}, estimate_T:{t}, loss:{loss}, q:{q}\")\n",
    "                    configs.append(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Common Functions\n",
    "def load_npz(path):\n",
    "    d = np.load(path)\n",
    "    Xtr, Str = d['Xtr'], d['Str']\n",
    "    Xts, Yts = d['Xts'], d['Yts']\n",
    "    return Xtr, Str, Xts, Yts\n",
    "\n",
    "def pick_known_T(tag):\n",
    "    if tag == 'fashion03':\n",
    "        return known_T_fashion_03\n",
    "    elif tag == 'fashion06':\n",
    "        return known_T_fashion_06\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def set_seed(seed=0):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    # Cuda specific seeds\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "       \n",
    "def stabilize_T(T, alpha=0.7):\n",
    "    \"\"\"\n",
    "    Blend estimated T with identity to avoid near-singular/over-uniform matrices.\n",
    "    Keeps rows summing to 1.\n",
    "    \"\"\"\n",
    "    C = T.size(0)\n",
    "    I = torch.eye(C, device=T.device, dtype=T.dtype)\n",
    "    T = alpha * T + (1 - alpha) * I\n",
    "    T = torch.clamp(T, 1e-3, 0.999)\n",
    "    T = T / T.sum(dim=1, keepdim=True)\n",
    "    return T\n",
    "\n",
    "def plot_run_performance(all_acc, all_mean_loss, cfg, folder):\n",
    "    runs = np.arange(1, len(all_acc) + 1)\n",
    "    display_title = f\"{cfg['dataset'].upper()} — loss:{cfg['loss']} — t_esimation:{cfg['estimate_T']} — q:{cfg['q']} Performance Across Runs\"\n",
    "    file_name = f\"{cfg['dataset'].upper()}—loss={cfg['loss']}—t_esimation={cfg['estimate_T']}—q={cfg['q']}-Performance_Across_Runs\"\n",
    "    fig, ax1 = plt.subplots(figsize=(8,5))\n",
    "    ax2 = ax1.twinx()\n",
    "    # Accuracy (left axis)\n",
    "    ax1.plot(runs, np.array(all_acc)*100, 'o-', color='tab:blue', label='Test Accuracy (%)')\n",
    "    ax1.set_xlabel('Run')\n",
    "    ax1.set_ylabel('Accuracy (%)', color='tab:blue')\n",
    "    ax1.tick_params(axis='y', labelcolor='tab:blue')\n",
    "    ax1.set_ylim(max((min(all_acc)*100)-10, 0), 100)\n",
    "\n",
    "    # Loss (right axis)\n",
    "    ax2.plot(runs, all_mean_loss, 's--', color='tab:red', label='Mean Train Loss')\n",
    "    ax2.set_ylabel('Loss', color='tab:red')\n",
    "    ax2.tick_params(axis='y', labelcolor='tab:red')\n",
    "\n",
    "    plt.title(display_title)\n",
    "    fig.tight_layout()\n",
    "\n",
    "    # combine legends from both axes\n",
    "    lines, labels = ax1.get_legend_handles_labels()\n",
    "    lines2, labels2 = ax2.get_legend_handles_labels()\n",
    "    ax1.legend(lines + lines2, labels + labels2, loc='best')\n",
    "\n",
    "    # Ensures img folder exists before saving\n",
    "    os.makedirs(os.path.join(folder, \"img\"), exist_ok=True)\n",
    "    save_path = os.path.join(folder, \"img\", f\"{file_name}.png\")\n",
    "    plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "    print(f\"Saved plot to: {save_path}\")\n",
    "\n",
    "    plt.show()\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qYzWuiytl-9I"
   },
   "source": [
    "## 1. Load Dataset\n",
    "\n",
    "### 1.0 Data Folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NpzDataset(Dataset):\n",
    "    def __init__(self, X, y, is_cifar=False):\n",
    "        self.X = X.astype(np.float32)\n",
    "        self.y = y.astype(np.int64)\n",
    "        self.is_cifar = is_cifar\n",
    "\n",
    "        # Normalize to [0,1]\n",
    "        self.X = self.X / 255.0 if self.X.max() > 1.0 else self.X\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x = self.X[idx]\n",
    "        if x.ndim == 1:\n",
    "            # flat; try to infer shape 28x28 or 32x32x3\n",
    "            if x.size == 28*28:\n",
    "                x = x.reshape(1, 28, 28)\n",
    "            elif x.size == 32*32*3:\n",
    "                x = x.reshape(3, 32, 32)\n",
    "            else:\n",
    "                raise ValueError(\"Unknown flat image shape: {}\".format(x.shape))\n",
    "        else:\n",
    "            # (H,W) or (H,W,C)\n",
    "            if x.ndim == 2:\n",
    "                x = x[None, ...]  # to (1,H,W)\n",
    "            elif x.ndim == 3:\n",
    "                # assume HWC -> CHW\n",
    "                x = np.transpose(x, (2, 0, 1))\n",
    "            else:\n",
    "                raise ValueError(f\"Unexpected image dims: {x.shape}\")\n",
    "        return torch.from_numpy(x), torch.tensor(self.y[idx])\n",
    "\n",
    "\n",
    "def make_loaders(Xtr, Str, batch_size=128, seed=0, test_size=0.2):\n",
    "    # 80/20 split each repetition\n",
    "    X_tr, X_val, y_tr, y_val = train_test_split(\n",
    "        Xtr, Str, test_size=test_size, random_state=seed, stratify=Str\n",
    "    )\n",
    "\n",
    "    is_cifar = (X_tr.shape[-1] == 3) if X_tr.ndim == 4 else (X_tr.shape[-1] == 32*32*3)\n",
    "\n",
    "    train_ds = NpzDataset(X_tr, y_tr, is_cifar)\n",
    "    val_ds   = NpzDataset(X_val, y_val, is_cifar)\n",
    "\n",
    "    train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "    val_loader   = DataLoader(val_ds,   batch_size=batch_size, shuffle=False, num_workers=0)\n",
    "\n",
    "    return train_loader, val_loader, is_cifar\n",
    "\n",
    "def make_test_loader(Xts, Yts, batch_size=256):\n",
    "    is_cifar = (Xts.shape[-1] == 3) if Xts.ndim == 4 else (Xts.shape[-1] == 32*32*3)\n",
    "    test_ds = NpzDataset(Xts, Yts, is_cifar)\n",
    "    return DataLoader(test_ds, batch_size=batch_size, shuffle=False, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ForwardCorrectedCE(nn.Module):\n",
    "    \"\"\"\n",
    "    Forward loss correction: minimizes CE between T^T p and noisy labels.\n",
    "    T: class-transition matrix where T[i,j] = P(S=j | Y=i). Shape [C,C].\n",
    "    \"\"\"\n",
    "    def __init__(self, T):\n",
    "        super().__init__()\n",
    "        self.register_buffer('T', T)  # [C,C]\n",
    "\n",
    "    def forward(self, logits, y_noisy):\n",
    "        # logits -> p(y|x)\n",
    "        p = F.softmax(logits, dim=1)  # [B,C]\n",
    "        # mix via T^T\n",
    "        mixed = torch.clamp(p @ self.T, 1e-6, 1.0)\n",
    "        log_mixed = torch.log(mixed)\n",
    "        return F.nll_loss(log_mixed, y_noisy) \n",
    "    \n",
    "class TRevisionLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    Joint T-revision loss: learns transition matrix T along with model weights.\n",
    "    Combines forward correction with a regularization toward identity (or prior).\n",
    "    \"\"\"\n",
    "    def __init__(self, device, C=3, q=0.7, lambda_reg=1e-1, T0=None):\n",
    "        super().__init__()\n",
    "        self.q = q\n",
    "        self.lambda_reg = lambda_reg\n",
    "        eps = 1e-6\n",
    "        # ------------------------------\n",
    "        # Initialize T0\n",
    "        # ------------------------------\n",
    "        if T0 is None:\n",
    "            # Start from identity (clean labels)\n",
    "            T0 = torch.eye(C, device=device, dtype=torch.float32)\n",
    "        else:\n",
    "            # Safely handle both numpy arrays and tensors\n",
    "            T0 = torch.as_tensor(T0, dtype=torch.float32, device=device)\n",
    "            # Ensure it's detached and cloned to avoid warnings\n",
    "            T0 = T0.detach().clone()\n",
    "\n",
    "        # Clamp to ensure positivity and numerical stability\n",
    "        T0 = torch.clamp(T0, eps, 1.0)\n",
    "\n",
    "        # ------------------------------\n",
    "        # Logits parameterization\n",
    "        # ------------------------------\n",
    "        # Learn unnormalized log-values; softmax over columns later gives valid T\n",
    "        self.logits = nn.Parameter(torch.log(T0))\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, logits, y):\n",
    "        \"\"\"\n",
    "        logits: model outputs (B, C)\n",
    "        y: noisy labels (B,)\n",
    "        \"\"\"\n",
    "        C = logits.size(1)\n",
    "        T = torch.softmax(self.logits, dim=1)  # ensure column-stochastic\n",
    "        p = F.softmax(logits, dim=1)\n",
    "        p_noisy = torch.clamp(p @ T, 1e-6, 1.0)\n",
    "        p_s = p_noisy.gather(1, y.view(-1, 1)).clamp(1e-6, 1.0)\n",
    "\n",
    "        if self.q == 1.0:\n",
    "            loss = -torch.log(p_s)\n",
    "        else:\n",
    "            loss = (1 - p_s.pow(self.q)) / self.q\n",
    "        loss = loss.mean()\n",
    "\n",
    "        # regularize T toward identity or prior\n",
    "        T_prior = torch.eye(C, device=T.device)\n",
    "        reg = self.lambda_reg * torch.norm(T - T_prior, p='fro')\n",
    "        return loss + reg\n",
    "\n",
    "class ForwardCorrectedGCE(nn.Module):\n",
    "    def __init__(self, T, q):\n",
    "        super().__init__()\n",
    "        self.register_buffer('T', T)\n",
    "        self.q = q\n",
    "    def forward(self, logits, y_noisy):\n",
    "        p_noisy = torch.clamp(F.softmax(logits,1) @ self.T, 1e-6, 1.0)\n",
    "        p_s = p_noisy.gather(1, y_noisy.view(-1,1)).clamp(1e-6,1.0)\n",
    "        return (-(p_s.log()) if self.q==1.0 else (1 - p_s**self.q)/self.q).mean()\n",
    "    \n",
    "class GeneralizedCrossEntropy(nn.Module):\n",
    "    \"\"\"\n",
    "    GCE loss: L_q(p, y) = (1 - p_y^q) / q, with q in (0,1].\n",
    "    q→1 recovers CE; smaller q is more robust to label noise.\n",
    "    \"\"\"\n",
    "    def __init__(self, q):\n",
    "        super().__init__()\n",
    "        assert 0 < q <= 1\n",
    "        self.q = q\n",
    "\n",
    "    def forward(self, logits, y):\n",
    "        p = F.softmax(logits, dim=1)\n",
    "        p_y = p.gather(1, y.view(-1,1)).clamp(min=1e-6, max=1.0)\n",
    "        if self.q == 1.0:\n",
    "            return -torch.log(p_y).mean()\n",
    "        return ((1 - p_y.pow(self.q)) / self.q).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_block(cin, cout):\n",
    "    return nn.Sequential(\n",
    "        nn.Conv2d(cin, cout, 3, padding=1),\n",
    "        nn.BatchNorm2d(cout),\n",
    "        nn.ReLU(inplace=True),\n",
    "        nn.MaxPool2d(2)\n",
    "    )\n",
    "\n",
    "class SmallCNN28(nn.Module):\n",
    "    \"\"\"For 1×28×28 images.\"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            conv_block(1, 32),  # 14x14\n",
    "            conv_block(32, 64), # 7x7\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(64*7*7, 128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(128, 3)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "\n",
    "class SmallCNNCifar(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            conv_block(3, 64),     # 16x16\n",
    "            conv_block(64, 128),   # 8x8\n",
    "            conv_block(128, 256),  # 4x4\n",
    "            conv_block(256, 512),  # 2x2\n",
    "            nn.Dropout(0.3)\n",
    "        )\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(512*2*2, 256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(256, 3)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.classifier(self.features(x))\n",
    "\n",
    "\n",
    "def make_model(is_cifar, device=None):\n",
    "    model = SmallCNNCifar() if is_cifar else SmallCNN28()\n",
    "    if device is not None:\n",
    "        model = model.to(device)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def accuracy(model, loader, device):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for xb, yb in loader:\n",
    "        xb, yb = xb.to(device), yb.to(device)\n",
    "        logits = model(xb)\n",
    "        pred = logits.argmax(dim=1)\n",
    "        correct += (pred == yb).sum().item()\n",
    "        total += yb.numel()\n",
    "    return correct / max(total, 1)\n",
    "\n",
    "\n",
    "def fit_model(model, train_loader, val_loader, device, loss_name, T, q, beta=0.2, epochs=10, lr=1e-3):\n",
    "\n",
    "    model.to(device)\n",
    "\n",
    "    if loss_name == 'forward':\n",
    "        assert T is not None, \"Forward correction requires known/estimated T\"\n",
    "        criterion = ForwardCorrectedCE(T)\n",
    "    elif loss_name == 'forwardGCE':\n",
    "        assert T is not None, \"Forward correction requires known/estimated T\"\n",
    "        criterion = ForwardCorrectedGCE(T, q=q)\n",
    "    elif loss_name == 't-revision':\n",
    "        assert T is not None, \"T-revision requires known/estimated T\"\n",
    "        criterion = TRevisionLoss(C=3,device=device, q=q, lambda_reg=beta, T0=T).to(device)\n",
    "    else:\n",
    "        criterion = GeneralizedCrossEntropy(q=q)\n",
    "        \n",
    "    criterion = criterion.to(device)\n",
    "\n",
    "    if isinstance(criterion, TRevisionLoss):\n",
    "       optimizer = optim.Adam([\n",
    "            {'params': model.parameters(), 'lr': lr},\n",
    "            {'params': criterion.parameters(), 'lr': lr * 20}\n",
    "        ], weight_decay=1e-4)\n",
    "    else:\n",
    "        optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=1e-4)\n",
    "\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer, mode='max', factor=0.5, patience=2, min_lr=1e-5\n",
    "    )\n",
    "\n",
    "    # ---- Optional warm-up on clean CE loss ----\n",
    "    if loss_name in ['forwardGCE', 't-revision']:\n",
    "        warmup_epochs = 3\n",
    "        warmup_criterion = nn.CrossEntropyLoss()\n",
    "        print(f\"-----------Warm-up training for {warmup_epochs} epochs with CE loss-----------\")\n",
    "        warmup_optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=1e-4)\n",
    "        for w in range(warmup_epochs):\n",
    "            train_one_epoch(model, train_loader, warmup_optimizer, warmup_criterion, device)\n",
    "        print(\"-----------Warm-up for {warmup_epochs} epochs with CE loss finished.-----------\\n\")\n",
    "\n",
    "    best_val = -np.inf\n",
    "    best_state = None\n",
    "    loss_arr = [] \n",
    "    grad_norms = []\n",
    "    val_acc_arr = []\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        mean_loss, mean_acc, _ = train_one_epoch(model, train_loader, optimizer, criterion, device)\n",
    "        val_acc = accuracy(model, val_loader, device)\n",
    "        val_acc_arr.append(val_acc)\n",
    "        # --- Gradient norm tracking ---\n",
    "        total_norm = 0.0\n",
    "        for p in model.parameters():\n",
    "            if p.grad is not None:\n",
    "                total_norm += p.grad.data.norm(2).item()\n",
    "        grad_norms.append(total_norm)\n",
    "\n",
    "        # --- Scheduler step ---\n",
    "        old_lr = optimizer.param_groups[0]['lr']\n",
    "        if epoch > 5:\n",
    "            scheduler.step(val_acc)\n",
    "        new_lr = optimizer.param_groups[0]['lr']\n",
    "        if new_lr < old_lr:\n",
    "            print(f\"-----------LR reduced from {old_lr:.6f} → {new_lr:.6f}-----------\")\n",
    "\n",
    "        # --- Best model checkpoint ---\n",
    "        if val_acc > best_val:\n",
    "            best_val = val_acc\n",
    "            best_state = {k: v.detach().cpu().clone() for k, v in model.state_dict().items()}\n",
    "        if epoch==0 or epoch == (epochs-1):\n",
    "            print(f\"Epoch {epoch+1}/{epochs} |: val_acc={val_acc*100:.2f}%, best_val={best_val*100:.2f}%, mean_loss={mean_loss:.4f}, mean_acc={mean_acc*100:.2f}%, grad_norm: {total_norm:.4f}\")\n",
    "        loss_arr.append(mean_loss)\n",
    "        if isinstance(criterion, TRevisionLoss):\n",
    "            with torch.no_grad():\n",
    "                T_current = torch.softmax(criterion.logits, dim=1)\n",
    "                delta_T_epoch = torch.norm(T_current - torch.softmax(torch.log(T), dim=1)).item()\n",
    "                print(f\"ΔT (epoch {epoch+1}): {delta_T_epoch:.5f}\")\n",
    "\n",
    "    # --- Restore best model ---\n",
    "    if best_state is not None:\n",
    "        model.load_state_dict(best_state)\n",
    "\n",
    "    # --- After all epochs have finished ---\n",
    "    if isinstance(criterion, TRevisionLoss):\n",
    "        with torch.no_grad():\n",
    "            T_final = torch.softmax(criterion.logits, dim=1)\n",
    "            delta_T = torch.norm(T_final - torch.softmax(torch.log(T), dim=1)).item() if isinstance(T, torch.Tensor) else None\n",
    "            print(f\"ΔT magnitude: {delta_T:.6f}\" if delta_T is not None else \"ΔT magnitude: n/a (no baseline tensor)\")\n",
    "            print(\"T_final:\")\n",
    "            print(T_final.cpu().numpy().round(4))\n",
    "    return model, loss_arr, val_acc_arr, grad_norms, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(model, loader, optimizer, criterion=None, device=None):\n",
    "    # ---- device setup ----\n",
    "    if device is None:\n",
    "        device = (\n",
    "            \"cuda\" if torch.cuda.is_available()\n",
    "            else \"mps\" if torch.backends.mps.is_available()\n",
    "            else \"cpu\"\n",
    "        )\n",
    "    device = torch.device(device)\n",
    "\n",
    "    # move model to device\n",
    "    model.to(device)\n",
    "    model.train()\n",
    "\n",
    "    if criterion is None:\n",
    "        criterion = torch.nn.CrossEntropyLoss(reduction='none')  # per-sample losses\n",
    "\n",
    "    all_losses = []   # for histogram\n",
    "    batch_metrics = []  # (batch_idx, acc, mean_loss)\n",
    "\n",
    "    for batch_idx, (xb, yb) in enumerate(loader, 1):\n",
    "        xb, yb = xb.to(device), yb.to(device)\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "        logits = model(xb)\n",
    "        per_sample_loss = criterion(logits, yb)\n",
    "\n",
    "        # ensure scalar loss\n",
    "        if per_sample_loss.ndim > 0:\n",
    "            loss = per_sample_loss.mean()\n",
    "            all_losses.extend(per_sample_loss.detach().cpu().tolist())\n",
    "        else:\n",
    "            loss = per_sample_loss\n",
    "            all_losses.append(loss.detach().cpu().item())\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        preds = logits.argmax(dim=1)\n",
    "        acc = (preds == yb).float().mean().item()\n",
    "        batch_metrics.append((batch_idx, acc, loss.item()))        \n",
    "\n",
    "    # ---- epoch summary ----\n",
    "    mean_loss = sum([l for _,_,l in batch_metrics]) / len(batch_metrics)\n",
    "    mean_acc = sum([a for _,a,_ in batch_metrics]) / len(batch_metrics)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        total_norm = 0\n",
    "        for p in model.parameters():\n",
    "            total_norm += p.grad.data.norm(2).item() if p.grad is not None else 0\n",
    "\n",
    "    return mean_loss, mean_acc, all_losses\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def predict_proba(model, loader, device):\n",
    "\n",
    "    model.eval()\n",
    "    p_list, y_list = [], []\n",
    "    for xb, yb in loader:\n",
    "        xb = xb.to(device)\n",
    "        output = model(xb)\n",
    "        p = F.softmax(output, dim=1)\n",
    "        p_list.append(p)\n",
    "        y_list.append(yb.to(device))\n",
    "    return torch.cat(p_list), torch.cat(y_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_transition_anchor(t, train_loader, is_cifar, q, device, epochs):\n",
    "    \"\"\"\n",
    "    Simple anchor/confident-example estimator (Patrini et al., 2017 style):\n",
    "    1) Train a base classifier on noisy data.\n",
    "    2) Get p(y|x) on training set.\n",
    "    3) For each clean class i, find indices whose predicted argmax == noisy label == i and with high confidence.\n",
    "    4) For those indices, estimate column i of T as average of empirical noisy label distribution given model predicts i.\n",
    "    Here: since we only have noisy labels S, we approximate T[:, i] ≈ E[ onehot(S) | argmax p = i, p_i >= τ ].\n",
    "    Normalize columns to sum to 1.\n",
    "    \"\"\"\n",
    "\n",
    "    model = make_model(is_cifar).to(device)\n",
    "\n",
    "    optimizer = optim.Adam(model.parameters(), lr=1e-3, weight_decay=1e-4)\n",
    "    criterion = GeneralizedCrossEntropy(q=q)\n",
    "    # quick warmup training on noisy labels\n",
    "    for _ in range(epochs):\n",
    "        train_one_epoch(model, train_loader, optimizer, criterion, device=device) \n",
    "        \n",
    "    #print('devise - estimation')   \n",
    "    #print(next(model.parameters()).device)\n",
    "\n",
    "\n",
    "    # collect probs & noisy labels\n",
    "    p_arr, y_noisy = predict_proba(model, train_loader, device)\n",
    "\n",
    "    # ensure both are torch tensors on same device\n",
    "    if not torch.is_tensor(p_arr):\n",
    "        p_arr = torch.tensor(p_arr, dtype=torch.float32, device=device)\n",
    "    if not torch.is_tensor(y_noisy):\n",
    "        y_noisy = torch.tensor(y_noisy, dtype=torch.long, device=device)\n",
    "    else:\n",
    "        y_noisy = y_noisy.to(device, dtype=torch.long)\n",
    "\n",
    "    preds = p_arr.argmax(dim=1)\n",
    "    maxp = p_arr.max(dim=1).values\n",
    "\n",
    "    C = num_classes\n",
    "    T = t\n",
    "\n",
    "    if isinstance(T, np.ndarray):\n",
    "        T = torch.tensor(T, dtype=torch.float32, device=device)\n",
    "    elif torch.is_tensor(T):\n",
    "        T = T.to(device)\n",
    "        \n",
    "    # choose class-wise thresholds based on quantiles for stability\n",
    "    for i in range(C):\n",
    "        idx = (preds == i).nonzero(as_tuple=True)[0]\n",
    "        if idx.numel() == 0:\n",
    "            T[i, :] = torch.ones(C, device=device) / C\n",
    "            continue\n",
    "        # high-confidence subset (top 30% by p_i)\n",
    "        conf = maxp[idx]\n",
    "        if conf.numel() > 50:\n",
    "            tau = torch.quantile(conf, 0.7).item()\n",
    "        else:\n",
    "            tau = conf.min().item()\n",
    "        keep = idx[conf >= tau]\n",
    "        if keep.size == 0:\n",
    "            keep = idx\n",
    "        # empirical distribution of noisy labels among confident examples\n",
    "        hist = torch.bincount(y_noisy[keep], minlength=C).float()\n",
    "        if hist.sum() == 0:\n",
    "            T[i, :] = torch.ones(C, device=device) / C\n",
    "        else:\n",
    "            T[i, :] = hist / hist.sum()\n",
    "\n",
    "    # normalize rows to sum to 1\n",
    "    T = torch.clamp(T, min=1e-8)\n",
    "    T = T / T.sum(dim=1, keepdim=True)\n",
    "\n",
    "    return T.detach().to(device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_once(cfg, T, seed, train_loader, val_loader):\n",
    "    set_seed(seed)\n",
    "    device = torch.device(cfg['device'])\n",
    "\n",
    "    #C=3\n",
    "    # load data\n",
    "    Xtr, Str, Xts, Yts = load_npz(DATA_PATHS[cfg['dataset']])\n",
    "\n",
    "    # loaders for this split\n",
    "    #train_loader, val_loader, is_cifar = make_loaders(Xtr, Str, batch_size=cfg['batch_size'], seed=seed)\n",
    "    \n",
    "    if train_loader is None or val_loader is None:\n",
    "        train_loader, val_loader, is_cifar = make_loaders(Xtr, Str, batch_size=cfg['batch_size'], seed=seed)\n",
    "    else:\n",
    "        is_cifar = cfg['is_cifar']\n",
    "    #xb, yb = next(iter(train_loader))\n",
    "    #print(\"Batch shape:\", xb.shape, \"min/max:\", xb.min().item(), xb.max().item())\n",
    "    test_loader = make_test_loader(Xts, Yts, batch_size=512)\n",
    "    \n",
    "    # choose model\n",
    "    model = make_model(is_cifar).to(device)\n",
    "\n",
    "    # fit\n",
    "    model, loss_arr, val_acc, grad_norm= fit_model(\n",
    "        model,\n",
    "        train_loader,\n",
    "        val_loader,\n",
    "        device,\n",
    "        loss_name=cfg['loss'],\n",
    "        T=T,\n",
    "        q=cfg['q'],\n",
    "        beta=cfg['beta'],\n",
    "        epochs=cfg['epochs'],\n",
    "        lr=cfg['lr'],\n",
    "    )\n",
    "\n",
    "    all_loss_arrays = loss_arr\n",
    "    \n",
    "    # evaluate on clean test set\n",
    "    test_acc = accuracy(model, test_loader, device)\n",
    "    return float(test_acc), val_acc, (T if T is not None else None), all_loss_arrays, grad_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_rows = []\n",
    "C = 3\n",
    "graph_data = []\n",
    "all_figs = []\n",
    "os.mkdir(os.path.join(folder, 'img'))\n",
    "for cfg in configs:\n",
    "    \n",
    "    print(\"=\" * 100)\n",
    "    print(f\"Running config {configs.index(cfg)+1}/{len(configs)}\")\n",
    "    print(json.dumps(cfg, indent=2))\n",
    "    # collect summary rows for TSV\n",
    "    dataset_file = f\"{folder}/{cfg['dataset']}_results.json\"\n",
    "    if os.path.exists(dataset_file):\n",
    "        with open(dataset_file, 'r') as f:\n",
    "            try:\n",
    "                all_data = json.load(f)\n",
    "            except json.JSONDecodeError:\n",
    "                all_data = []\n",
    "    else:\n",
    "        all_data = []\n",
    "\n",
    "\n",
    "    all_acc = []\n",
    "    all_mean_loss = []\n",
    "    all_val_acc = []\n",
    "    all_loss_arrays = []\n",
    "    last_T = None\n",
    "    t_arr = []\n",
    "    \n",
    "    # load data\n",
    "    Xtr, Str, Xts, Yts = load_npz(DATA_PATHS[cfg['dataset']])\n",
    "    # Transition matrix\n",
    "    T = None\n",
    "    train_loader, val_loader, is_cifar = make_loaders(Xtr, Str, batch_size=cfg['batch_size'], seed=0)\n",
    "   \n",
    "    if cfg['estimate_T']=='anchor':\n",
    "        T = torch.zeros((C, C), dtype=torch.float32, device=device)\n",
    "        T = estimate_transition_anchor(T, train_loader, is_cifar, q, device=device, epochs=cfg['est_epochs'])\n",
    "        print(\"Estimated T:\")\n",
    "        print(np.array2string(T.cpu().numpy(), formatter={'float_kind':lambda x: f\"{x:.4f}\"}))\n",
    "    else:\n",
    "        T = pick_known_T(cfg['dataset'])\n",
    "        print('known_T')\n",
    "        print(T)\n",
    "        if T is None:\n",
    "            raise ValueError(\"Forward loss selected but no known T for this dataset; use --estimate_T.\")\n",
    "            \n",
    "    #if isinstance(T, np.ndarray):\n",
    "    #    T = torch.tensor(T, dtype=torch.float32, device=device)\n",
    "\n",
    "    # get true T if known\n",
    "    T_true = pick_known_T(cfg['dataset'])\n",
    "    has_T_true = T_true is not None\n",
    "\n",
    "    if T is not None:\n",
    "        if not torch.is_tensor(T):\n",
    "            T = torch.tensor(T, dtype=torch.float32, device=device)\n",
    "        else:\n",
    "            T = T.to(device=device, dtype=torch.float32)\n",
    "\n",
    "        # Ensure row-stochastic\n",
    "        T = torch.clamp(T, 1e-6, 1.0)\n",
    "        T = T / T.sum(dim=1, keepdim=True)\n",
    "\n",
    "        # Stabilize\n",
    "        T = stabilize_T(T, alpha=0.5)\n",
    "\n",
    "    train_loader, val_loader, is_cifar = make_loaders(Xtr, Str, batch_size=cfg['batch_size'], seed=0)\n",
    "\n",
    "    for r in range(cfg['runs']):\n",
    "        print(f\">> Run:{r} Starting >>\")\n",
    "        acc, val_acc, T, loss_arr, grad_norms = run_once(cfg, T, seed=1000+r,   # seed only for weight init\n",
    "                                                        train_loader=train_loader,\n",
    "                                                        val_loader=val_loader)\n",
    "\n",
    "        all_acc.append(acc)\n",
    "        all_loss_arrays.append(loss_arr)\n",
    "        all_mean_loss.append(np.mean(loss_arr))\n",
    "\n",
    "\n",
    "        all_val_acc.append(val_acc)\n",
    "        \n",
    "        if cfg['loss'] == 't-revision':    # Only for TRevision\n",
    "            t_arr.append(T.detach().cpu().numpy().tolist())\n",
    "\n",
    "        last_T = T if T is not None else last_T\n",
    "\n",
    "        print(f\">> Run:{r} Finishing >>\")\n",
    "\n",
    "    graph_data.append({'cfg':cfg, 'all_acc':all_acc, 'all_mean_loss': all_mean_loss})\n",
    "    fig = plot_run_performance(all_acc, all_mean_loss, cfg, folder)\n",
    "\n",
    "    all_figs.append({\n",
    "        \"cfg\": cfg,\n",
    "        \"figure\": fig\n",
    "    })\n",
    "    \n",
    "    mean_acc = float(np.mean(all_acc))\n",
    "    std_acc  = float(np.std(all_acc))\n",
    "    mean_loss = float(np.mean(all_mean_loss))\n",
    "    std_loss  = float(np.std(all_mean_loss))\n",
    "\n",
    "    # compute metrics vs true T (if available)\n",
    "    fro_err = mae_err = rre_err = None\n",
    "    if has_T_true and last_T is not None:\n",
    "        T_true = np.array(T_true)\n",
    "        T_est = last_T.detach().cpu().numpy().tolist()\n",
    "\n",
    "        fro_err = float(np.linalg.norm(T_est - T_true, 'fro'))\n",
    "        mae_err = float(np.mean(np.abs(T_est - T_true)))\n",
    "        rre_err = float(fro_err / np.linalg.norm(T_true, 'fro'))\n",
    "\n",
    "    # build summary object\n",
    "    summary = {\n",
    "        'cfg': cfg,\n",
    "        'dataset': cfg['dataset'],\n",
    "        'loss': cfg['loss'],\n",
    "        'estimate_T': cfg['estimate_T'],\n",
    "        'epochs': cfg['epochs'],\n",
    "        'runs': cfg['runs'],\n",
    "        'mean_test_acc': mean_acc,\n",
    "        'std_test_acc': std_acc,\n",
    "        'mean_train_loss': mean_loss,\n",
    "        'std_train_loss': std_loss,\n",
    "        'per_run_acc': all_acc,\n",
    "        'per_run_mean_loss': all_mean_loss,\n",
    "        'per_run_loss_arrays': all_loss_arrays,\n",
    "    }\n",
    "    if has_T_true:\n",
    "        summary['true_T'] = T_true.tolist()\n",
    "        summary['fro_error'] = fro_err\n",
    "        summary['mae_error'] = mae_err\n",
    "        summary['rre_error'] = rre_err\n",
    "    \n",
    "    if t_arr is not None:\n",
    "        summary['t_arr'] = t_arr,\n",
    "\n",
    "\n",
    "    all_data.append(summary)\n",
    "    \n",
    "    print(f\"{cfg['dataset']} | {cfg['loss']} | mean±std acc: {mean_acc*100:.2f}±{std_acc*100:.2f}% | mean loss: {mean_loss:.4f}\")\n",
    "    print(\"=\" * 100) \n",
    "\n",
    "    if fro_err is not None:\n",
    "        print(f\"T-error: Fro {fro_err:.4f}, MAE {mae_err:.4f}, RRE {rre_err:.4f}\")\n",
    "\n",
    "    with open(dataset_file, 'w') as f:\n",
    "        json.dump(all_data, f, indent=2)\n",
    "    print(f\"Appended results for {cfg['loss']} → {dataset_file}\")\n",
    "\n",
    "    # ➕ Add a summary row for TSV\n",
    "    results_rows.append({\n",
    "        'dataset': cfg['dataset'],\n",
    "        'loss': cfg['loss'],\n",
    "        'estimate_T': cfg['estimate_T'],\n",
    "        'epochs': cfg['epochs'],\n",
    "        'runs': cfg['runs'],\n",
    "        'q': cfg['q'],\n",
    "        'lr': cfg['lr'],\n",
    "        'beta': cfg['beta'],\n",
    "        'mean_test_acc': round(mean_acc * 100, 2),\n",
    "        'std_test_acc': round(std_acc * 100, 2),\n",
    "        'mean_train_loss': round(mean_loss, 4),\n",
    "        'std_train_loss': round(std_loss, 4),\n",
    "        'fro_error': None if fro_err is None else round(fro_err, 4),\n",
    "        'mae_error': None if mae_err is None else round(mae_err, 4),\n",
    "        'rre_error': None if rre_err is None else round(rre_err, 4)\n",
    "    })\n",
    "\n",
    "# After each config loop, write TSV summary\n",
    "if results_rows:\n",
    "    df = pd.DataFrame(results_rows)\n",
    "    tsv_path = os.path.join(folder, \"summary_results.tsv\")\n",
    "    df.to_csv(tsv_path, sep='\\t', index=False)\n",
    "    print(f\"\\n✅ Wrote table summary to {tsv_path}\")\n",
    "    print(df.to_markdown(index=False))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for filename in os.listdir(folder):\n",
    "    if filename.endswith(\".json\"):\n",
    "        filepath = os.path.join(folder, filename)\n",
    "        try:\n",
    "            with open(filepath, \"r\", encoding=\"utf-8\") as f:\n",
    "                data = json.load(f)\n",
    "            \n",
    "            dataset = data.get(\"dataset\", \"N/A\")\n",
    "            loss = data.get(\"loss\", \"N/A\")\n",
    "            mean_acc = data.get(\"mean_test_acc\", None)\n",
    "            std_acc = data.get(\"std_test_acc\", None)\n",
    "\n",
    "            print(f\"{filename}:\")\n",
    "            \n",
    "            print(f\"  dataset       = {dataset}\")\n",
    "            print(f\"  loss          = {loss}\")\n",
    "            print(f\"  mean_test_acc = {mean_acc:.4f}\" if mean_acc is not None else \"  mean_test_acc = N/A\")\n",
    "            print(f\"  std_test_acc  = {std_acc:.4f}\" if std_acc is not None else \"  std_test_acc = N/A\")\n",
    "            print(\"-\" * 60)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error reading {filename}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "C = 3\n",
    "files = os.listdir()\n",
    "files = sorted(glob.glob('results*'), reverse=True)\n",
    "folder = files[0]\n",
    "print(folder)\n",
    "\n",
    "# pattern for files starting with \"name\" and ending with \".json\"\n",
    "files = sorted(glob.glob(os.path.join(folder,\"fashion03*.json\")))\n",
    "print(files)\n",
    "\n",
    "# pick the first matching file\n",
    "first_file = files[0]\n",
    "print(\"Loading:\", first_file)\n",
    "\n",
    "# load the JSON contents\n",
    "with open(first_file, \"r\", encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "T_prime = np.array(data['last_estimated_T'])\n",
    "T_true = pick_known_T('fashion03')\n",
    "\n",
    "#checking recreation performance\n",
    "print(T_prime)\n",
    "print(T_true)\n",
    "print(f\"Fro error: {np.linalg.norm(T_prime - T_true, 'fro')}\")\n",
    "print(f\"rre error: {np.linalg.norm(T_prime - T_true, 'fro') / np.linalg.norm(T_true, 'fro')}\")\n",
    "print(f\"mae error: {np.mean(np.abs(T_prime - T_true))}\")\n",
    "\n",
    "\n",
    "corrs = [st.pearsonr(T_true[i], T_prime[i])[0] for i in range(C)]\n",
    "print(\"Per-row correlations:\", corrs)\n",
    "print(\"Mean:\", np.mean(corrs))\n",
    "\n",
    "\n",
    "# pattern for files starting with \"name\" and ending with \".json\"\n",
    "files = sorted(f for f in glob.glob(os.path.join(folder, \"fashion06*.json\")) if \"None\" not in f)\n",
    "\n",
    "\n",
    "# pick the first matching file\n",
    "first_file = files[0]\n",
    "print(\"\\nLoading:\", first_file)\n",
    "# load the JSON contents\n",
    "with open(first_file, \"r\", encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)\n",
    "print()\n",
    "T_prime = np.array(data['last_estimated_T'])\n",
    "\n",
    "T_true = pick_known_T('fashion06')\n",
    "\n",
    "#checking recreation performance\n",
    "print(T_prime)\n",
    "print(T_true)\n",
    "print(f\"Fro error: {np.linalg.norm(T_prime - T_true, 'fro')}\")\n",
    "print(f\"rre error: {np.linalg.norm(T_prime - T_true, 'fro') / np.linalg.norm(T_true, 'fro')}\")\n",
    "print(f\"mae error: {np.mean(np.abs(T_prime - T_true))}\")\n",
    "\n",
    "\n",
    "corrs = [st.pearsonr(T_true[i], T_prime[i])[0] for i in range(C)]\n",
    "print(\"Per-row correlations:\", corrs)\n",
    "print(\"Mean:\", np.mean(corrs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "old_path = \"img\"\n",
    "for img in os.listdir(old_path):\n",
    "    # Build the new path in the same directory\n",
    "    safe_name = img.replace('_png', '.png')\n",
    "    dir_name = os.path.dirname(old_path)\n",
    "    new_path = os.path.join(old_path\n",
    ", safe_name)\n",
    "\n",
    "    # Rename the file\n",
    "    os.rename(os.path.join(old_path, img), new_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "old_path = \"img\"\n",
    "for \n",
    "# Generate the new safe filename\n",
    "filename = os.path.basename(old_path)\n",
    "safe_name = re.sub(r'[:\\s]+', '_', filename).strip('_')\n",
    "\n",
    "# Build the new path in the same directory\n",
    "dir_name = os.path.dirname(old_path)\n",
    "new_path = os.path.join(dir_name, safe_name)\n",
    "\n",
    "# Rename the file\n",
    "os.rename(old_path, new_path)\n",
    "\n",
    "print(f\"Renamed to: {new_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import json\n",
    "import numpy as np\n",
    "from scipy import stats as st\n",
    "\n",
    "def evaluate_T_matrices(folder, pattern, dataset_name, C=3):\n",
    "    \"\"\"\n",
    "    Loops over all JSON files matching pattern, computes error metrics \n",
    "    and correlations vs. known T, and returns best-performing result.\n",
    "    \"\"\"\n",
    "    files = sorted(glob.glob(os.path.join(folder, pattern)))\n",
    "    if not files:\n",
    "        print(f\"No files found matching: {pattern}\")\n",
    "        return None, None\n",
    "    \n",
    "    T_true = pick_known_T(dataset_name)\n",
    "    results = []\n",
    "\n",
    "    print(f\"Evaluating {len(files)} files for dataset: {dataset_name}\\n\")\n",
    "\n",
    "    for file in files:\n",
    "        with open(file, \"r\", encoding=\"utf-8\") as f:\n",
    "            data = json.load(f)\n",
    "        T_prime = np.array(data['last_estimated_T'])\n",
    "        \n",
    "        fro_err = np.linalg.norm(T_prime - T_true, 'fro')\n",
    "        rre_err = fro_err / np.linalg.norm(T_true, 'fro')\n",
    "        mae_err = np.mean(np.abs(T_prime - T_true))\n",
    "        corrs = [st.pearsonr(T_true[i], T_prime[i])[0] for i in range(C)]\n",
    "        mean_corr = np.mean(corrs)\n",
    "        \n",
    "        print(f\"File: {os.path.basename(file)}\")\n",
    "        print(f\"Fro error: {fro_err:.6f}\")\n",
    "        print(f\"RRE error: {rre_err:.6f}\")\n",
    "        print(f\"MAE error: {mae_err:.6f}\")\n",
    "        print(f\"Per-row correlations: {np.round(corrs, 4)} | Mean: {mean_corr:.4f}\")\n",
    "        print(\"-\" * 60)\n",
    "\n",
    "        #checking recreation performance\n",
    "        print(\"T_prime:\")\n",
    "        print(np.round(T_prime, 2))\n",
    "        print(\"T_true:\")\n",
    "        print(np.round(T_true, 2))\n",
    "        print(\"T_true - T_prime:\")\n",
    "        print(np.round(T_true - T_prime, 2))\n",
    "        \n",
    "        results.append({\n",
    "            'file': file,\n",
    "            'T_prime': T_prime,\n",
    "            'fro': fro_err,\n",
    "            'rre': rre_err,\n",
    "            'mae': mae_err,\n",
    "            'corr_mean': mean_corr\n",
    "        })\n",
    "    \n",
    "    # Choose the best result (lowest Frobenius error)\n",
    "    best = min(results, key=lambda x: x['fro'])\n",
    "    print(f\"\\n✅ Best result: {os.path.basename(best['file'])}\")\n",
    "    print(f\"   Fro error: {best['fro']:.6f}, RRE: {best['rre']:.6f}, MAE: {best['mae']:.6f}, Corr: {best['corr_mean']:.4f}\")\n",
    "    \n",
    "    return best['file'], best['T_prime']\n",
    "\n",
    "\n",
    "# Example usage\n",
    "\n",
    "best_file_03, best_T_03 = evaluate_T_matrices(folder, \"fashion03*.json\", \"fashion03\", C=3)\n",
    "best_file_06, best_T_06 = evaluate_T_matrices(folder, \"fashion06*True*.json\", \"fashion06\", C=3)\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "dl-gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
